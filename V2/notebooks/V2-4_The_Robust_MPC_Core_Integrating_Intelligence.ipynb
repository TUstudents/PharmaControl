{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Notebook 4: The Robust MPC Core: Integrating Intelligence\n",
    "\n",
    "**Project:** `RobustMPC-Pharma` (V2)\n",
    "**Goal:** To assemble our advanced components into the final `RobustMPCController`. This new controller core will be uncertainty-aware, adaptive to disturbances, and capable of sophisticated optimization. This is where all our hard work comes together.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Theory: The Pillars of a Robust Controller](#1.-Theory:-The-Pillars-of-a-Robust-Controller)\n",
    "2. [Implementing the `RobustMPCController` V2](#2.-Implementing-the-RobustMPCController-V2)\n",
    "3. [Standalone Test of the Integrated Controller](#3.-Standalone-Test-of-the-Integrated-Controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Theory: The Pillars of a Robust Controller\n",
    "\n",
    "Our V2 controller is built on three pillars that address the weaknesses of the V1 prototype:\n",
    "\n",
    "1.  **Stable State Perception (The Kalman Filter):** It will not react to raw sensor noise. Instead, it will act on a smooth, stable estimate of the true process state. This prevents control jitter and improves efficiency.\n",
    "\n",
    "2.  **Risk-Aware Decision Making (The Probabilistic Model):** The controller's cost function will not just evaluate the model's mean prediction. It will incorporate the model's uncertainty (standard deviation). By optimizing a **risk-adjusted** forecast (e.g., Upper Confidence Bound), the controller will act more cautiously when the model is uncertain, making it fundamentally safer.\n",
    "\n",
    "3.  **Adaptability to Disturbances (Integral Action):** Real-world processes suffer from un-modeled, persistent disturbances (e.g., gradual equipment fouling, changes in raw material). A standard controller will have a steady-state error in these cases. By implementing **Integral Action**, our controller will learn to recognize and automatically compensate for these disturbances, ensuring it always drives the process precisely to the target setpoint. This is known as **Offset-Free MPC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Implementing the `RobustMPCController` V2\n",
    "\n",
    "We will now create the final `RobustMPCController` class in `src/core.py`. This class will be a high-level orchestrator, managing the interactions between the state estimator, the predictive model, and the optimizer.\n",
    "\n",
    "The core logic will follow this sequence:\n",
    "1.  Receive a noisy measurement from the plant.\n",
    "2.  Use the `KalmanStateEstimator` to get a clean state estimate.\n",
    "3.  Update its internal `disturbance_estimate` using the error between the setpoint and the clean state (Integral Action).\n",
    "4.  Define a `fitness_function` for the optimizer. This function will:\n",
    "    a. Take a candidate control plan.\n",
    "    b. Get a probabilistic prediction from the `ProbabilisticTransformer`.\n",
    "    c. Add the `disturbance_estimate` to the mean prediction.\n",
    "    d. Calculate a risk-adjusted cost using the mean and standard deviation.\n",
    "5.  Pass this fitness function to the `GeneticOptimizer` to find the best control plan.\n",
    "6.  Return the first step of the winning plan."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%writefile ../robust_mpc/core.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from .data_buffer import DataBuffer, StartupHistoryGenerator\n",
    "\n",
    "class RobustMPCController:\n",
    "    \"\"\"Advanced Model Predictive Controller for robust pharmaceutical process control.\n",
    "    \n",
    "    This controller integrates multiple advanced techniques to achieve superior performance\n",
    "    in pharmaceutical continuous granulation processes:\n",
    "    \n",
    "    1. **Robust State Estimation**: Bias-corrected Kalman filtering for accurate state estimation\n",
    "    2. **Probabilistic Prediction**: Uncertainty quantification through dropout-based ensembles\n",
    "    3. **Genetic Optimization**: Global optimization for complex, constrained control problems\n",
    "    4. **Integral Action**: Disturbance estimation for offset-free tracking performance\n",
    "    \n",
    "    Architecture Components:\n",
    "        - State Estimator: Handles sensor noise and systematic model bias\n",
    "        - Probabilistic Model: Provides mean predictions with uncertainty bounds\n",
    "        - Genetic Optimizer: Searches complex action spaces with constraints\n",
    "        - Risk Management: Considers prediction uncertainty in control decisions\n",
    "    \n",
    "    Key Features:\n",
    "        - Offset-free control through integral action and bias correction\n",
    "        - Uncertainty-aware control decisions based on prediction confidence\n",
    "        - Robust optimization using genetic algorithms for global optima\n",
    "        - Multi-objective optimization balancing tracking performance and risk\n",
    "        - Industrial-grade error handling with safe fallback strategies\n",
    "    \n",
    "    CRITICAL IMPLEMENTATION DETAIL - Data Scaling:\n",
    "        This controller implements TWO DISTINCT scaling methods for different data types:\n",
    "        \n",
    "        1. **Value Scaling** (_scale_cma_vector, _scale_cma_plan, _scale_cpp_plan):\n",
    "           - Formula: scaled = (value - min) / (max - min)\n",
    "           - Used for: Absolute measurements, setpoints, control actions\n",
    "           - Includes translation term (- min) to map range to [0,1]\n",
    "           - Appropriate for scaling actual process measurements\n",
    "        \n",
    "        2. **Offset Scaling** (_scale_cma_offset):\n",
    "           - Formula: scaled = offset / (max - min)  [NO translation]\n",
    "           - Used for: Disturbance estimates, corrections, integral action terms\n",
    "           - NO translation term - preserves zero-mean property\n",
    "           - CRITICAL for proper integral action functionality\n",
    "        \n",
    "        Mathematical Justification:\n",
    "            Offsets represent corrections/disturbances, not absolute values.\n",
    "            Adding a constant (min) to a correction term fundamentally changes\n",
    "            its meaning and breaks the integral action. The offset scaling \n",
    "            preserves proportionality while maintaining the zero-mean nature\n",
    "            of disturbance estimates.\n",
    "        \n",
    "        Example Impact:\n",
    "            - True disturbance: +30 μm in d50 (range 300-600 μm)\n",
    "            - Value scaling: (30 - 300) / 300 = -0.9  [WRONG - negative bias!]\n",
    "            - Offset scaling: 30 / 300 = 0.1           [CORRECT - preserves sign]\n",
    "        \n",
    "        This distinction is ESSENTIAL for offset-free MPC functionality.\n",
    "    \n",
    "    Args:\n",
    "        model: Probabilistic neural network model implementing predict_distribution()\n",
    "            Must provide both mean predictions and uncertainty estimates\n",
    "        estimator: State estimator (KalmanStateEstimator or bias-corrected variant)\n",
    "            Provides filtered state estimates from noisy sensor measurements\n",
    "        optimizer_class: Genetic algorithm optimizer class for action optimization\n",
    "            Should implement population-based optimization with constraint handling\n",
    "        config (dict): Controller configuration containing:\n",
    "            - 'cma_names': List of Critical Material Attribute names\n",
    "            - 'cpp_names': List of Critical Process Parameter names  \n",
    "            - 'horizon': Control and prediction horizon length\n",
    "            - 'integral_gain': Adaptation rate for disturbance estimation\n",
    "            - 'mc_samples': Number of Monte Carlo samples for uncertainty quantification\n",
    "            - 'risk_aversion': Risk penalty weight (higher = more conservative)\n",
    "            - 'verbose': Enable verbose validation logging (default: False)\n",
    "        scalers (dict): Data scaling transformations for consistent preprocessing\n",
    "            Should contain fitted scalers for all process variables\n",
    "    \n",
    "    Attributes:\n",
    "        model: Stored probabilistic prediction model\n",
    "        estimator: Stored state estimation module\n",
    "        optimizer_class: Stored optimization algorithm class\n",
    "        config (dict): Stored controller configuration\n",
    "        scalers (dict): Stored data preprocessing scalers\n",
    "        device (str): PyTorch device for model computations\n",
    "        disturbance_estimate (np.ndarray): Current integral action term\n",
    "    \n",
    "    Example:\n",
    "        >>> # Configure robust MPC for granulation process\n",
    "        >>> controller = RobustMPCController(\n",
    "        ...     model=probabilistic_transformer,\n",
    "        ...     estimator=bias_corrected_kalman,\n",
    "        ...     optimizer_class=GeneticOptimizer,\n",
    "        ...     config=mpc_config,\n",
    "        ...     scalers=data_scalers\n",
    "        ... )\n",
    "        >>> \n",
    "        >>> # Execute control step\n",
    "        >>> past_cmas = df_history[['d50', 'lod']].values\n",
    "        >>> past_cpps = df_history[['spray_rate', 'air_flow', 'carousel_speed']].values\n",
    "        >>> targets = np.array([450.0, 1.8])  # Target d50=450μm, LOD=1.8%\n",
    "        >>> optimal_action = controller.suggest_action(past_cmas, past_cpps, targets)\n",
    "    \n",
    "    Notes:\n",
    "        - Automatically handles device placement for GPU acceleration when available\n",
    "        - Integral action provides offset-free tracking for unmeasured disturbances\n",
    "        - Risk-aware optimization considers both tracking error and prediction uncertainty\n",
    "        - Genetic optimization enables handling of complex, non-convex constraint spaces\n",
    "    \"\"\"\n",
    "    def __init__(self, model, estimator, optimizer_class, config, scalers):\n",
    "        self.model = model\n",
    "        self.estimator = estimator\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.config = config\n",
    "        self.scalers = scalers\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Validate configuration and scalers\n",
    "        self._validate_initialization()\n",
    "\n",
    "        # Initialize the disturbance estimate for Integral Action\n",
    "        self.disturbance_estimate = np.zeros(len(config['cma_names']))\n",
    "        \n",
    "        # Pre-initialize fallback control action with guaranteed safe defaults\n",
    "        # CRITICAL: Ensures safe fallback is available from very first control step\n",
    "        self._last_successful_action = self._calculate_safe_default_action()\n",
    "        \n",
    "        # Initialize setpoint tracking for intelligent optimizer reset\n",
    "        self._last_setpoint = None\n",
    "        \n",
    "        # Initialize rolling history buffer for real trajectory tracking\n",
    "        buffer_size = config.get('history_buffer_size', max(100, 3 * config['lookback']))\n",
    "        self.history_buffer = DataBuffer(\n",
    "            cma_features=len(config['cma_names']),\n",
    "            cpp_features=len(config['cpp_names']),\n",
    "            buffer_size=buffer_size,\n",
    "            validate_sequence=True\n",
    "        )\n",
    "        \n",
    "        # Startup history generator for initial operation\n",
    "        self.startup_generator = None\n",
    "        self._initialization_complete = False\n",
    "        \n",
    "        # Initialize optimizer as instance variable for efficiency\n",
    "        # This prevents creating new DEAP populations on every control step\n",
    "        if optimizer_class is not None:\n",
    "            param_bounds = self._get_param_bounds()\n",
    "            # Create complete GA config with required parameters\n",
    "            ga_config = config['ga_config'].copy()\n",
    "            ga_config['horizon'] = config['horizon']\n",
    "            ga_config['num_cpps'] = len(config['cpp_names'])\n",
    "            self.optimizer = optimizer_class(param_bounds, ga_config)\n",
    "        else:\n",
    "            self.optimizer = None\n",
    "\n",
    "    def _update_disturbance_estimate(self, smooth_state, setpoint):\n",
    "        \"\"\"Updates the integral error term for offset-free control.\"\"\"\n",
    "        error = setpoint - smooth_state\n",
    "        # The gain (alpha) determines how quickly the controller adapts to the disturbance\n",
    "        alpha = self.config.get('integral_gain', 0.1)\n",
    "        self.disturbance_estimate += alpha * error\n",
    "\n",
    "    def _get_fitness_function(self, past_cmas_scaled, past_cpps_scaled, target_cmas_unscaled):\n",
    "        \"\"\"\n",
    "        Creates and returns the fitness function to be used by the GA.\n",
    "        This function captures the current state and target.\n",
    "        \n",
    "        CRITICAL: Expects scaled control plans from GA since _get_param_bounds()\n",
    "        now returns scaled bounds. This ensures consistency in the optimization space.\n",
    "        \"\"\"\n",
    "        def fitness(control_plan_scaled):\n",
    "            # --- 1. Prepare Inputs ---\n",
    "            # GA now provides scaled control plan directly (no scaling needed)\n",
    "            plan_scaled = control_plan_scaled.copy()  # Use scaled plan directly\n",
    "\n",
    "            # Convert all inputs to tensors\n",
    "            past_cmas_tensor = torch.tensor(past_cmas_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            past_cpps_tensor = torch.tensor(past_cpps_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            future_cpps_tensor = torch.tensor(plan_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # --- 2. Get Probabilistic Prediction ---\n",
    "            mean_pred_scaled, std_pred_scaled = self.model.predict_distribution(\n",
    "                past_cmas_tensor, past_cpps_tensor, future_cpps_tensor, \n",
    "                n_samples=self.config.get('mc_samples', 30)\n",
    "            )\n",
    "\n",
    "            # --- 3. Calculate Risk-Adjusted, Corrected Prediction ---\n",
    "            # Correct for disturbance (Integral Action)\n",
    "            # CRITICAL: Use offset scaling for disturbance estimates, not value scaling\n",
    "            disturbance_scaled = self._scale_cma_offset(self.disturbance_estimate)\n",
    "            corrected_mean_scaled = mean_pred_scaled + torch.tensor(disturbance_scaled, device=self.device)\n",
    "\n",
    "            # Adjust for risk (Uncertainty-Awareness)\n",
    "            beta = self.config.get('risk_beta', 1.5) # Higher beta = more cautious\n",
    "            # For minimization, we penalize the upper bound of the error\n",
    "            risk_adjusted_pred_scaled = corrected_mean_scaled + beta * std_pred_scaled\n",
    "\n",
    "            # --- 4. Calculate Cost ---\n",
    "            target_scaled = self._scale_cma_plan(target_cmas_unscaled)\n",
    "            target_tensor = torch.tensor(target_scaled, dtype=torch.float32).to(self.device)\n",
    "            cost = torch.mean(torch.abs(risk_adjusted_pred_scaled - target_tensor))\n",
    "\n",
    "            return cost.item()\n",
    "\n",
    "        return fitness\n",
    "\n",
    "    def suggest_action(self, noisy_measurement, control_input, setpoint, timestamp=None):\n",
    "        \"\"\"Enhanced suggest_action with real history buffer integration.\n",
    "        \n",
    "        Args:\n",
    "            noisy_measurement (np.ndarray): Current CMA measurements with sensor noise\n",
    "            control_input (np.ndarray): Current CPP control inputs  \n",
    "            setpoint (np.ndarray): Target CMA setpoints\n",
    "            timestamp (float, optional): Unix timestamp for sequencing. If None, uses current time.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Optimal control action for next time step\n",
    "        \"\"\"\n",
    "        # 0. CRITICAL SAFETY CHECK: Validate buffer integrity for pharmaceutical manufacturing\n",
    "        if len(self.history_buffer) > 0:\n",
    "            cma_len = len(self.history_buffer._cma_buffer)\n",
    "            cpp_len = len(self.history_buffer._cpp_buffer)\n",
    "            if cma_len != cpp_len:\n",
    "                if self.config.get('verbose', False):\n",
    "                    print(f\"WARNING: Buffer misalignment detected - CMA: {cma_len}, CPP: {cpp_len}\")\n",
    "        \n",
    "        # 1. Get a clean state estimate\n",
    "        smooth_state = self.estimator.estimate(noisy_measurement, control_input)\n",
    "\n",
    "        # 2. Update history buffer with real data (critical for accurate predictions)\n",
    "        # CRITICAL SAFETY FIX: Use atomic operation to prevent race condition data misalignment\n",
    "        try:\n",
    "            self.history_buffer.add_sample(smooth_state, control_input, timestamp)\n",
    "        except Exception as e:\n",
    "            if self.config.get('verbose', False):\n",
    "                print(f\"Warning: Failed to update history buffer: {e}\")\n",
    "\n",
    "        # 3. Update the integral error term\n",
    "        self._update_disturbance_estimate(smooth_state, setpoint)\n",
    "\n",
    "        # 4. Intelligent optimizer reset on significant setpoint changes\n",
    "        if self._should_reset_optimizer(setpoint):\n",
    "            if self.config.get('verbose', False):\n",
    "                print(f\"Significant setpoint change detected, resetting optimizer for fresh exploration\")\n",
    "            self._reset_optimizer()\n",
    "        \n",
    "        # Update setpoint tracking for next comparison\n",
    "        self._last_setpoint = setpoint.copy()\n",
    "\n",
    "        # 5. Get historical data for model prediction (real or startup)\n",
    "        past_cmas_scaled, past_cpps_scaled = self._get_real_history()\n",
    "\n",
    "        # 6. Create the fitness function for this specific time step\n",
    "        target_plan = np.tile(setpoint, (self.config['horizon'], 1))\n",
    "        fitness_func = self._get_fitness_function(past_cmas_scaled, past_cpps_scaled, target_plan)\n",
    "\n",
    "        # 7. Use existing optimizer instance with current fitness function\n",
    "        if self.optimizer is None:\n",
    "            if self.config.get('verbose', False):\n",
    "                print(\"No optimizer available, using fallback control strategy\")\n",
    "            return self._get_fallback_action(control_input)\n",
    "            \n",
    "        try:\n",
    "            best_plan_scaled = self.optimizer.optimize(fitness_func)\n",
    "            \n",
    "            # Validate optimization result\n",
    "            if best_plan_scaled is None or best_plan_scaled.size == 0:\n",
    "                raise ValueError(\"Optimizer returned invalid result\")\n",
    "            \n",
    "            # CRITICAL: Unscale the optimizer result back to physical units\n",
    "            # Since optimizer now works in scaled space, we need to convert back\n",
    "            best_plan_unscaled = self._unscale_cpp_plan(best_plan_scaled)\n",
    "                \n",
    "            # Store successful action for fallback (in physical units)\n",
    "            self._last_successful_action = best_plan_unscaled[0].copy()\n",
    "            \n",
    "            # 8. Return the first step of the optimal plan (in physical units)\n",
    "            return best_plan_unscaled[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.config.get('verbose', False):\n",
    "                print(f\"Optimizer failed: {e}\")\n",
    "                print(\"Using fallback control strategy\")\n",
    "            \n",
    "            # Fallback strategy: return safe control action\n",
    "            return self._get_fallback_action(control_input)\n",
    "\n",
    "    # --- Helper methods for scaling and data management ---\n",
    "    def _get_real_history(self):\n",
    "        \"\"\"Get real historical data from buffer or startup generator.\n",
    "        \n",
    "        This method replaces the previous mock history generation with actual\n",
    "        trajectory data, critical for accurate model predictions in MPC.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (past_cmas_scaled, past_cpps_scaled) for model input\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If unable to provide sufficient historical data\n",
    "        \"\"\"\n",
    "        lookback = self.config['lookback']\n",
    "        \n",
    "        # Check if we have sufficient real data in buffer\n",
    "        if self.history_buffer.is_ready(lookback):\n",
    "            # Use real historical data\n",
    "            past_cmas_unscaled, past_cpps_unscaled = self.history_buffer.get_model_inputs(lookback)\n",
    "            \n",
    "            if self.config.get('verbose', False) and not self._initialization_complete:\n",
    "                print(\"Switched to real history data for MPC predictions\")\n",
    "                self._initialization_complete = True\n",
    "                \n",
    "        else:\n",
    "            # Use startup generator during initial operation\n",
    "            available_samples = len(self.history_buffer)\n",
    "            \n",
    "            if available_samples > 0:\n",
    "                # Partial real data available - combine with startup generation\n",
    "                real_cmas, real_cpps = self.history_buffer.get_model_inputs(available_samples)\n",
    "                \n",
    "                # Initialize startup generator if needed\n",
    "                if self.startup_generator is None:\n",
    "                    latest_cma, latest_cpp, _ = self.history_buffer.get_latest()\n",
    "                    if latest_cma is not None and latest_cpp is not None:\n",
    "                        self.startup_generator = StartupHistoryGenerator(\n",
    "                            cma_features=len(self.config['cma_names']),\n",
    "                            cpp_features=len(self.config['cpp_names']),\n",
    "                            initial_cma_state=latest_cma,\n",
    "                            initial_cpp_state=latest_cpp\n",
    "                        )\n",
    "                \n",
    "                # Generate startup history for missing samples\n",
    "                missing_samples = lookback - available_samples\n",
    "                startup_cmas, startup_cpps = self.startup_generator.generate_startup_history(missing_samples)\n",
    "                \n",
    "                # Combine startup and real data\n",
    "                past_cmas_unscaled = np.vstack([startup_cmas, real_cmas])\n",
    "                past_cpps_unscaled = np.vstack([startup_cpps, real_cpps])\n",
    "                \n",
    "            else:\n",
    "                # No real data yet - use pure startup generation\n",
    "                if self.startup_generator is None:\n",
    "                    # Create default startup generator with safe pharmaceutical baselines\n",
    "                    default_cma_state = np.array([450.0, 1.8])  # d50=450μm, LOD=1.8%\n",
    "                    default_cpp_state = np.array([130.0, 550.0, 30.0])  # Safe baselines\n",
    "                    \n",
    "                    # Trim to actual feature counts\n",
    "                    default_cma_state = default_cma_state[:len(self.config['cma_names'])]\n",
    "                    default_cpp_state = default_cpp_state[:len(self.config['cpp_names'])]\n",
    "                    \n",
    "                    self.startup_generator = StartupHistoryGenerator(\n",
    "                        cma_features=len(self.config['cma_names']),\n",
    "                        cpp_features=len(self.config['cpp_names']),\n",
    "                        initial_cma_state=default_cma_state,\n",
    "                        initial_cpp_state=default_cpp_state\n",
    "                    )\n",
    "                \n",
    "                past_cmas_unscaled, past_cpps_unscaled = self.startup_generator.generate_startup_history(lookback)\n",
    "                \n",
    "                if self.config.get('verbose', False):\n",
    "                    print(f\"Using startup history generation (lookback={lookback})\")\n",
    "        \n",
    "        # Scale both CMA and CPP historical data\n",
    "        past_cmas_scaled = self._scale_cma_plan(past_cmas_unscaled)\n",
    "        past_cpps_scaled = self._scale_cpp_plan(past_cpps_unscaled, with_soft_sensors=True)\n",
    "        \n",
    "        return past_cmas_scaled, past_cpps_scaled\n",
    "\n",
    "    def _scale_cpp_plan(self, plan_unscaled, with_soft_sensors=False):\n",
    "        \"\"\"Scale CPP control plan using fitted scalers, optionally adding soft sensors.\n",
    "        \n",
    "        Args:\n",
    "            plan_unscaled (np.ndarray): Unscaled CPP values, shape (horizon, num_cpps)\n",
    "            with_soft_sensors (bool): Whether to add soft sensor features\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Scaled CPP plan with optional soft sensors\n",
    "        \"\"\"\n",
    "        self._validate_scaling_inputs(plan_unscaled, \"CPP plan\", \"_scale_cpp_plan\")\n",
    "        \n",
    "        if plan_unscaled.ndim == 1:\n",
    "            plan_unscaled = plan_unscaled.reshape(1, -1)\n",
    "            \n",
    "        horizon, num_cpps = plan_unscaled.shape\n",
    "        \n",
    "        if with_soft_sensors:\n",
    "            # Create robust DataFrame-based soft sensor calculation\n",
    "            cpp_full_names = self.config['cpp_full_names']\n",
    "            \n",
    "            # Validate required variables for soft sensor calculations\n",
    "            required_base_vars = ['spray_rate', 'carousel_speed']\n",
    "            required_soft_vars = ['specific_energy', 'froude_number_proxy']\n",
    "            \n",
    "            missing_base = [var for var in required_base_vars if var not in cpp_full_names]\n",
    "            missing_soft = [var for var in required_soft_vars if var not in cpp_full_names]\n",
    "            \n",
    "            if missing_base:\n",
    "                raise ValueError(f\"Missing required base variables for soft sensors: {missing_base}\")\n",
    "            if missing_soft:\n",
    "                raise ValueError(f\"Missing required soft sensor variables: {missing_soft}\")\n",
    "            \n",
    "            # Initialize DataFrame with all cpp_full_names columns\n",
    "            plan_df = pd.DataFrame(\n",
    "                data=np.zeros((horizon, len(cpp_full_names))), \n",
    "                columns=cpp_full_names\n",
    "            )\n",
    "            \n",
    "            # Copy basic CPPs by name (robust to column order changes)\n",
    "            for i, cpp_name in enumerate(self.config['cpp_names']):\n",
    "                if i < num_cpps:\n",
    "                    if cpp_name not in cpp_full_names:\n",
    "                        raise ValueError(f\"CPP '{cpp_name}' not found in cpp_full_names\")\n",
    "                    plan_df[cpp_name] = plan_unscaled[:, i]\n",
    "            \n",
    "            # Calculate soft sensors using robust column-name-based approach\n",
    "            # CRITICAL: This approach is immune to column order changes\n",
    "            try:\n",
    "                # Specific energy: normalized spray rate × carousel speed interaction\n",
    "                plan_df['specific_energy'] = (plan_df['spray_rate'] * plan_df['carousel_speed']) / 1000.0\n",
    "                \n",
    "                # Froude number proxy: dimensionless mixing intensity measure  \n",
    "                plan_df['froude_number_proxy'] = (plan_df['carousel_speed'] ** 2) / 9.81\n",
    "                \n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Soft sensor calculation failed - missing column: {e}\")\n",
    "            \n",
    "            # Convert back to numpy array for scaling\n",
    "            plan_with_sensors = plan_df.values\n",
    "            \n",
    "            # Scale all features using fitted scalers\n",
    "            plan_scaled = np.zeros_like(plan_with_sensors)\n",
    "            for i, col_name in enumerate(cpp_full_names):\n",
    "                if col_name in self.scalers:\n",
    "                    # Reshape for scaler (expects 2D)\n",
    "                    col_data = plan_with_sensors[:, i].reshape(-1, 1)\n",
    "                    plan_scaled[:, i] = self.scalers[col_name].transform(col_data).flatten()\n",
    "                else:\n",
    "                    raise ValueError(f\"Scaler for '{col_name}' not found in scalers dict\")\n",
    "            \n",
    "            return plan_scaled\n",
    "        else:\n",
    "            # Scale basic CPPs only\n",
    "            plan_scaled = np.zeros_like(plan_unscaled)\n",
    "            for i, col_name in enumerate(self.config['cpp_names']):\n",
    "                if i < num_cpps and col_name in self.scalers:\n",
    "                    col_data = plan_unscaled[:, i].reshape(-1, 1)\n",
    "                    plan_scaled[:, i] = self.scalers[col_name].transform(col_data).flatten()\n",
    "                else:\n",
    "                    raise ValueError(f\"Scaler for '{col_name}' not found in scalers dict\")\n",
    "            \n",
    "            return plan_scaled\n",
    "\n",
    "    def _scale_cma_plan(self, plan_unscaled):\n",
    "        \"\"\"Scale CMA plan/target using fitted scalers.\n",
    "        \n",
    "        Args:\n",
    "            plan_unscaled (np.ndarray): Unscaled CMA values, shape (horizon, num_cmas) or (num_cmas,)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Scaled CMA plan with same shape as input\n",
    "        \"\"\"\n",
    "        self._validate_scaling_inputs(plan_unscaled, \"CMA plan\", \"_scale_cma_plan\")\n",
    "        \n",
    "        original_shape = plan_unscaled.shape\n",
    "        \n",
    "        # Handle both 1D vectors and 2D plans\n",
    "        if plan_unscaled.ndim == 1:\n",
    "            plan_unscaled = plan_unscaled.reshape(1, -1)\n",
    "            \n",
    "        horizon, num_cmas = plan_unscaled.shape\n",
    "        plan_scaled = np.zeros_like(plan_unscaled)\n",
    "        \n",
    "        # Scale each CMA column using fitted scalers\n",
    "        for i, col_name in enumerate(self.config['cma_names']):\n",
    "            if i < num_cmas and col_name in self.scalers:\n",
    "                # Reshape for scaler (expects 2D)\n",
    "                col_data = plan_unscaled[:, i].reshape(-1, 1)\n",
    "                plan_scaled[:, i] = self.scalers[col_name].transform(col_data).flatten()\n",
    "            else:\n",
    "                raise ValueError(f\"Scaler for CMA '{col_name}' not found in scalers dict\")\n",
    "        \n",
    "        # Return in original shape\n",
    "        if len(original_shape) == 1:\n",
    "            return plan_scaled.flatten()\n",
    "        else:\n",
    "            return plan_scaled\n",
    "\n",
    "    def _scale_cma_vector(self, vector_unscaled):\n",
    "        \"\"\"Scale single CMA vector using fitted scalers (for absolute values, not offsets).\n",
    "        \n",
    "        Args:\n",
    "            vector_unscaled (np.ndarray): Unscaled CMA vector, shape (num_cmas,)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Scaled CMA vector with same shape\n",
    "        \"\"\"\n",
    "        self._validate_scaling_inputs(vector_unscaled, \"CMA vector\", \"_scale_cma_vector\")\n",
    "        \n",
    "        if vector_unscaled.ndim != 1:\n",
    "            raise ValueError(f\"Expected 1D vector, got shape {vector_unscaled.shape}\")\n",
    "            \n",
    "        vector_scaled = np.zeros_like(vector_unscaled)\n",
    "        \n",
    "        # Scale each CMA element using fitted scalers\n",
    "        for i, col_name in enumerate(self.config['cma_names']):\n",
    "            if i < len(vector_unscaled) and col_name in self.scalers:\n",
    "                # Convert scalar to 2D array for scaler (expects 2D)\n",
    "                value_reshaped = np.array([[vector_unscaled[i]]])\n",
    "                vector_scaled[i] = self.scalers[col_name].transform(value_reshaped).flatten()[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Scaler for CMA '{col_name}' not found in scalers dict\")\n",
    "        \n",
    "        return vector_scaled\n",
    "\n",
    "    def _scale_cma_offset(self, offset_unscaled):\n",
    "        \"\"\"Scale CMA offset vector for integral action using only the scale factor.\n",
    "        \n",
    "        CRITICAL: Offsets represent corrections/disturbances, not absolute values.\n",
    "        Unlike absolute values, offsets should NOT be translated by the scaler's minimum.\n",
    "        \n",
    "        Mathematical Foundation:\n",
    "            - MinMaxScaler: scaled_value = (value - min) / (max - min)\n",
    "            - For offsets: scaled_offset = offset / (max - min)  [NO translation]\n",
    "            - This preserves the offset's zero-mean property and correct magnitude\n",
    "        \n",
    "        Args:\n",
    "            offset_unscaled (np.ndarray): Unscaled CMA offset vector, shape (num_cmas,)\n",
    "                Represents disturbance estimates or integral action corrections\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Correctly scaled offset vector with same shape\n",
    "            \n",
    "        Example:\n",
    "            >>> # Disturbance estimate of +50 μm in d50, +0.2% in LOD\n",
    "            >>> offset = np.array([50.0, 0.2])\n",
    "            >>> scaled_offset = controller._scale_cma_offset(offset)\n",
    "            >>> # Result preserves proportional correction without bias\n",
    "            \n",
    "        Notes:\n",
    "            - Essential for proper offset-free MPC functionality\n",
    "            - Incorrect scaling breaks integral action and causes steady-state error\n",
    "            - Only uses scaler.scale_ (1/(max-min)), not data_min_ translation\n",
    "        \"\"\"\n",
    "        self._validate_scaling_inputs(offset_unscaled, \"CMA offset\", \"_scale_cma_offset\")\n",
    "        \n",
    "        if offset_unscaled.ndim != 1:\n",
    "            raise ValueError(f\"Expected 1D offset vector, got shape {offset_unscaled.shape}\")\n",
    "            \n",
    "        offset_scaled = np.zeros_like(offset_unscaled)\n",
    "        \n",
    "        # Scale each CMA offset using only the range factor (no translation)\n",
    "        for i, col_name in enumerate(self.config['cma_names']):\n",
    "            if i < len(offset_unscaled) and col_name in self.scalers:\n",
    "                scaler = self.scalers[col_name]\n",
    "                # CRITICAL: Use only scale factor, no translation for offsets\n",
    "                # scale_[0] = 1 / (max - min) from the fitted MinMaxScaler\n",
    "                scale_factor = scaler.scale_[0]\n",
    "                offset_scaled[i] = offset_unscaled[i] * scale_factor\n",
    "            else:\n",
    "                raise ValueError(f\"Scaler for CMA '{col_name}' not found in scalers dict\")\n",
    "        \n",
    "        return offset_scaled\n",
    "\n",
    "    def _unscale_cpp_plan(self, plan_scaled):\n",
    "        \"\"\"Unscale CPP control plan from scaled space back to physical units.\n",
    "        \n",
    "        This method reverses the MinMaxScaler transformation to convert scaled\n",
    "        control plans (from GeneticOptimizer) back to physical units.\n",
    "        \n",
    "        Args:\n",
    "            plan_scaled (np.ndarray): Scaled CPP values, shape (horizon, num_cpps)\n",
    "                with values in [0,1] range from MinMaxScaler\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Unscaled CPP plan in physical units (g/min, m³/h, rpm, etc.)\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If scalers are missing for any CPP parameter\n",
    "            \n",
    "        Notes:\n",
    "            - Uses inverse_transform() method of fitted MinMaxScalers\n",
    "            - Essential for converting optimizer output to physical control actions\n",
    "            - Only handles basic CPPs, not soft sensors\n",
    "        \"\"\"\n",
    "        self._validate_scaling_inputs(plan_scaled, \"Scaled CPP plan\", \"_unscale_cpp_plan\")\n",
    "        \n",
    "        if plan_scaled.ndim == 1:\n",
    "            plan_scaled = plan_scaled.reshape(1, -1)\n",
    "            \n",
    "        horizon, num_cpps = plan_scaled.shape\n",
    "        plan_unscaled = np.zeros_like(plan_scaled)\n",
    "        \n",
    "        # Unscale each CPP column using fitted scalers\n",
    "        for i, col_name in enumerate(self.config['cpp_names']):\n",
    "            if i < num_cpps and col_name in self.scalers:\n",
    "                # Reshape for scaler (expects 2D)\n",
    "                col_data = plan_scaled[:, i].reshape(-1, 1)\n",
    "                plan_unscaled[:, i] = self.scalers[col_name].inverse_transform(col_data).flatten()\n",
    "            else:\n",
    "                raise ValueError(f\"Scaler for CPP '{col_name}' not found in scalers dict\")\n",
    "        \n",
    "        return plan_unscaled\n",
    "\n",
    "    def _get_param_bounds(self):\n",
    "        \"\"\"Get scaled parameter bounds for GeneticOptimizer.\n",
    "        \n",
    "        CRITICAL: Returns bounds in scaled space [0,1] to match the fitness function's\n",
    "        expectation that the optimizer provides scaled control plans.\n",
    "        \n",
    "        Returns:\n",
    "            list: Scaled parameter bounds [(min_scaled, max_scaled), ...] for each\n",
    "                parameter in the control horizon. All bounds are in [0,1] range.\n",
    "                \n",
    "        Raises:\n",
    "            ValueError: If scalers are missing for any CPP parameter\n",
    "            \n",
    "        Notes:\n",
    "            - GeneticOptimizer works entirely in scaled space for consistency\n",
    "            - Fitness function expects scaled control plans as input\n",
    "            - Bounds correspond to horizon × num_cpps parameters\n",
    "        \"\"\"\n",
    "        param_bounds = []\n",
    "        cpp_config = self.config['cpp_constraints']\n",
    "        \n",
    "        for _ in range(self.config['horizon']):\n",
    "            for name in self.config['cpp_names']:\n",
    "                if name not in self.scalers:\n",
    "                    raise ValueError(f\"Scaler for CPP '{name}' not found in scalers dict\")\n",
    "                \n",
    "                # Get unscaled constraint bounds\n",
    "                min_val = cpp_config[name]['min_val']\n",
    "                max_val = cpp_config[name]['max_val']\n",
    "                \n",
    "                # Transform to scaled space using fitted scaler\n",
    "                scaler = self.scalers[name]\n",
    "                min_scaled = scaler.transform([[min_val]])[0, 0]\n",
    "                max_scaled = scaler.transform([[max_val]])[0, 0]\n",
    "                \n",
    "                # Ensure proper ordering (min <= max)\n",
    "                if min_scaled > max_scaled:\n",
    "                    min_scaled, max_scaled = max_scaled, min_scaled\n",
    "                \n",
    "                param_bounds.append((min_scaled, max_scaled))\n",
    "                \n",
    "        return param_bounds\n",
    "    \n",
    "    def _calculate_safe_default_action(self):\n",
    "        \"\"\"Calculate safe default control action using constraint midpoints.\n",
    "        \n",
    "        This method provides guaranteed safe control values by using the midpoint\n",
    "        of each parameter's constraint bounds. These values are always within \n",
    "        operational limits and provide a conservative, stable operating point\n",
    "        for pharmaceutical process control.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Safe control action with constraint midpoint values\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If required constraints are missing or invalid\n",
    "            \n",
    "        Notes:\n",
    "            - Called during initialization to ensure safe fallback always available\n",
    "            - Uses constraint midpoints as conservative safe operating point\n",
    "            - Essential for pharmaceutical manufacturing safety during startup\n",
    "        \"\"\"\n",
    "        safe_action = np.zeros(len(self.config['cpp_names']))\n",
    "        cpp_config = self.config['cpp_constraints']\n",
    "        \n",
    "        for i, name in enumerate(self.config['cpp_names']):\n",
    "            if name in cpp_config:\n",
    "                min_val = cpp_config[name]['min_val']\n",
    "                max_val = cpp_config[name]['max_val']\n",
    "                \n",
    "                # Validate constraint bounds\n",
    "                if min_val >= max_val:\n",
    "                    raise ValueError(f\"Invalid constraint bounds for '{name}': min_val={min_val} >= max_val={max_val}\")\n",
    "                \n",
    "                # Use conservative midpoint as safe default\n",
    "                safe_action[i] = (min_val + max_val) / 2.0\n",
    "            else:\n",
    "                raise ValueError(f\"Missing constraint configuration for CPP '{name}' - required for safe fallback\")\n",
    "                \n",
    "        return safe_action\n",
    "    \n",
    "    def _should_reset_optimizer(self, current_setpoint):\n",
    "        \"\"\"Determine if optimizer should be reset due to significant setpoint change.\n",
    "        \n",
    "        Detects when setpoint changes are large enough to warrant fresh GA population\n",
    "        exploration rather than continuing with existing population bias.\n",
    "        \n",
    "        Args:\n",
    "            current_setpoint (np.ndarray): Current target setpoint values\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if optimizer should be reset for fresh exploration\n",
    "            \n",
    "        Notes:\n",
    "            - Uses configurable threshold for change detection\n",
    "            - Supports both absolute and relative change metrics\n",
    "            - Essential for pharmaceutical grade transitions\n",
    "        \"\"\"\n",
    "        # Skip reset if feature disabled\n",
    "        if not self.config.get('reset_optimizer_on_setpoint_change', True):\n",
    "            return False\n",
    "            \n",
    "        # Always reset on first setpoint (no previous reference)\n",
    "        if self._last_setpoint is None:\n",
    "            return False  # Don't reset on very first call\n",
    "            \n",
    "        # Calculate setpoint change magnitude\n",
    "        setpoint_change = np.abs(current_setpoint - self._last_setpoint)\n",
    "        \n",
    "        # Get configured threshold (default: 5% relative change)\n",
    "        threshold = self.config.get('setpoint_change_threshold', 0.05)\n",
    "        \n",
    "        # Use relative change detection for pharmaceutical applications\n",
    "        if np.any(setpoint_change / (np.abs(self._last_setpoint) + 1e-6) > threshold):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _reset_optimizer(self):\n",
    "        \"\"\"Reset optimizer to fresh state for new exploration.\n",
    "        \n",
    "        Reinitializes the genetic algorithm optimizer with fresh random population\n",
    "        to avoid population bias when setpoints change significantly.\n",
    "        \n",
    "        Notes:\n",
    "            - Called automatically on significant setpoint changes\n",
    "            - Maintains same parameter bounds and GA configuration\n",
    "            - Essential for pharmaceutical grade transition control\n",
    "        \"\"\"\n",
    "        if self.optimizer is not None and self.optimizer_class is not None:\n",
    "            # Reinitialize with same bounds and configuration\n",
    "            param_bounds = self._get_param_bounds()\n",
    "            ga_config = self.config['ga_config'].copy()\n",
    "            ga_config['horizon'] = self.config['horizon']\n",
    "            ga_config['num_cpps'] = len(self.config['cpp_names'])\n",
    "            self.optimizer = self.optimizer_class(param_bounds, ga_config)\n",
    "    \n",
    "    def _get_fallback_action(self, current_control_input):\n",
    "        \"\"\"Get safe fallback control action when optimizer fails.\n",
    "        \n",
    "        Implements multiple fallback strategies in order of preference:\n",
    "        1. Last successful optimization result\n",
    "        2. Hold current control input (if valid)\n",
    "        3. Safe default control values\n",
    "        \n",
    "        Args:\n",
    "            current_control_input: Current control input values\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Safe control action\n",
    "        \"\"\"\n",
    "        # Strategy 1: Use last successful optimization result\n",
    "        if self._last_successful_action is not None:\n",
    "            if self._validate_control_action(self._last_successful_action):\n",
    "                return self._last_successful_action.copy()\n",
    "        \n",
    "        # Strategy 2: Hold current control input (if valid)\n",
    "        if current_control_input is not None:\n",
    "            if self._validate_control_action(current_control_input):\n",
    "                return current_control_input.copy()\n",
    "        \n",
    "        # Strategy 3: Use pre-calculated safe default control values\n",
    "        return self._calculate_safe_default_action()\n",
    "    \n",
    "    def _validate_control_action(self, action):\n",
    "        \"\"\"Validate that control action satisfies constraints.\n",
    "        \n",
    "        Args:\n",
    "            action: Control action to validate\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if action is valid and safe\n",
    "        \"\"\"\n",
    "        if action is None or not isinstance(action, np.ndarray):\n",
    "            return False\n",
    "            \n",
    "        if action.size != len(self.config['cpp_names']):\n",
    "            return False\n",
    "            \n",
    "        if not np.all(np.isfinite(action)):\n",
    "            return False\n",
    "            \n",
    "        # Check constraint bounds\n",
    "        cpp_config = self.config['cpp_constraints']\n",
    "        for i, name in enumerate(self.config['cpp_names']):\n",
    "            if name in cpp_config:\n",
    "                min_val = cpp_config[name]['min_val']\n",
    "                max_val = cpp_config[name]['max_val']\n",
    "                if not (min_val <= action[i] <= max_val):\n",
    "                    return False\n",
    "                    \n",
    "        return True\n",
    "\n",
    "    def _validate_initialization(self):\n",
    "        \"\"\"Validate controller configuration and scalers during initialization.\"\"\"\n",
    "        # Check required configuration keys\n",
    "        required_keys = ['cma_names', 'cpp_names', 'cpp_full_names', 'horizon', 'lookback']\n",
    "        for key in required_keys:\n",
    "            if key not in self.config:\n",
    "                raise ValueError(f\"Missing required configuration key: '{key}'\")\n",
    "        \n",
    "        # Validate CMA configuration\n",
    "        if not isinstance(self.config['cma_names'], list) or len(self.config['cma_names']) == 0:\n",
    "            raise ValueError(\"'cma_names' must be a non-empty list\")\n",
    "            \n",
    "        # Validate CPP configuration\n",
    "        if not isinstance(self.config['cpp_names'], list) or len(self.config['cpp_names']) == 0:\n",
    "            raise ValueError(\"'cpp_names' must be a non-empty list\")\n",
    "            \n",
    "        if not isinstance(self.config['cpp_full_names'], list) or len(self.config['cpp_full_names']) == 0:\n",
    "            raise ValueError(\"'cpp_full_names' must be a non-empty list\")\n",
    "        \n",
    "        # Validate that all basic CPPs are included in full CPP list\n",
    "        for cpp_name in self.config['cpp_names']:\n",
    "            if cpp_name not in self.config['cpp_full_names']:\n",
    "                raise ValueError(f\"CPP '{cpp_name}' not found in cpp_full_names\")\n",
    "        \n",
    "        # Validate soft sensor configuration for robust pharmaceutical control\n",
    "        required_soft_sensor_base = ['spray_rate', 'carousel_speed']\n",
    "        required_soft_sensors = ['specific_energy', 'froude_number_proxy']\n",
    "        \n",
    "        missing_soft_base = [var for var in required_soft_sensor_base if var not in self.config['cpp_full_names']]\n",
    "        missing_soft_sensors = [var for var in required_soft_sensors if var not in self.config['cpp_full_names']]\n",
    "        \n",
    "        if missing_soft_base:\n",
    "            raise ValueError(f\"Missing required base variables for soft sensor calculations: {missing_soft_base}\")\n",
    "        if missing_soft_sensors:\n",
    "            raise ValueError(f\"Missing required soft sensor variables in cpp_full_names: {missing_soft_sensors}\")\n",
    "        \n",
    "        # Validate horizon and lookback\n",
    "        if self.config['horizon'] <= 0:\n",
    "            raise ValueError(\"'horizon' must be positive\")\n",
    "        if self.config['lookback'] <= 0:\n",
    "            raise ValueError(\"'lookback' must be positive\")\n",
    "            \n",
    "        # Validate scalers availability\n",
    "        if not isinstance(self.scalers, dict):\n",
    "            raise ValueError(\"'scalers' must be a dictionary\")\n",
    "            \n",
    "        # Check that all required scalers are available\n",
    "        required_scalers = self.config['cma_names'] + self.config['cpp_full_names']\n",
    "        missing_scalers = []\n",
    "        \n",
    "        for scaler_name in required_scalers:\n",
    "            if scaler_name not in self.scalers:\n",
    "                missing_scalers.append(scaler_name)\n",
    "                \n",
    "        if missing_scalers:\n",
    "            raise ValueError(f\"Missing scalers for: {missing_scalers}\")\n",
    "            \n",
    "        # Validate that scalers have required methods\n",
    "        for scaler_name, scaler in self.scalers.items():\n",
    "            if not hasattr(scaler, 'transform'):\n",
    "                raise ValueError(f\"Scaler for '{scaler_name}' missing 'transform' method\")\n",
    "        \n",
    "        # Validate optimizer reset configuration parameters\n",
    "        if 'setpoint_change_threshold' in self.config:\n",
    "            threshold = self.config['setpoint_change_threshold']\n",
    "            if not isinstance(threshold, (int, float)) or threshold <= 0:\n",
    "                raise ValueError(\"'setpoint_change_threshold' must be a positive number\")\n",
    "        \n",
    "        if 'reset_optimizer_on_setpoint_change' in self.config:\n",
    "            reset_flag = self.config['reset_optimizer_on_setpoint_change']\n",
    "            if not isinstance(reset_flag, bool):\n",
    "                raise ValueError(\"'reset_optimizer_on_setpoint_change' must be a boolean\")\n",
    "                \n",
    "        if self.config.get('verbose', False):\n",
    "            print(\"RobustMPCController validation passed\")\n",
    "            print(f\"  - CMAs: {self.config['cma_names']}\")\n",
    "            print(f\"  - CPPs: {self.config['cpp_names']}\")  \n",
    "            print(f\"  - Full CPPs: {self.config['cpp_full_names']}\")\n",
    "            print(f\"  - Horizon: {self.config['horizon']}, Lookback: {self.config['lookback']}\")\n",
    "            print(f\"  - Available scalers: {len(self.scalers)}\")\n",
    "\n",
    "    def _validate_scaling_inputs(self, data, expected_shape_desc, method_name):\n",
    "        \"\"\"Validate inputs to scaling methods.\"\"\"\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            raise TypeError(f\"{method_name}: Expected numpy array, got {type(data)}\")\n",
    "            \n",
    "        if data.size == 0:\n",
    "            raise ValueError(f\"{method_name}: Input array is empty\")\n",
    "            \n",
    "        if not np.all(np.isfinite(data)):\n",
    "            raise ValueError(f\"{method_name}: Input contains non-finite values (NaN/inf)\")\n",
    "            \n",
    "        if self.config.get('verbose', False):\n",
    "            print(f\"{method_name} input validation passed: shape {data.shape} ({expected_shape_desc})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Standalone Test of the Integrated Controller\n",
    "\n",
    "Before the final showdown in Notebook 5, let's do a quick standalone test of the new `RobustMPCController` class. This will ensure all the components are communicating correctly. We will mock the necessary inputs and call the main `.suggest_action()` method to see if it produces a sensible result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustMPCController class instantiated successfully!\n",
      "Note: The helper methods for scaling and history need to be fully implemented for the final test.\n"
     ]
    }
   ],
   "source": [
    "# This cell is for testing the class logic. A full implementation requires all components.\n",
    "# Due to the complexity of mocking all inputs (history, scalers, etc.), a full test is deferred to Notebook 5.\n",
    "# Here we will just instantiate the class to check for syntax errors.\n",
    "\n",
    "from V2.robust_mpc.core import RobustMPCController\n",
    "from V2.robust_mpc.estimators import KalmanStateEstimator\n",
    "from V2.robust_mpc.models import ProbabilisticTransformer\n",
    "from V2.robust_mpc.optimizers import GeneticOptimizer\n",
    "\n",
    "# --- Mock Components and Configs (for instantiation) ---\n",
    "mock_model = ProbabilisticTransformer(cma_features=2, cpp_features=5, d_model=32, nhead=2)\n",
    "mock_estimator = 'KalmanFilterPlaceholder'\n",
    "mock_optimizer_class = GeneticOptimizer\n",
    "mock_scalers = 'ScalersPlaceholder'\n",
    "\n",
    "MPC_CONFIG_V2 = {\n",
    "    'lookback': 36,\n",
    "    'horizon': 10, # Shorter for faster testing\n",
    "    'cma_names': ['d50', 'lod'],\n",
    "    'cpp_names': ['spray_rate', 'air_flow', 'carousel_speed'],\n",
    "    'cpp_full_names': ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy'],\n",
    "    'integral_gain': 0.1,\n",
    "    'risk_beta': 1.5,\n",
    "    'mc_samples': 10,\n",
    "    'cpp_constraints': {\n",
    "        'spray_rate': {'min_val': 80.0, 'max_val': 180.0},\n",
    "        'air_flow': {'min_val': 400.0, 'max_val': 700.0},\n",
    "        'carousel_speed': {'min_val': 20.0, 'max_val': 40.0}\n",
    "    },\n",
    "    'ga_config': {\n",
    "        'population_size': 40,\n",
    "        'num_generations': 15,\n",
    "        'crossover_prob': 0.7,\n",
    "        'mutation_prob': 0.2,\n",
    "        'horizon': 10, # Must match outer horizon\n",
    "        'num_cpps': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    controller = RobustMPCController(\n",
    "        model=mock_model,\n",
    "        estimator=mock_estimator,\n",
    "        optimizer_class=mock_optimizer_class,\n",
    "        config=MPC_CONFIG_V2,\n",
    "        scalers=mock_scalers\n",
    "    )\n",
    "    print(\"RobustMPCController class instantiated successfully!\")\n",
    "    print(\"Note: The helper methods for scaling and history need to be fully implemented for the final test.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during instantiation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis and Next Steps\n",
    "\n",
    "We have successfully designed and implemented the architecture for our advanced `RobustMPCController`. This new core brings together all the state-of-the-art components we've developed in this V2 series. While we have only performed a basic instantiation test here, we have laid out the complete logic that will be put to the test in our final notebook.\n",
    "\n",
    "The key takeaways are:\n",
    "*   **Modular Design:** The controller is built to accept different estimators, models, and optimizers, making it highly flexible.\n",
    "*   **Integral Action:** The logic for `disturbance_estimate` is in place, ready to eliminate steady-state error.\n",
    "*   **Risk-Awareness:** The fitness function is designed to use the probabilistic output of our model, making the controller inherently safer.\n",
    "\n",
    "In the final notebook, we will implement the missing helper methods, connect this controller to our plant, and perform a head-to-head showdown against our V1 controller to definitively prove its superior performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
