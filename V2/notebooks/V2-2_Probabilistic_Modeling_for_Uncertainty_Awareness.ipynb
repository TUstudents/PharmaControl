{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Notebook 2: Probabilistic Modeling for Uncertainty-Awareness\n",
    "\n",
    "**Project:** `RobustMPC-Pharma` (V2)\n",
    "**Goal:** Upgrade our predictive model to be uncertainty-aware. We will build a **probabilistic model** that outputs not just a single prediction, but a full predictive distribution (mean and standard deviation). This allows the controller to know how confident the model is, enabling safer, risk-adjusted decision-making.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Theory: The Importance of Knowing What You Don't Know](#1.-Theory:-The-Importance-of-Knowing-What-You-Don't-Know)\n",
    "2. [Method: Probabilistic Forecasting with Monte Carlo Dropout](#2.-Method:-Probabilistic-Forecasting-with-Monte-Carlo-Dropout)\n",
    "3. [Implementing the `ProbabilisticTransformer`](#3.-Implementing-the-ProbabilisticTransformer)\n",
    "4. [Training the Probabilistic Model](#4.-Training-the-Probabilistic-Model)\n",
    "5. [Validation: Visualizing Predictive Uncertainty](#5.-Validation:-Visualizing-Predictive-Uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Theory: The Importance of Knowing What You Don't Know\n",
    "\n",
    "A standard neural network provides a **point forecast**â€”a single best guess for the future. This is useful, but it can be dangerously overconfident. The model has no mechanism to communicate when it is operating in a region of high uncertainty.\n",
    "\n",
    "We care about two main types of uncertainty:\n",
    "\n",
    "*   **Aleatoric Uncertainty:** Inherent randomness or noise in the process itself. This is irreducible noise that the model can learn to represent.\n",
    "*   **Epistemic Uncertainty:** Uncertainty due to a lack of knowledge. This happens when the model encounters data that is very different from what it saw during training (out-of-distribution data). This is the uncertainty in the model's parameters.\n",
    "\n",
    "A **probabilistic model** captures both. It outputs a probability distribution (e.g., a mean and a standard deviation) for its prediction. A controller using this model can then make more robust decisions. For example, if the model is very uncertain (high standard deviation), the controller might choose a more conservative, safer action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Method: Probabilistic Forecasting with Monte Carlo Dropout\n",
    "\n",
    "There are many ways to create probabilistic neural networks. One of the simplest and most effective techniques is **Monte Carlo (MC) Dropout**.\n",
    "\n",
    "The core idea is surprisingly simple:\n",
    "1.  Train a standard neural network that contains **Dropout layers**.\n",
    "2.  At inference time, instead of turning off Dropout (which is the standard `model.eval()` behavior), we **keep it active**.\n",
    "3.  Because Dropout randomly deactivates neurons, each forward pass through the network will produce a slightly different result.\n",
    "4.  We perform `N` forward passes on the same input data. This gives us a sample of `N` different predictions.\n",
    "5.  The **mean** of this sample serves as our final prediction.\n",
    "6.  The **standard deviation** of this sample serves as our measure of model uncertainty.\n",
    "\n",
    "This technique effectively treats our single trained network as an ensemble of many smaller, slightly different networks, providing a robust way to estimate uncertainty without changing the training process significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "# Add V1 project to path to reuse its dataset class\n",
    "V1_PROJECT_PATH = '../PharmaControl-Pro/src/'\n",
    "sys.path.append(os.path.abspath(V1_PROJECT_PATH))\n",
    "from dataset import GranulationDataset\n",
    "\n",
    "# Import the V1 model architecture as a base\n",
    "from model_architecture import GranulationPredictor, PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Implementing the `ProbabilisticTransformer`\n",
    "\n",
    "We will now create our new probabilistic model in `src/models.py`. The architecture will be identical to our V1 `GranulationPredictor`. The key difference is the addition of a `predict_distribution` method that implements the MC Dropout logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../robust_mpc/models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# We can reuse the PositionalEncoding class from V1\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ProbabilisticTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model that supports probabilistic forecasting via MC Dropout.\n",
    "    The architecture is identical to the V1 predictor, but with added methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, cma_features, cpp_features, d_model=64, nhead=4, \n",
    "                 num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cma_features = cma_features\n",
    "        self.cpp_features = cpp_features\n",
    "\n",
    "        # --- Layers (identical to V1 model) ---\n",
    "        self.cma_encoder_embedding = nn.Linear(cma_features, d_model)\n",
    "        self.cpp_encoder_embedding = nn.Linear(cpp_features, d_model)\n",
    "        self.cpp_decoder_embedding = nn.Linear(cpp_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.output_linear = nn.Linear(d_model, cma_features)\n",
    "\n",
    "    def forward(self, past_cmas, past_cpps, future_cpps):\n",
    "        past_cma_emb = self.cma_encoder_embedding(past_cmas)\n",
    "        past_cpp_emb = self.cpp_encoder_embedding(past_cpps)\n",
    "        src = self.pos_encoder(past_cma_emb + past_cpp_emb)\n",
    "        tgt = self.pos_encoder(self.cpp_decoder_embedding(future_cpps))\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.output_linear(output)\n",
    "\n",
    "    def predict_distribution(self, past_cmas, past_cpps, future_cpps, n_samples=30):\n",
    "        \"\"\"\n",
    "        Performs probabilistic forecasting using Monte Carlo Dropout.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (mean_prediction, std_prediction)\n",
    "        \"\"\"\n",
    "        # Set to evaluation mode BUT keep dropout layers active.\n",
    "        # This is done by activating training mode only for dropout layers.\n",
    "        self.train()\n",
    "        # An alternative, cleaner way is to create a custom method to only turn on dropout.\n",
    "        # For simplicity, we use train() as it activates dropout.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Collect multiple predictions\n",
    "            predictions = [self.forward(past_cmas, past_cpps, future_cpps) for _ in range(n_samples)]\n",
    "            \n",
    "            # Stack predictions into a new dimension for calculation\n",
    "            # Shape: (n_samples, batch_size, horizon, features)\n",
    "            predictions_stacked = torch.stack(predictions)\n",
    "            \n",
    "            # Calculate mean and standard deviation across the samples dimension\n",
    "            mean_prediction = torch.mean(predictions_stacked, dim=0)\n",
    "            std_prediction = torch.std(predictions_stacked, dim=0)\n",
    "            \n",
    "        # Return model to standard evaluation mode\n",
    "        self.eval()\n",
    "        \n",
    "        return mean_prediction, std_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Training the Probabilistic Model\n",
    "\n",
    "One of the biggest advantages of MC Dropout is that the training process remains exactly the same. We train the network with Dropout as a regularization technique, just as we did in V1. The probabilistic nature only comes into play during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section reuses the training logic from V1's Notebook 3.\n",
    "# For brevity, we will assume it has been run and we are just loading a pre-trained model.\n",
    "# In a full run, you would re-train using this new model class.\n",
    "from robust_mpc.models import ProbabilisticTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "MODEL_SAVE_PATH = os.path.join(DATA_DIR, 'probabilistic_model.pth')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define column groups and config\n",
    "CMA_COLS = ['d50', 'lod']\n",
    "CPP_COLS_FULL = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
    "LOOKBACK = 36\n",
    "HORIZON = 72\n",
    "\n",
    "# For demonstration, we will load the weights from the V1 model training.\n",
    "# The architectures are identical, so this is valid.\n",
    "V1_MODEL_PATH = '../PharmaControl-Pro/data/best_predictor_model.pth'\n",
    "BEST_HPARAMS = {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.15}\n",
    "\n",
    "prob_model = ProbabilisticTransformer(\n",
    "    cma_features=len(CMA_COLS),\n",
    "    cpp_features=len(CPP_COLS_FULL),\n",
    "    **BEST_HPARAMS\n",
    ").to(DEVICE)\n",
    "\n",
    "if os.path.exists(V1_MODEL_PATH):\n",
    "    print(\"Loading pre-trained weights from V1 model...\")\n",
    "    prob_model.load_state_dict(torch.load(V1_MODEL_PATH, map_location=DEVICE))\n",
    "    # Save it under the new name for V2\n",
    "    torch.save(prob_model.state_dict(), MODEL_SAVE_PATH)\n",
    "else:\n",
    "    print(\"Pre-trained model not found. Please run V1 training first or retrain here.\")\n",
    "    # Add the full training loop here if needed.\n",
    "\n",
    "print(\"Probabilistic model is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Validation: Visualizing Predictive Uncertainty\n",
    "\n",
    "Now for the most important part: validating our new model's ability to quantify uncertainty. We will take a sample from the test set and use the `predict_distribution` method to get both the mean and standard deviation. We will then plot these results to see how the model's confidence changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Test Data ---\n",
    "df_test = pd.read_csv(os.path.join(DATA_DIR, 'test_data.csv'))\n",
    "scalers = joblib.load(os.path.join(DATA_DIR, 'scalers.joblib'))\n",
    "test_dataset = GranulationDataset(df_test, CMA_COLS, CPP_COLS_FULL, LOOKBACK, HORIZON)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get one sample from the test set\n",
    "past_cmas, past_cpps, future_cpps, future_cmas_target = next(iter(test_loader))\n",
    "past_cmas, past_cpps, future_cpps = [b.to(DEVICE) for b in [past_cmas, past_cpps, future_cpps]]\n",
    "\n",
    "# --- Get Probabilistic Prediction ---\n",
    "mean_pred_scaled, std_pred_scaled = prob_model.predict_distribution(past_cmas, past_cpps, future_cpps, n_samples=50)\n",
    "\n",
    "# Move to CPU and detach for numpy/plotting\n",
    "mean_pred_scaled = mean_pred_scaled.squeeze(0).cpu().numpy()\n",
    "std_pred_scaled = std_pred_scaled.squeeze(0).cpu().numpy()\n",
    "target_scaled = future_cmas_target.squeeze(0).cpu().numpy()\n",
    "\n",
    "# --- Inverse Transform to Original Scale ---\n",
    "# Helper function for clarity\n",
    "def unscale_cma(scaled_data, scalers_dict, cma_names):\n",
    "    unscaled = np.zeros_like(scaled_data)\n",
    "    for i, name in enumerate(cma_names):\n",
    "        unscaled[:, i] = scalers_dict[name].inverse_transform(scaled_data[:, i].reshape(-1, 1)).flatten()\n",
    "    return unscaled\n",
    "\n",
    "mean_pred_unscaled = unscale_cma(mean_pred_scaled, scalers, CMA_COLS)\n",
    "target_unscaled = unscale_cma(target_scaled, scalers, CMA_COLS)\n",
    "\n",
    "# For the standard deviation, we need to be careful. We unscale the upper and lower bounds.\n",
    "upper_bound_scaled = mean_pred_scaled + 2 * std_pred_scaled\n",
    "lower_bound_scaled = mean_pred_scaled - 2 * std_pred_scaled\n",
    "upper_bound_unscaled = unscale_cma(upper_bound_scaled, scalers, CMA_COLS)\n",
    "lower_bound_unscaled = unscale_cma(lower_bound_scaled, scalers, CMA_COLS)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axes = plt.subplots(len(CMA_COLS), 1, figsize=(18, 10), sharex=True)\n",
    "fig.suptitle('Probabilistic Forecast with Uncertainty Bounds (MC Dropout)', fontsize=18)\n",
    "\n",
    "for i, col in enumerate(CMA_COLS):\n",
    "    ax = axes[i]\n",
    "    ax.plot(target_unscaled[:, i], label='Ground Truth', color='blue', linestyle='--', linewidth=2)\n",
    "    ax.plot(mean_pred_unscaled[:, i], label='Mean Prediction', color='red', linewidth=2.5)\n",
    "    \n",
    "    # Fill the area between the upper and lower confidence bounds\n",
    "    ax.fill_between(\n",
    "        np.arange(HORIZON),\n",
    "        lower_bound_unscaled[:, i],\n",
    "        upper_bound_unscaled[:, i],\n",
    "        color='red', alpha=0.2, label='95% Confidence Interval (Â±2Ïƒ)'\n",
    "    )\n",
    "    \n",
    "    ax.set_ylabel(col, fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=':')\n",
    "\n",
    "axes[-1].set_xlabel('Time Steps into Horizon', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis\n",
    "\n",
    "The visualization is key. The plot shows not only the model's best guess (the red line) but also its confidence (the shaded red area). Notice how the uncertainty band might be narrower at the beginning of the horizon and tends to widen for predictions further into the future. This is intuitive: it's harder to be certain about events far away.\n",
    "\n",
    "With this `ProbabilisticTransformer`, we now have a much more powerful tool. In Notebook 4, we will show how the `RobustMPCController` can leverage this uncertainty information to make safer and more reliable decisions. We have successfully built the second major component of our V2 framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}