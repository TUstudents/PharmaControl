{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Phase 2 checkpoint decision\nprint(\"Phase 2 Checkpoint: Multi-Scenario Testing Framework Complete\")\nprint(\"=\" * 59)\n\nprint(\"‚úÖ PHASE 2 FRAMEWORK COMPLETE:\")\nprint(\"   ‚úì 5 comprehensive test scenarios designed (A-E)\")\nprint(\"   ‚úì Statistical testing parameters configured\")\nprint(\"   ‚úì Performance metrics collection framework implemented\")\nprint(\"   ‚úì Comprehensive execution and analysis functions ready\")\n\ntotal_planned_tests = len(test_scenarios) * stats_params['num_runs_per_scenario'] * len(stats_params['noise_levels']) * 2\nestimated_minutes = total_planned_tests * 2 / 60\n\nprint(f\"\\\\nüìä TEST SUITE SPECIFICATIONS:\")\nprint(f\"   Total planned tests: {total_planned_tests}\")\nprint(f\"   Estimated execution time: {estimated_minutes:.0f} minutes\")\nprint(f\"   Controllers: V1 (Grid Search) vs V2 (Genetic Algorithm)\")\nprint(f\"   Scenarios: Standard, High Quality, Low Quality, Disturbance, Boundary\")\n\nprint(f\"\\\\nüéØ NEXT PHASE OPTIONS:\")\nprint(f\"   Option A: Execute comprehensive comparison now (Phase 2 execution)\")\nprint(f\"   Option B: Proceed to Phase 3 (Advanced feature analysis framework)\")\nprint(f\"   Option C: Execute smaller subset for verification first\")\n\n# Save Phase 2 framework completion\nphase2_framework_complete = {\n    'timestamp': datetime.now().isoformat(),\n    'scenarios_designed': len(test_scenarios),\n    'framework_ready': True,\n    'estimated_test_duration_minutes': estimated_minutes,\n    'total_planned_tests': total_planned_tests,\n    'next_phase_options': ['Execute Phase 2', 'Proceed to Phase 3', 'Verification subset']\n}\n\nwith open(RESULTS_PATH / \"phase2_framework_complete.json\", 'w') as f:\n    json.dump(phase2_framework_complete, f, indent=2, default=str)\n\nprint(f\"\\\\nüìã Phase 2 framework status saved to: {RESULTS_PATH / 'phase2_framework_complete.json'}\")\nprint(f\"üöÄ Framework ready for execution or Phase 3 development\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 2 Checkpoint: Ready for Full Comparison Execution?\n\n**Decision Point**: The comprehensive multi-scenario testing framework is complete. Ready to execute the full comparison test suite or proceed to Phase 3 advanced feature analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def execute_comprehensive_comparison():\n    \"\"\"Execute comprehensive V1 vs V2 controller comparison across all scenarios.\n    \n    This is the main execution function that runs all tests and collects results.\n    \"\"\"\n    \n    print(\"Phase 2.3: Comprehensive V1 vs V2 Controller Comparison\")\n    print(\"=\" * 58)\n    \n    # Check if controllers are ready from Phase 1\n    if not proceed_to_phase2:\n        print(\"‚ùå Cannot proceed: Phase 1 validation failed\")\n        print(\"   Please resolve baseline validation issues before running comparison\")\n        return None\n    \n    print(\"‚úÖ Phase 1 validation passed - proceeding with comprehensive comparison\")\n    print(\"\\\\nExecution Plan:\")\n    print(f\"   Scenarios: {list(test_scenarios.keys())} ({len(test_scenarios)} total)\")\n    print(f\"   Runs per scenario: {stats_params['num_runs_per_scenario']}\")\n    print(f\"   Noise levels: {stats_params['noise_levels']}\")\n    print(f\"   Controllers: V1 (Grid Search) vs V2 (Genetic Algorithm)\")\n    \n    # Initialize results storage\n    all_scenario_results = {}\\n    comparison_start_time = time.time()\\n    \\n    try:\\n        # Execute each scenario\\n        for scenario_key, scenario_config in test_scenarios.items():\\n            print(f\"\\\\n{'='*60}\")\n            print(f\"EXECUTING SCENARIO {scenario_key}: {scenario_config['name']}\")\n            print(f\"{'='*60}\")\n            \\n            scenario_start = time.time()\\n            \\n            # Collect metrics for this scenario\\n            scenario_results = metrics_collector.collect_scenario_metrics(\\n                scenario_key, scenario_config, stats_params\\n            )\\n            \\n            all_scenario_results[scenario_key] = scenario_results\\n            scenario_duration = time.time() - scenario_start\\n            \\n            print(f\"\\\\n‚úÖ Scenario {scenario_key} completed in {scenario_duration:.1f}s\")\\n            \\n            # Scenario-level summary\\n            v1_success_rate = np.mean([m['optimization_success'] for m in scenario_results['v1']])\\n            v2_success_rate = np.mean([m['optimization_success'] for m in scenario_results['v2']])\\n            \\n            v1_mean_time = np.mean([m['computation_time'] for m in scenario_results['v1']])\\n            v2_mean_time = np.mean([m['computation_time'] for m in scenario_results['v2']])\\n            \\n            print(f\"   üìä Scenario Summary:\")\\n            print(f\"      V1 Success: {v1_success_rate:.1%}, Avg Time: {v1_mean_time:.3f}s\")\\n            print(f\"      V2 Success: {v2_success_rate:.1%}, Avg Time: {v2_mean_time:.3f}s\")\\n    \\n    except KeyboardInterrupt:\\n        print(f\"\\\\n‚ö†Ô∏è  Comparison interrupted by user\")\\n        print(f\"   Partial results collected for scenarios: {list(all_scenario_results.keys())}\")\\n    \\n    except Exception as e:\\n        print(f\"\\\\n‚ùå Comparison failed: {e}\")\\n        import traceback\\n        traceback.print_exc()\\n    \\n    comparison_duration = time.time() - comparison_start_time\\n    \\n    print(f\"\\\\n{'='*60}\")\n    print(f\"COMPREHENSIVE COMPARISON COMPLETE\")\n    print(f\"{'='*60}\")\n    print(f\"Total duration: {comparison_duration:.1f}s ({comparison_duration/60:.1f} minutes)\")\n    print(f\"Total tests executed: {metrics_collector.test_count}\")\n    \\n    return all_scenario_results\\n\\ndef analyze_comparison_results(scenario_results):\\n    \\\"\\\"\\\"Perform statistical analysis of V1 vs V2 comparison results.\\\"\\\"\\\"\\n    \\n    print(f\\\"\\\\nStatistical Analysis of V1 vs V2 Comparison Results\\\")\\n    print(f\\\"=\\\" * 52)\\n    \\n    # Get overall summary statistics\\n    summary = metrics_collector.get_summary_statistics()\\n    \\n    print(f\\\"\\\\nüìä OVERALL PERFORMANCE SUMMARY:\\\")\\n    print(f\\\"=\\\" * 33)\\n    \\n    for controller in ['v1', 'v2']:\\n        if 'error' not in summary[controller]:\\n            stats = summary[controller]\\n            print(f\\\"\\\\n{controller.upper()} Controller ({controller.upper()} Optimization):\\\")\\n            print(f\\\"   Tests completed: {stats['total_tests']}\\\")\\n            print(f\\\"   Success rate: {stats['success_rate']:.1%}\\\")\\n            print(f\\\"   Mean computation time: {stats['mean_computation_time']:.3f}s ¬± {stats['std_computation_time']:.3f}s\\\")\\n            print(f\\\"   Mean setpoint distance: {stats['mean_setpoint_distance']:.2f} ¬± {stats['std_setpoint_distance']:.2f}\\\")\\n            print(f\\\"   Constraint violation rate: {stats['constraint_violation_rate']:.1%}\\\")\\n        else:\\n            print(f\\\"\\\\n{controller.upper()} Controller: {summary[controller]['error']}\\\")\\n    \\n    # Statistical significance testing\\n    if summary['v1']['total_tests'] > 0 and summary['v2']['total_tests'] > 0:\\n        from scipy import stats as scipy_stats\\n        \\n        print(f\\\"\\\\nüìà STATISTICAL SIGNIFICANCE TESTING:\\\")\\n        print(f\\\"=\\\" * 36)\\n        \\n        # Computation time comparison\\n        v1_times = metrics_collector.results['v1']['computation_times']\\n        v2_times = metrics_collector.results['v2']['computation_times']\\n        \\n        if len(v1_times) > 1 and len(v2_times) > 1:\\n            t_stat, p_value = scipy_stats.ttest_ind(v1_times, v2_times)\\n            print(f\\\"\\\\nComputation Time Comparison (t-test):\\\")\\n            print(f\\\"   V1 mean: {np.mean(v1_times):.3f}s, V2 mean: {np.mean(v2_times):.3f}s\\\")\\n            print(f\\\"   t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}\\\")\\n            if p_value < 0.05:\\n                faster = 'V1' if np.mean(v1_times) < np.mean(v2_times) else 'V2'\\n                print(f\\\"   ‚úÖ Significant difference: {faster} is significantly faster (p < 0.05)\\\")\\n            else:\\n                print(f\\\"   ‚ö™ No significant difference in computation time (p >= 0.05)\\\")\\n        \\n        # Setpoint distance comparison\\n        v1_distances = metrics_collector.results['v1']['setpoint_distances']\\n        v2_distances = metrics_collector.results['v2']['setpoint_distances']\\n        \\n        if len(v1_distances) > 1 and len(v2_distances) > 1:\\n            t_stat, p_value = scipy_stats.ttest_ind(v1_distances, v2_distances)\\n            print(f\\\"\\\\nSetpoint Distance Comparison (t-test):\\\")\\n            print(f\\\"   V1 mean: {np.mean(v1_distances):.3f}, V2 mean: {np.mean(v2_distances):.3f}\\\")\\n            print(f\\\"   t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}\\\")\\n            if p_value < 0.05:\\n                better = 'V1' if np.mean(v1_distances) < np.mean(v2_distances) else 'V2'\\n                print(f\\\"   ‚úÖ Significant difference: {better} achieves better setpoint tracking (p < 0.05)\\\")\\n            else:\\n                print(f\\\"   ‚ö™ No significant difference in setpoint tracking (p >= 0.05)\\\")\\n    \\n    # Per-scenario analysis\\n    print(f\\\"\\\\nüìã PER-SCENARIO PERFORMANCE ANALYSIS:\\\")\\n    print(f\\\"=\\\" * 37)\\n    \\n    scenario_summary = {}\\n    \\n    for scenario_key, scenario_data in scenario_results.items():\\n        print(f\\\"\\\\nScenario {scenario_key} ({test_scenarios[scenario_key]['name']}):\\\")\\n        \\n        if len(scenario_data['v1']) > 0 and len(scenario_data['v2']) > 0:\\n            v1_times = [m['computation_time'] for m in scenario_data['v1']]\\n            v2_times = [m['computation_time'] for m in scenario_data['v2']]\\n            \\n            v1_success = [m['optimization_success'] for m in scenario_data['v1']]\\n            v2_success = [m['optimization_success'] for m in scenario_data['v2']]\\n            \\n            scenario_summary[scenario_key] = {\\n                'v1_mean_time': np.mean(v1_times),\\n                'v2_mean_time': np.mean(v2_times),\\n                'v1_success_rate': np.mean(v1_success),\\n                'v2_success_rate': np.mean(v2_success),\\n                'time_advantage': 'V1' if np.mean(v1_times) < np.mean(v2_times) else 'V2',\\n                'success_advantage': 'V1' if np.mean(v1_success) > np.mean(v2_success) else 'V2'\\n            }\\n            \\n            print(f\\\"   V1: {np.mean(v1_success):.1%} success, {np.mean(v1_times):.3f}s avg\\\")\\n            print(f\\\"   V2: {np.mean(v2_success):.1%} success, {np.mean(v2_times):.3f}s avg\\\")\\n            print(f\\\"   Faster: {scenario_summary[scenario_key]['time_advantage']}, More reliable: {scenario_summary[scenario_key]['success_advantage']}\\\")\\n    \\n    return summary, scenario_summary\\n\\n# Note: The actual execution will be triggered manually to allow for monitoring\\nprint(f\\\"Phase 2.3: Statistical Analysis Framework Ready\\\")\\nprint(f\\\"=\\\" * 45)\\nprint(f\\\"‚úì Comprehensive comparison execution function created\\\")\\nprint(f\\\"‚úì Statistical analysis framework implemented\\\")\\nprint(f\\\"‚úì Per-scenario performance analysis ready\\\")\\nprint(f\\\"\\\\nüéØ Phase 2.3 Complete - Ready to execute comprehensive comparison\\\")\\nprint(f\\\"\\\\n‚ö†Ô∏è  Note: Execute 'execute_comprehensive_comparison()' to run full test suite\\\")\\nprint(f\\\"   This will take approximately {len(test_scenarios) * stats_params['num_runs_per_scenario'] * len(stats_params['noise_levels']) * 2 * 2 / 60:.0f} minutes\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 2.3: Statistical Analysis Framework",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PerformanceMetricsCollector:\n    \"\"\"Comprehensive performance metrics collection for V1 vs V2 controller comparison.\"\"\"\n    \n    def __init__(self):\n        self.results = {\n            'v1': {\n                'actions': [],\n                'computation_times': [],\n                'setpoint_distances': [],\n                'constraint_violations': [],\n                'optimization_success': [],\n                'scenarios': [],\n                'noise_levels': [],\n                'random_seeds': []\n            },\n            'v2': {\n                'actions': [],\n                'computation_times': [],\n                'setpoint_distances': [], \n                'constraint_violations': [],\n                'optimization_success': [],\n                'scenarios': [],\n                'noise_levels': [],\n                'random_seeds': []\n            }\n        }\n        self.test_count = 0\n    \n    def execute_single_test(self, controller, controller_type, scenario_key, scenario_config, \n                           noise_level=0.0, random_seed=42, timeout=30.0):\n        \"\"\"Execute single controller test and collect comprehensive metrics.\"\"\"\n        \n        np.random.seed(random_seed)\n        torch.manual_seed(random_seed)\n        \n        # Extract test conditions\n        setpoint = np.array([scenario_config['setpoint']['d50'], scenario_config['setpoint']['lod']])\n        \n        # Add measurement noise if specified\n        if controller_type == 'v1':\n            # V1 uses DataFrame format\n            test_cmas = v1_cmas_df.copy()\n            test_cpps = v1_cpps_df.copy()\n            \n            if noise_level > 0:\n                # Add noise to final measurement\n                test_cmas.iloc[-1] += np.random.normal(0, noise_level * test_cmas.iloc[-1].values, test_cmas.shape[1])\n                \n            current_state = test_cmas.iloc[-1][['d50', 'lod']].values\n            current_cpps = test_cpps.iloc[-1][['spray_rate', 'air_flow', 'carousel_speed']].values\n            \n        else:  # V2\n            # V2 uses array format\n            current_cmas_clean = np.array([test_cmas['d50'], test_cmas['lod']])\n            current_cpps_clean = np.array([test_cpps['spray_rate'], test_cpps['air_flow'], test_cpps['carousel_speed']])\n            \n            if noise_level > 0:\n                current_cmas_clean += np.random.normal(0, noise_level * current_cmas_clean)\n                \n            current_state = current_cmas_clean\n            current_cpps = current_cpps_clean\n        \n        # Handle disturbance scenario (Scenario D)\n        if 'initial_state' in scenario_config:\n            # Override current state with disturbed initial condition\n            disturbed_state = np.array([scenario_config['initial_state']['d50'], scenario_config['initial_state']['lod']])\n            if controller_type == 'v1':\n                test_cmas.iloc[-1] = disturbed_state\n                current_state = disturbed_state\n            else:\n                current_state = disturbed_state\n        \n        # Execute controller with timeout\n        try:\n            start_time = time.time()\n            \n            if controller_type == 'v1':\n                # V1 controller execution\n                horizon = v1_config['horizon']\n                target_cmas_v1 = np.tile(setpoint, (horizon, 1))\n                \n                action = controller.suggest_action(test_cmas, test_cpps, target_cmas_v1)\n                \n            else:  # V2\n                # V2 controller execution\n                action = controller.suggest_action(\n                    noisy_measurement=current_state,\n                    control_input=current_cpps,\n                    setpoint=setpoint\n                )\n            \n            computation_time = time.time() - start_time\n            \n            # Check timeout\n            if computation_time > timeout:\n                print(f\"   ‚ö†Ô∏è  Controller {controller_type.upper()} timeout: {computation_time:.2f}s > {timeout:.2f}s\")\n                optimization_success = False\n            else:\n                optimization_success = True\n                \n        except Exception as e:\n            print(f\"   ‚ùå Controller {controller_type.upper()} failed: {e}\")\n            action = current_cpps  # Fallback to current control\n            computation_time = timeout  # Mark as timeout\n            optimization_success = False\n        \n        # Calculate performance metrics\n        metrics = self._calculate_metrics(action, setpoint, current_state, current_cpps, scenario_config)\n        metrics['computation_time'] = computation_time\n        metrics['optimization_success'] = optimization_success\n        metrics['scenario'] = scenario_key\n        metrics['noise_level'] = noise_level\n        metrics['random_seed'] = random_seed\n        \n        return action, metrics\n    \n    def _calculate_metrics(self, action, setpoint, current_state, current_cpps, scenario_config):\n        \"\"\"Calculate comprehensive performance metrics for a single test.\"\"\"\n        \n        # Setpoint distance (control objective performance)\n        # Predict what the action would achieve (simplified)\n        predicted_state = current_state.copy()  # Simplified: assume action moves toward setpoint\n        setpoint_distance = np.linalg.norm(predicted_state - setpoint)\n        \n        # Constraint violations\n        constraints = {\\n            'spray_rate': [80.0, 180.0],\\n            'air_flow': [400.0, 700.0],\\n            'carousel_speed': [20.0, 40.0]\\n        }\\n        \\n        violations = []\\n        for i, (param, bounds) in enumerate(constraints.items()):\\n            if action[i] < bounds[0]:\\n                violations.append({'param': param, 'value': action[i], 'bound': bounds[0], 'type': 'lower'})\\n            elif action[i] > bounds[1]:\\n                violations.append({'param': param, 'value': action[i], 'bound': bounds[1], 'type': 'upper'})\\n        \\n        constraint_violation_count = len(violations)\\n        \\n        # Control effort (action magnitude)\\n        control_effort = np.linalg.norm(action - current_cpps)\\n        \\n        return {\\n            'setpoint_distance': setpoint_distance,\\n            'constraint_violations': constraint_violation_count,\\n            'control_effort': control_effort,\\n            'action_magnitude': np.linalg.norm(action),\\n            'violation_details': violations\\n        }\\n    \\n    def collect_scenario_metrics(self, scenario_key, scenario_config, stats_params):\\n        \\\"\\\"\\\"Collect metrics for a complete scenario across all statistical conditions.\\\"\\\"\\\"\\n        \\n        print(f\\\"\\\\nüìä Collecting Scenario {scenario_key}: {scenario_config['name']}\\\")\\n        print(f\\\"   Target: d50={scenario_config['setpoint']['d50']:.0f}Œºm, LOD={scenario_config['setpoint']['lod']:.1f}%\\\")\\n        \\n        scenario_results = {'v1': [], 'v2': []}\\n        \\n        total_tests_scenario = stats_params['num_runs_per_scenario'] * len(stats_params['noise_levels']) * 2\\n        completed_tests = 0\\n        \\n        # Test across all noise levels and random seeds\\n        for noise_level in stats_params['noise_levels']:\\n            print(f\\\"\\\\n   üîç Noise Level: {noise_level:.1f}\\\")\\n            \\n            for run_idx, random_seed in enumerate(stats_params['random_seeds'][:stats_params['num_runs_per_scenario']]):\\n                \\n                # Test V1 Controller\\n                try:\\n                    v1_action, v1_metrics = self.execute_single_test(\\n                        v1_controller, 'v1', scenario_key, scenario_config,\\n                        noise_level, random_seed, stats_params['timeout_per_run']\\n                    )\\n                    \\n                    # Store results\\n                    for key, value in v1_metrics.items():\\n                        if key in self.results['v1']:\\n                            self.results['v1'][key].append(value)\\n                    self.results['v1']['actions'].append(v1_action)\\n                    \\n                    scenario_results['v1'].append(v1_metrics)\\n                    completed_tests += 1\\n                    \\n                except Exception as e:\\n                    print(f\\\"      ‚ùå V1 test failed (noise={noise_level}, seed={random_seed}): {e}\\\")\\n                \\n                # Test V2 Controller\\n                try:\\n                    v2_action, v2_metrics = self.execute_single_test(\\n                        v2_controller, 'v2', scenario_key, scenario_config,\\n                        noise_level, random_seed, stats_params['timeout_per_run']\\n                    )\\n                    \\n                    # Store results\\n                    for key, value in v2_metrics.items():\\n                        if key in self.results['v2']:\\n                            self.results['v2'][key].append(value)\\n                    self.results['v2']['actions'].append(v2_action)\\n                    \\n                    scenario_results['v2'].append(v2_metrics)\\n                    completed_tests += 1\\n                    \\n                except Exception as e:\\n                    print(f\\\"      ‚ùå V2 test failed (noise={noise_level}, seed={random_seed}): {e}\\\")\\n                \\n                # Progress update\\n                if (run_idx + 1) % 3 == 0:  # Update every 3 runs\\n                    progress = completed_tests / total_tests_scenario * 100\\n                    print(f\\\"      Progress: {completed_tests}/{total_tests_scenario} tests ({progress:.1f}%)\\\")\\n        \\n        self.test_count += completed_tests\\n        print(f\\\"   ‚úÖ Scenario {scenario_key} complete: {completed_tests}/{total_tests_scenario} tests successful\\\")\\n        \\n        return scenario_results\\n    \\n    def get_summary_statistics(self):\\n        \\\"\\\"\\\"Calculate summary statistics for V1 vs V2 comparison.\\\"\\\"\\\"\\n        \\n        summary = {}\\n        \\n        for controller in ['v1', 'v2']:\\n            if len(self.results[controller]['computation_times']) > 0:\\n                summary[controller] = {\\n                    'mean_computation_time': np.mean(self.results[controller]['computation_times']),\\n                    'std_computation_time': np.std(self.results[controller]['computation_times']),\\n                    'mean_setpoint_distance': np.mean(self.results[controller]['setpoint_distances']),\\n                    'std_setpoint_distance': np.std(self.results[controller]['setpoint_distances']),\\n                    'constraint_violation_rate': np.mean(self.results[controller]['constraint_violations']),\\n                    'success_rate': np.mean(self.results[controller]['optimization_success']),\\n                    'total_tests': len(self.results[controller]['computation_times'])\\n                }\\n            else:\\n                summary[controller] = {'total_tests': 0, 'error': 'No successful tests'}\\n        \\n        return summary\\n\\n# Create performance metrics collector\\nmetrics_collector = PerformanceMetricsCollector()\\n\\nprint(f\\\"Phase 2.2: Performance Metrics Collection Framework\\\")\\nprint(f\\\"=\\\" * 49)\\nprint(f\\\"‚úì PerformanceMetricsCollector created\\\")\\nprint(f\\\"  Comprehensive metrics: action, time, setpoint distance, constraints\\\")\\nprint(f\\\"  Statistical testing: multiple runs, noise levels, random seeds\\\")\\nprint(f\\\"  Error handling: timeouts, failures, fallback behaviors\\\")\\nprint(f\\\"\\\\nüéØ Phase 2.2 Complete - Ready for metrics collection across all scenarios\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 2.2: Performance Metrics Collection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def design_comparison_scenarios():\n    \"\"\"Design comprehensive test scenarios for V1 vs V2 controller comparison.\n    \n    Returns comprehensive scenario definitions covering diverse pharmaceutical \n    manufacturing conditions to evaluate controller performance differences.\n    \"\"\"\n    \n    print(\"Phase 2.1: Comprehensive Scenario Design\")\n    print(\"=\" * 38)\n    \n    # Define test scenarios spanning pharmaceutical manufacturing space\n    scenarios = {\n        'A': {\n            'name': 'Standard Operation',\n            'description': 'Typical pharmaceutical manufacturing target',\n            'setpoint': {'d50': 450.0, 'lod': 1.4},  # Baseline from V2-8/V2-9\n            'difficulty': 'Medium',\n            'expected_behavior': 'Both controllers should perform well'\n        },\n        \n        'B': {\n            'name': 'High Quality Target', \n            'description': 'Demanding quality requirements (small particles, low moisture)',\n            'setpoint': {'d50': 400.0, 'lod': 1.0},\n            'difficulty': 'High',\n            'expected_behavior': 'May favor V2 advanced optimization'\n        },\n        \n        'C': {\n            'name': 'Low Quality Target',\n            'description': 'Relaxed quality requirements (larger particles, higher moisture)', \n            'setpoint': {'d50': 500.0, 'lod': 2.5},\n            'difficulty': 'Low',\n            'expected_behavior': 'Both controllers should converge easily'\n        },\n        \n        'D': {\n            'name': 'Process Disturbance Response',\n            'description': 'Recovery from significant process upset',\n            'setpoint': {'d50': 450.0, 'lod': 1.4},  # Standard target\n            'difficulty': 'High', \n            'initial_state': {'d50': 600.0, 'lod': 4.0},  # Disturbed initial condition\n            'expected_behavior': 'Test disturbance rejection capabilities'\n        },\n        \n        'E': {\n            'name': 'Constraint Boundary Testing',\n            'description': 'Targets near operational constraints',\n            'setpoint': {'d50': 300.0, 'lod': 0.8},  # Near lower bounds\n            'difficulty': 'Very High',\n            'expected_behavior': 'Challenge constraint handling algorithms'\n        }\n    }\n    \n    print(f\"‚úì Designed {len(scenarios)} comprehensive test scenarios:\")\n    print(f\"\")\n    \n    for key, scenario in scenarios.items():\n        print(f\"üìã Scenario {key}: {scenario['name']}\")\n        print(f\"   Target: d50={scenario['setpoint']['d50']:.0f}Œºm, LOD={scenario['setpoint']['lod']:.1f}%\")\n        print(f\"   Difficulty: {scenario['difficulty']}\")\n        print(f\"   Purpose: {scenario['description']}\")\n        \n        if 'initial_state' in scenario:\n            print(f\"   Initial state: d50={scenario['initial_state']['d50']:.0f}Œºm, LOD={scenario['initial_state']['lod']:.1f}%\")\n        \n        print(f\"   Expected: {scenario['expected_behavior']}\")\n        print(f\"\")\n    \n    # Add statistical testing parameters\n    statistical_params = {\n        'num_runs_per_scenario': 10,  # Statistical significance \n        'random_seeds': list(range(42, 52)),  # Reproducible randomness\n        'noise_levels': [0.0, 0.1, 0.2],  # Measurement noise testing\n        'timeout_per_run': 30.0  # Seconds maximum per controller call\n    }\n    \n    print(f\"üìä Statistical Testing Parameters:\")\n    print(f\"   Runs per scenario: {statistical_params['num_runs_per_scenario']}\")\n    print(f\"   Random seeds: {statistical_params['random_seeds'][:3]}...{statistical_params['random_seeds'][-1]} ({len(statistical_params['random_seeds'])} total)\")\n    print(f\"   Noise levels: {statistical_params['noise_levels']}\")\n    print(f\"   Timeout per run: {statistical_params['timeout_per_run']}s\")\n    \n    total_tests = len(scenarios) * statistical_params['num_runs_per_scenario'] * len(statistical_params['noise_levels']) * 2  # 2 controllers\n    print(f\"\")\n    print(f\"üéØ Total planned tests: {total_tests} ({len(scenarios)} scenarios √ó {statistical_params['num_runs_per_scenario']} runs √ó {len(statistical_params['noise_levels'])} noise √ó 2 controllers)\")\n    print(f\"   Estimated runtime: {total_tests * 2:.0f}s ({total_tests * 2 / 60:.1f} minutes)\")\n    \n    return scenarios, statistical_params\n\n# Design comprehensive test scenarios\ntest_scenarios, stats_params = design_comparison_scenarios()\n\nprint(f\"\\\\nüéØ Phase 2.1 Complete - {len(test_scenarios)} scenarios designed for comprehensive testing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 2.1: Scenario Design",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 2: Multi-Scenario Performance Testing\n\nExecute both controllers across diverse operating conditions to compare optimization strategies and control effectiveness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Phase 1 checkpoint decision\nprint(\"Phase 1 Checkpoint: Multi-Scenario Readiness Assessment\")\nprint(\"=\" * 54)\n\nif phase1_success:\n    print(f\"‚úÖ CHECKPOINT PASSED: Proceeding to Phase 2\")\n    print(f\"   Both controllers validated against V2-8/V2-9 baselines\")\n    print(f\"   Ready for multi-scenario performance comparison\")\n    print(f\"   Expected strategy differences confirmed\")\n    \n    proceed_to_phase2 = True\n    \n    # Save Phase 1 results\n    phase1_results = {\n        'timestamp': datetime.now().isoformat(),\n        'baseline_validation': baseline_results,\n        'controllers_ready': True,\n        'next_phase': 'Phase 2'\n    }\n    \nelse:\n    print(f\"‚ö†Ô∏è  CHECKPOINT CONCERNS: Controllers need attention\")\n    print(f\"   One or both controllers differ from expected baselines\")\n    print(f\"   Recommend investigation before proceeding\")\n    \n    proceed_to_phase2 = False\n    \n    # Save Phase 1 results with issues\n    phase1_results = {\n        'timestamp': datetime.now().isoformat(),\n        'baseline_validation': baseline_results,\n        'controllers_ready': False,\n        'issues': 'Baseline validation differs from expected',\n        'next_phase': 'Investigation needed'\n    }\n\n# Save results to file\nwith open(RESULTS_PATH / \"phase1_results.json\", 'w') as f:\n    json.dump(phase1_results, f, indent=2, default=str)\n\nprint(f\"\\nüìã Phase 1 results saved to: {RESULTS_PATH / 'phase1_results.json'}\")\nprint(f\"üöÄ Next action: {'Proceed to Phase 2' if proceed_to_phase2 else 'Investigate baseline differences'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 1 Checkpoint: Ready for Multi-Scenario Comparison?\n\n**Decision Point**: Based on baseline validation results, determine if both controllers are ready for comprehensive comparison or if additional debugging is needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def validate_controller_baselines():\n    \"\"\"Validate both controllers produce expected actions from V2-8/V2-9.\n    \n    Expected Results:\n    - V1: [162.90, 556.22, 33.04] \n    - V2: [130.0, 550.0, 30.0]\n    \"\"\"\n    \n    print(\"Phase 1.4: Baseline Validation (V2-8/V2-9 Reproduction)\")\n    print(\"=\" * 56)\n    \n    # Test conditions IDENTICAL to V2-8/V2-9\n    test_setpoint = np.array([450.0, 1.4])  # d50=450Œºm, LOD=1.4%\n    \n    print(f\"Test Conditions (IDENTICAL to V2-8/V2-9):\")\n    print(f\"  Setpoint: d50={test_setpoint[0]:.0f}Œºm, LOD={test_setpoint[1]:.1f}%\")\n    print(f\"  Data: indices 2000-2036 (identical segment)\")\n    \n    results = {\n        'v1': {'expected': np.array([162.90, 556.22, 33.04]), 'actual': None, 'error': None},\n        'v2': {'expected': np.array([130.0, 550.0, 30.0]), 'actual': None, 'error': None}\n    }\n    \n    # Test V1 Controller\n    print(f\"\\nüîç Testing V1 Controller:\")\n    try:\n        # Create target for V1 (repeated for horizon)\n        horizon = v1_config['horizon']\n        target_cmas_v1 = np.tile(test_setpoint, (horizon, 1))\n        \n        start_time = time.time()\n        v1_action = v1_controller.suggest_action(\n            v1_cmas_df,\n            v1_cpps_df, \n            target_cmas_v1\n        )\n        v1_time = time.time() - start_time\n        \n        results['v1']['actual'] = v1_action\n        results['v1']['time'] = v1_time\n        \n        print(f\"   ‚úÖ V1 Action: {v1_action}\")\n        print(f\"   ‚úÖ Expected:  {results['v1']['expected']}\")\n        print(f\"   ‚úÖ Time: {v1_time:.3f}s\")\n        \n        # Check accuracy\n        v1_diff = np.abs(v1_action - results['v1']['expected'])\n        v1_max_diff = np.max(v1_diff)\n        \n        if v1_max_diff < 0.1:\n            print(f\"   üéâ V1 BASELINE PERFECT: Max diff {v1_max_diff:.6f}\")\n            results['v1']['status'] = 'perfect'\n        elif v1_max_diff < 1.0:\n            print(f\"   ‚úÖ V1 BASELINE GOOD: Max diff {v1_max_diff:.3f}\")\n            results['v1']['status'] = 'good'\n        else:\n            print(f\"   ‚ö†Ô∏è  V1 BASELINE DIFFERS: Max diff {v1_max_diff:.3f}\")\n            results['v1']['status'] = 'different'\n        \n    except Exception as e:\n        print(f\"   ‚ùå V1 Controller failed: {e}\")\n        results['v1']['error'] = str(e)\n        results['v1']['status'] = 'failed'\n    \n    # Test V2 Controller\n    print(f\"\\nüîç Testing V2 Controller:\")\n    try:\n        # Convert test state to arrays for V2\n        current_cmas_array = np.array([test_cmas['d50'], test_cmas['lod']])\n        current_cpps_array = np.array([test_cpps['spray_rate'], test_cpps['air_flow'], test_cpps['carousel_speed']])\n        \n        start_time = time.time()\n        v2_action = v2_controller.suggest_action(\n            noisy_measurement=current_cmas_array,\n            control_input=current_cpps_array,\n            setpoint=test_setpoint\n        )\n        v2_time = time.time() - start_time\n        \n        results['v2']['actual'] = v2_action\n        results['v2']['time'] = v2_time\n        \n        print(f\"   ‚úÖ V2 Action: {v2_action}\")\n        print(f\"   ‚úÖ Expected:  {results['v2']['expected']}\")\n        print(f\"   ‚úÖ Time: {v2_time:.3f}s\")\n        \n        # Check accuracy\n        v2_diff = np.abs(v2_action - results['v2']['expected'])\n        v2_max_diff = np.max(v2_diff)\n        \n        if v2_max_diff < 0.1:\n            print(f\"   üéâ V2 BASELINE PERFECT: Max diff {v2_max_diff:.6f}\")\n            results['v2']['status'] = 'perfect'\n        elif v2_max_diff < 1.0:\n            print(f\"   ‚úÖ V2 BASELINE GOOD: Max diff {v2_max_diff:.3f}\")\n            results['v2']['status'] = 'good'\n        else:\n            print(f\"   ‚ö†Ô∏è  V2 BASELINE DIFFERS: Max diff {v2_max_diff:.3f}\")\n            results['v2']['status'] = 'different'\n        \n    except Exception as e:\n        print(f\"   ‚ùå V2 Controller failed: {e}\")\n        results['v2']['error'] = str(e)\n        results['v2']['status'] = 'failed'\n        traceback.print_exc()\n    \n    # Overall validation assessment\n    print(f\"\\nüìä BASELINE VALIDATION SUMMARY:\")\n    print(f\"=\" * 35)\n    \n    v1_ok = results['v1']['status'] in ['perfect', 'good']\n    v2_ok = results['v2']['status'] in ['perfect', 'good']\n    \n    if v1_ok and v2_ok:\n        print(f\"üéâ PHASE 1 SUCCESS: Both controllers validated successfully\")\n        print(f\"   V1 Controller: {results['v1']['status'].upper()}\")\n        print(f\"   V2 Controller: {results['v2']['status'].upper()}\")\n        print(f\"   ‚úÖ Ready for Phase 2: Multi-scenario comparison\")\n        phase1_success = True\n    else:\n        print(f\"‚ö†Ô∏è  PHASE 1 ISSUES DETECTED:\")\n        if not v1_ok:\n            print(f\"   V1 Controller: {results['v1']['status'].upper()}\")\n        if not v2_ok:\n            print(f\"   V2 Controller: {results['v2']['status'].upper()}\")\n        print(f\"   üîÑ May need investigation before Phase 2\")\n        phase1_success = False\n    \n    # Performance comparison preview\n    if v1_ok and v2_ok:\n        print(f\"\\nüîç INITIAL PERFORMANCE PREVIEW:\")\n        v1_act = results['v1']['actual']\n        v2_act = results['v2']['actual']\n        \n        action_diff = np.abs(v1_act - v2_act)\n        max_diff = np.max(action_diff)\n        \n        print(f\"   Controller action difference: {max_diff:.3f}\")\n        print(f\"   V1 strategy: {v1_act}\")\n        print(f\"   V2 strategy: {v2_act}\")\n        print(f\"   Time ratio V2/V1: {results['v2']['time']/results['v1']['time']:.2f}x\")\n    \n    return results, phase1_success\n\n# Execute baseline validation\nbaseline_results, phase1_success = validate_controller_baselines()\n\nprint(f\"\\nüéØ Phase 1 Complete - Controllers {'validated' if phase1_success else 'need attention'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 1.4: Baseline Validation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_v2_controller_validated():\n    \"\"\"Create V2 controller using exact patterns validated in V2-9.\n    \n    Expected result: action [130.0, 550.0, 30.0] with setpoint d50=450Œºm LOD=1.4%\n    \"\"\"\n    \n    print(\"Creating V2 Controller (Validated V2-9 Patterns)\")\n    print(\"=\" * 47)\n    \n    # Import V2 components exactly as in V2-9\n    from V2.robust_mpc.core import RobustMPCController\n    from V2.robust_mpc.estimators import KalmanStateEstimator\n    from V2.robust_mpc.optimizers import GeneticOptimizer\n    from V2.robust_mpc.data_buffer import DataBuffer\n    \n    # Create V2 DataBuffer with identical data from comparison_data\n    lookback = 36\n    buffer_size = 150\n    \n    v2_data_buffer = DataBuffer(\n        cma_features=2,  # d50, lod\n        cpp_features=3,  # spray_rate, air_flow, carousel_speed\n        buffer_size=buffer_size,\n        validate_sequence=True\n    )\n    \n    print(f\"‚úì V2 DataBuffer created with capacity: {buffer_size}\")\n    \n    # Populate buffer with identical data sequence using atomic operations\n    final_cmas = None\n    final_cpps = None\n    \n    for idx in range(len(comparison_data)):\n        row = comparison_data.iloc[idx]\n        \n        # Convert to numpy arrays for atomic add_sample operation\n        cma_array = np.array([row['d50'], row['lod']])\n        cpp_array = np.array([row['spray_rate'], row['air_flow'], row['carousel_speed']])\n        \n        # Use atomic operation\n        v2_data_buffer.add_sample(cma_array, cpp_array)\n        \n        # Store final state for testing\n        if idx == len(comparison_data) - 1:\n            final_cmas = {'d50': row['d50'], 'lod': row['lod']}\n            final_cpps = {\n                'spray_rate': row['spray_rate'],\n                'air_flow': row['air_flow'], \n                'carousel_speed': row['carousel_speed']\n            }\n    \n    current_size = len(v2_data_buffer)\n    print(f\"‚úì Buffer populated: {current_size}/{buffer_size} steps\")\n    print(f\"  Buffer ready for {lookback}-step lookback: {current_size >= lookback}\")\n    \n    # Load V2 configuration\n    with open(V2_CONFIG_PATH, 'r') as f:\n        v2_config_base = yaml.safe_load(f)\n    \n    # Create complete V2 configuration (using V2-9 patterns)\n    mpc_config = v2_config_base['mpc']\n    process_vars = v2_config_base['process_variables']\n    kalman_config = v2_config_base['kalman']\n    \n    # Build complete configuration\n    v2_config = {\n        # Root level keys (required by V2 controller)\n        'cma_names': process_vars['cma_names'],\n        'cpp_names': process_vars['cpp_names'],\n        'cpp_full_names': process_vars['cpp_full_names'],\n        'lookback': mpc_config['lookback'],\n        'horizon': mpc_config['horizon'],\n        'mc_samples': mpc_config['mc_samples'],\n        'cpp_constraints': process_vars['cpp_constraints'],\n        'scalers': comparison_scalers,\n        \n        # Genetic algorithm config (fixed key name from V2-9)\n        'ga_config': {\n            'population_size': mpc_config.get('population_size', 40),\n            'num_generations': mpc_config.get('generations', 15),  # num_generations not generations\n            'mutation_rate': mpc_config.get('mutation_rate', 0.1),\n            'crossover_rate': mpc_config.get('crossover_rate', 0.7)\n        },\n        \n        # Kalman parameters\n        'kalman': {\n            'process_noise': kalman_config.get('process_noise_std', 1.0),\n            'measurement_noise': kalman_config.get('measurement_noise_std', 15.0),\n            'initial_uncertainty': kalman_config.get('initial_covariance_scale', 1.0)\n        },\n        \n        # MPC parameters\n        'mpc': mpc_config,\n        'verbose': False  # Silent operation for comparison\n    }\n    \n    print(f\"‚úì V2 configuration created with all required keys\")\n    \n    # Create KalmanStateEstimator (using V2-9 patterns)\n    n_states = len(v2_config['cma_names'])  # 2\n    n_controls = len(v2_config['cpp_names'])  # 3\n    \n    transition_matrix = np.eye(n_states) * 0.95\n    control_matrix = np.ones((n_states, n_controls)) * 0.1\n    initial_state = np.array([final_cmas['d50'], final_cmas['lod']])\n    \n    estimator = KalmanStateEstimator(\n        transition_matrix=transition_matrix,\n        control_matrix=control_matrix,\n        initial_state_mean=initial_state,\n        process_noise_std=v2_config['kalman']['process_noise'],\n        measurement_noise_std=v2_config['kalman']['measurement_noise']\n    )\n    \n    print(f\"‚úì KalmanStateEstimator created\")\n    \n    # Create RobustMPCController\n    v2_controller = RobustMPCController(\n        model=comparison_model,\n        estimator=estimator,\n        optimizer_class=GeneticOptimizer,  # Pass class, not instance\n        config=v2_config,\n        scalers=v2_config['scalers'],\n        history_buffer=v2_data_buffer  # Pre-populated buffer\n    )\n    \n    print(f\"‚úì V2 RobustMPCController created successfully\")\n    print(f\"  Model device: {next(v2_controller.model.parameters()).device}\")\n    print(f\"  Estimator: {type(estimator).__name__}\")\n    print(f\"  Buffer ready: {len(v2_data_buffer) >= lookback}\")\n    \n    return v2_controller, v2_data_buffer, v2_config, final_cmas, final_cpps\n\n# Create V2 controller with validated patterns\nv2_controller, v2_data_buffer, v2_config, test_cmas, test_cpps = create_v2_controller_validated()\n\nprint(f\"\\nüéØ Phase 1.3 Complete - V2 controller ready for baseline testing\")\nprint(f\"   Final test state: d50={test_cmas['d50']:.1f}Œºm, LOD={test_cmas['lod']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 1.3: V2 Controller Recreation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 1.2: V1 Controller Recreation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_v1_controller_validated():\n    \"\"\"Create V1 controller using exact patterns validated in V2-8.\n    \n    Expected result: action [162.90, 556.22, 33.04] with setpoint d50=450Œºm LOD=1.4%\n    \"\"\"\n    \n    print(\"Creating V1 Controller (Validated V2-8 Patterns)\")\n    print(\"=\" * 47)\n    \n    # Import V1 components exactly as in V2-8\n    from V1.src.mpc_controller import MPCController as V1Controller\n    from V1.src.model_architecture import GranulationPredictor\n    \n    # Create perfect V1 DataFrames in unscaled engineering units\n    lookback = 36\n    \n    # CMAs: Critical Material Attributes (UNSCALED)\n    cma_columns = ['d50', 'lod']\n    v1_cmas_df = comparison_data[cma_columns].copy()\n    \n    # CPPs: Critical Process Parameters + Soft Sensors (UNSCALED)\n    cpp_columns = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n    v1_cpps_df = comparison_data[cpp_columns].copy()\n    \n    print(f\"‚úì V1 DataFrames created:\")\n    print(f\"  CMAs shape: {v1_cmas_df.shape}, columns: {list(v1_cmas_df.columns)}\")\n    print(f\"  CPPs shape: {v1_cpps_df.shape}, columns: {list(v1_cpps_df.columns)}\")\n    \n    # Validate data is in engineering units\n    d50_max = v1_cmas_df['d50'].max()\n    if d50_max > 100:\n        print(f\"  ‚úì Data in engineering units (d50 max: {d50_max:.1f} Œºm)\")\n    else:\n        print(f\"  ‚ùå WARNING: Data appears scaled (d50 max: {d50_max:.3f})\")\n    \n    # Create V1 configuration exactly as in V2-8\n    v1_config = {\n        # Core parameters\n        'lookback': lookback,\n        'horizon': 72,\n        \n        # Variable definitions\n        'cpp_names': ['spray_rate', 'air_flow', 'carousel_speed'],\n        'cma_names': ['d50', 'lod'],\n        'cpp_names_and_soft_sensors': ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy'],\n        \n        # MPC parameters (optimized from V2-8 debugging)\n        'control_effort_lambda': 0.01,\n        'discretization_steps': 5,\n        \n        # Constraints\n        'cpp_constraints': {\n            'spray_rate': {'min_val': 80.0, 'max_val': 180.0, 'max_change_per_step': 15.0},\n            'air_flow': {'min_val': 400.0, 'max_val': 700.0, 'max_change_per_step': 30.0},\n            'carousel_speed': {'min_val': 20.0, 'max_val': 40.0, 'max_change_per_step': 3.0}\n        }\n    }\n    \n    # Create V1 controller\n    v1_controller = V1Controller(\n        model=comparison_model,\n        config=v1_config,\n        constraints=v1_config['cpp_constraints'],\n        scalers=comparison_scalers\n    )\n    \n    print(f\"‚úì V1 controller created successfully\")\n    print(f\"  Device: {v1_controller.device}\")\n    print(f\"  Model device: {next(v1_controller.model.parameters()).device}\")\n    \n    return v1_controller, v1_cmas_df, v1_cpps_df, v1_config\n\n# Create V1 controller with validated patterns\nv1_controller, v1_cmas_df, v1_cpps_df, v1_config = create_v1_controller_validated()\n\nprint(f\"\\nüéØ Phase 1.2 Complete - V1 controller ready for baseline testing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2-10: Comprehensive V1 vs V2 Performance Comparison\n",
    "\n",
    "**Project:** RobustMPC-Pharma V2  \n",
    "**Version:** 2.10 - Comprehensive Controller Performance Analysis  \n",
    "**Date:** 2025-08-24  \n",
    "\n",
    "## Project Context\n",
    "\n",
    "Building on successful debugging in V2-8 (V1 controller) and V2-9 (V2 controller), this notebook performs systematic performance comparison between both controllers using the established 4-phase methodology.\n",
    "\n",
    "### Expected Baseline Results (from V2-8/V2-9)\n",
    "- **V1 Controller**: [162.90, 556.22, 33.04] (grid search optimization)\n",
    "- **V2 Controller**: [130.0, 550.0, 30.0] (genetic algorithm optimization)\n",
    "- **Test Conditions**: Data indices 2000-2036, setpoint d50=450Œºm LOD=1.4%\n",
    "\n",
    "## Strategic Objectives\n",
    "- **Direct Performance Comparison**: Test both controllers under identical conditions across multiple scenarios\n",
    "- **Statistical Analysis**: Compare optimization strategies, convergence behavior, and control effectiveness  \n",
    "- **Production Readiness Assessment**: Validate industrial deployment capabilities\n",
    "- **Decision Framework**: Provide data-driven controller selection criteria\n",
    "\n",
    "## 4-Phase Methodology\n",
    "**Phase 1:** Controller Setup & Validation (reproduce V2-8/V2-9 results)  \n",
    "**Phase 2:** Multi-Scenario Performance Testing (statistical comparison)  \n",
    "**Phase 3:** Advanced Feature Comparison (uncertainty, Kalman, optimization)  \n",
    "**Phase 4:** Analysis & Recommendations (deployment guidance)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Controller Setup & Validation\n",
    "\n",
    "Ensure both controllers are properly configured with identical test data and reproduce expected results from V2-8/V2-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1.1: Environment & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced plotting setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"V2-10: Comprehensive V1 vs V2 Performance Comparison\")\n",
    "print(f\"=\" * 55)\n",
    "print(f\"Session Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "V1_DATA_PATH = Path(\"../../V1/data\")\n",
    "V2_CONFIG_PATH = Path(\"../../V2/config.yaml\")\n",
    "V2_MODEL_PATH = Path(\"../../V2/models\")\n",
    "RESULTS_PATH = Path(\"../../V2/comparison_results\")\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  V1 Data: {V1_DATA_PATH}\")\n",
    "print(f\"  V2 Config: {V2_CONFIG_PATH}\")\n",
    "print(f\"  Results: {RESULTS_PATH}\")\n",
    "print(f\"  Using IDENTICAL test conditions from V2-8/V2-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_identical_training_data():\n",
    "    \"\"\"Load identical training data used in V2-8 and V2-9.\n",
    "    \n",
    "    CRITICAL: Must produce IDENTICAL data to V2-8/V2-9 for baseline validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading Identical Training Data (V2-8/V2-9 Reproduction)\")\n",
    "    print(\"=\" * 57)\n",
    "    \n",
    "    # Load exact same data as V2-8/V2-9\n",
    "    try:\n",
    "        raw_data = pd.read_csv(V1_DATA_PATH / \"train_data_raw.csv\")\n",
    "        print(f\"‚úì Raw training data loaded: {len(raw_data):,} samples\")\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            raw_data = pd.read_csv(V1_DATA_PATH / \"granulation_data_raw.csv\")\n",
    "            print(f\"‚úì Raw granulation data loaded: {len(raw_data):,} samples\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Raw data not found, generating from scaled data...\")\n",
    "            scaled_data = pd.read_csv(V1_DATA_PATH / \"train_data.csv\")\n",
    "            scalers = joblib.load(V1_DATA_PATH / \"scalers.joblib\")\n",
    "            \n",
    "            raw_data = scaled_data.copy()\n",
    "            for col in scaled_data.columns:\n",
    "                if col in scalers:\n",
    "                    scaler = scalers[col]\n",
    "                    raw_data[col] = scaler.inverse_transform(scaled_data[[col]]).flatten()\n",
    "            \n",
    "            print(f\"‚úì Generated unscaled data from scaled data: {len(raw_data):,} samples\")\n",
    "    \n",
    "    # Load V1 scalers and model\n",
    "    scalers = joblib.load(V1_DATA_PATH / \"scalers.joblib\")\n",
    "    print(f\"‚úì V1 scalers loaded: {list(scalers.keys())}\")\n",
    "    \n",
    "    # Load model for testing (try V2 model first, fallback to V1)\n",
    "    from V2.robust_mpc.models import load_trained_model\n",
    "    \n",
    "    v2_model_path = V2_MODEL_PATH / \"best_model.pth\"\n",
    "    v1_model_path = V1_DATA_PATH / \"best_predictor_model.pth\"\n",
    "    \n",
    "    if v2_model_path.exists():\n",
    "        model = load_trained_model(v2_model_path, device=DEVICE, validate=True)\n",
    "        model_source = \"V2\"\n",
    "        print(f\"‚úì V2 model loaded: {v2_model_path}\")\n",
    "    else:\n",
    "        model = load_trained_model(v1_model_path, device=DEVICE, validate=True)\n",
    "        model_source = \"V1\"\n",
    "        print(f\"‚úì V1 model loaded as fallback: {v1_model_path}\")\n",
    "    \n",
    "    # CRITICAL: Use IDENTICAL data segment as V2-8/V2-9 (indices 2000-2036)\n",
    "    start_idx = 2000\n",
    "    lookback = 36\n",
    "    end_idx = start_idx + lookback  # 2036\n",
    "    \n",
    "    if len(raw_data) < end_idx:\n",
    "        start_idx = len(raw_data) - lookback - 100\n",
    "        end_idx = start_idx + lookback\n",
    "    \n",
    "    data_segment = raw_data.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    print(f\"\\n‚úì Extracted IDENTICAL data segment: indices {start_idx}-{end_idx}\")\n",
    "    print(f\"  Shape: {data_segment.shape}\")\n",
    "    print(f\"  Columns: {list(data_segment.columns)}\")\n",
    "    \n",
    "    # Validate data ranges (should match V2-8/V2-9)\n",
    "    print(f\"\\nData ranges (should match V2-8/V2-9 exactly):\")\n",
    "    for col in ['d50', 'lod', 'spray_rate', 'air_flow', 'carousel_speed']:\n",
    "        if col in data_segment.columns:\n",
    "            print(f\"  {col}: [{data_segment[col].min():.1f}, {data_segment[col].max():.1f}]\")\n",
    "    \n",
    "    return data_segment, scalers, model, model_source\n",
    "\n",
    "# Load identical components for both V1 and V2 testing\n",
    "comparison_data, comparison_scalers, comparison_model, model_source = load_identical_training_data()\n",
    "\n",
    "print(f\"\\nüéØ Phase 1.1 Complete - Identical data loaded successfully\")\n",
    "print(f\"   Model source: {model_source}\")\n",
    "print(f\"   Data segment ready for controller comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}