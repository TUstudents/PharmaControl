import numpy as np
import torch

class RobustMPCController:
    """
    An advanced MPC controller that integrates a state estimator,
    a probabilistic model, and a genetic algorithm optimizer for robust,
    offset-free, and uncertainty-aware control.
    """
    def __init__(self, model, estimator, optimizer_class, config, scalers):
        self.model = model
        self.estimator = estimator
        self.optimizer_class = optimizer_class
        self.config = config
        self.scalers = scalers
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)

        # Initialize the disturbance estimate for Integral Action
        self.disturbance_estimate = np.zeros(len(config['cma_names']))

    def _update_disturbance_estimate(self, smooth_state, setpoint):
        """Updates the integral error term for offset-free control."""
        error = setpoint - smooth_state
        # The gain (alpha) determines how quickly the controller adapts to the disturbance
        alpha = self.config.get('integral_gain', 0.1)
        self.disturbance_estimate += alpha * error

    def _get_fitness_function(self, past_cmas_scaled, past_cpps_scaled, target_cmas_unscaled):
        """
        Creates and returns the fitness function to be used by the GA.
        This function captures the current state and target.
        """
        def fitness(control_plan_unscaled):
            # --- 1. Prepare Inputs ---
            # Scale the unscaled control plan generated by the GA
            plan_scaled = self._scale_cpp_plan(control_plan_unscaled)

            # Convert all inputs to tensors
            past_cmas_tensor = torch.tensor(past_cmas_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)
            past_cpps_tensor = torch.tensor(past_cpps_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)
            future_cpps_tensor = torch.tensor(plan_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)

            # --- 2. Get Probabilistic Prediction ---
            mean_pred_scaled, std_pred_scaled = self.model.predict_distribution(
                past_cmas_tensor, past_cpps_tensor, future_cpps_tensor, 
                n_samples=self.config.get('mc_samples', 30)
            )

            # --- 3. Calculate Risk-Adjusted, Corrected Prediction ---
            # Correct for disturbance (Integral Action)
            disturbance_scaled = self._scale_cma_vector(self.disturbance_estimate)
            corrected_mean_scaled = mean_pred_scaled + torch.tensor(disturbance_scaled, device=self.device)

            # Adjust for risk (Uncertainty-Awareness)
            beta = self.config.get('risk_beta', 1.5) # Higher beta = more cautious
            # For minimization, we penalize the upper bound of the error
            risk_adjusted_pred_scaled = corrected_mean_scaled + beta * std_pred_scaled

            # --- 4. Calculate Cost ---
            target_scaled = self._scale_cma_plan(target_cmas_unscaled)
            target_tensor = torch.tensor(target_scaled, dtype=torch.float32).to(self.device)
            cost = torch.mean(torch.abs(risk_adjusted_pred_scaled - target_tensor))

            return cost.item()

        return fitness

    def suggest_action(self, noisy_measurement, control_input, setpoint):
        # 1. Get a clean state estimate
        smooth_state = self.estimator.estimate(noisy_measurement, control_input)

        # 2. Update the integral error term
        self._update_disturbance_estimate(smooth_state, setpoint)

        # 3. Create the fitness function for this specific time step
        # This part requires getting the historical data, which we will simulate for the test.
        # In a real app, this would come from a data buffer.
        past_cmas_scaled, past_cpps_scaled = self._get_scaled_history(smooth_state)

        # The target is the setpoint repeated over the horizon
        target_plan = np.tile(setpoint, (self.config['horizon'], 1))
        fitness_func = self._get_fitness_function(past_cmas_scaled, past_cpps_scaled, target_plan)

        # 4. Instantiate and run the optimizer
        param_bounds = self._get_param_bounds()
        optimizer = self.optimizer_class(fitness_func, param_bounds, self.config['ga_config'])
        best_plan = optimizer.optimize()

        # 5. Return the first step of the optimal plan
        return best_plan[0]

    # --- Helper methods for scaling and data management ---
    def _get_scaled_history(self, current_smooth_state):
        # In a real app, this would pull from a historical data buffer.
        # For this test, we'll create dummy history.
        L = self.config['lookback']
        past_cmas_unscaled = np.tile(current_smooth_state, (L, 1))
        past_cpps_unscaled = np.tile([120, 500, 30], (L, 1)) # Dummy CPPs
        # Add soft sensors to CPPs
        # ... (logic from notebook V1-2) ...
        # Scale both
        past_cmas_scaled = self._scale_cma_plan(past_cmas_unscaled)
        past_cpps_scaled = self._scale_cpp_plan(past_cpps_unscaled, with_soft_sensors=True)
        return past_cmas_scaled, past_cpps_scaled

    def _scale_cpp_plan(self, plan_unscaled, with_soft_sensors=False):
        # This needs to be implemented robustly, matching the training preprocessing
        # For now, a placeholder
        if with_soft_sensors: return np.zeros((plan_unscaled.shape[0], len(self.config['cpp_full_names'])))
        return np.zeros_like(plan_unscaled)

    def _scale_cma_plan(self, plan_unscaled):
        return np.zeros_like(plan_unscaled)

    def _scale_cma_vector(self, vector_unscaled):
        return np.zeros_like(vector_unscaled)

    def _get_param_bounds(self):
        param_bounds = []
        cpp_config = self.config['cpp_constraints']
        for _ in range(self.config['horizon']):
            for name in self.config['cpp_names']:
                param_bounds.append((cpp_config[name]['min_val'], cpp_config[name]['max_val']))
        return param_bounds
