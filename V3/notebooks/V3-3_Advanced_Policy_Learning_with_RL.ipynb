{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3 Notebook 3: Advanced Policy Learning with Reinforcement Learning\n",
    "\n",
    "**Project:** `AutoPharm` (V3)\n",
    "**Goal:** To explore a fundamentally different approach to control by learning a policy directly from environmental interaction. This notebook implements a Reinforcement Learning (RL) agent using the Stable-Baselines3 library to control our simulated plant, moving beyond model-based control to direct policy optimization.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Theory: Beyond Prediction to Direct Action](#1.-Theory:-Beyond-Prediction-to-Direct-Action)\n",
    "2. [Framing the Control Problem for RL](#2.-Framing-the-Control-Problem-for-RL)\n",
    "3. [Building a Custom Gym Environment](#3.-Building-a-Custom-Gym-Environment)\n",
    "4. [Training the RL Agent](#4.-Training-the-RL-Agent)\n",
    "5. [Evaluating the Learned Policy](#5.-Evaluating-the-Learned-Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Theory: Beyond Prediction to Direct Action\n",
    "\n",
    "Our MPC approach (V1 and V2) is **model-based**. Its effectiveness is fundamentally limited by the accuracy of its predictive model. If the model is wrong, the control decisions will be suboptimal. Furthermore, the multi-step process of predicting, evaluating, and optimizing can be computationally intensive.\n",
    "\n",
    "**Reinforcement Learning (RL)** offers a **model-free** alternative. Instead of learning *what will happen*, the RL agent learns *what to do*. It directly learns a **policy**, which is a mapping from a given state to an optimal action.\n",
    "\n",
    "The agent learns through a process of trial and error:\n",
    "1.  It **observes** the state of the environment.\n",
    "2.  It takes an **action** based on its current policy.\n",
    "3.  The environment transitions to a new state and gives the agent a **reward** (or penalty).\n",
    "4.  The agent updates its policy to favor actions that lead to higher cumulative rewards over time.\n",
    "\n",
    "For complex, nonlinear systems, an RL agent can potentially discover highly effective, non-intuitive control policies that are difficult to formulate with a model-based approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Framing the Control Problem for RL\n",
    "\n",
    "To apply RL, we must define our problem in the standard RL terminology:\n",
    "\n",
    "*   **Environment:** Our `AdvancedPlantSimulator`.\n",
    "*   **Agent:** The RL algorithm we choose (e.g., PPO, SAC from Stable-Baselines3).\n",
    "*   **State (Observation):** A vector representing the current state of the plant. This must include the current CMAs (`d50`, `LOD`), the current CPPs, and the target setpoints. The agent needs to know both where it is and where it's supposed to go. A history of past states could also be included.\n",
    "*   **Action Space:** A continuous range of values for each of our controllable CPPs (`spray_rate`, `air_flow`, `carousel_speed`). The agent will output an action within this space.\n",
    "*   **Reward Function:** This is the most critical part of the design. The reward function guides the entire learning process. A good reward function for our problem would:\n",
    "    *   Give a large positive reward for being close to the CMA setpoints.\n",
    "    *   Give a small negative reward for large changes in CPPs (to encourage smooth control).\n",
    "    *   Give a large negative penalty for violating process constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Building a Custom Gym Environment\n",
    "\n",
    "RL libraries like Stable-Baselines3 expect the environment to follow the `gymnasium` (formerly OpenAI Gym) API. We need to create a wrapper around our `AdvancedPlantSimulator` that implements this standard interface (`step`, `reset`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from autopharm_core.rl.environment import GranulationEnv\n",
    "from autopharm_core.common.types import StateVector, ControlAction\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test our custom Gymnasium environment to ensure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "ENV_CONFIG = {\n",
    "    'initial_cpps': {'spray_rate': 120.0, 'air_flow': 500.0, 'carousel_speed': 30.0},\n",
    "    'target_d50': 400.0,\n",
    "    'target_lod': 2.0,\n",
    "    'episode_length': 500\n",
    "}\n",
    "\n",
    "# Create and test the environment\n",
    "env = GranulationEnv(config=ENV_CONFIG)\n",
    "\n",
    "print(\"Environment created successfully!\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Test environment step\n",
    "obs, _ = env.reset()\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "\n",
    "# Take a random action\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"After random action {action}: obs={obs[:2]}, reward={reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Training the RL Agent\n",
    "\n",
    "With our custom environment in place, we can now use Stable-Baselines3 to train an RL agent. We will use the **Proximal Policy Optimization (PPO)** algorithm, which is a robust, state-of-the-art choice for continuous control problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "\n",
    "# Create directories for model and logs\n",
    "os.makedirs('../data/models', exist_ok=True)\n",
    "os.makedirs('../data/tensorboard', exist_ok=True)\n",
    "\n",
    "MODEL_SAVE_PATH = \"../data/models/ppo_granulation_policy\"\n",
    "\n",
    "# --- 1. Create and check the environment ---\n",
    "train_env = GranulationEnv(config=ENV_CONFIG)\n",
    "train_env = Monitor(train_env)  # Monitor for logging\n",
    "\n",
    "eval_env = GranulationEnv(config=ENV_CONFIG)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "check_env(train_env, warn=True)\n",
    "print(\"Environment check passed!\")\n",
    "\n",
    "# --- 2. Configure PPO with optimized hyperparameters ---\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    train_env, \n",
    "    verbose=1,\n",
    "    tensorboard_log=\"../data/tensorboard/\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01\n",
    ")\n",
    "\n",
    "# --- 3. Set up evaluation callback ---\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path=MODEL_SAVE_PATH,\n",
    "    log_path=\"../data/models/\", \n",
    "    eval_freq=10000,\n",
    "    deterministic=True, \n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(\"\\nStarting RL training... (This will take some time)\")\n",
    "print(\"Training with 100,000 timesteps for demonstration\")\n",
    "print(\"For production use, consider 1,000,000+ timesteps\")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000, callback=eval_callback)\n",
    "\n",
    "# --- 4. Save the final policy ---\n",
    "model.save(MODEL_SAVE_PATH + \"_final\")\n",
    "print(f\"\\nTraining complete. Policy saved to {MODEL_SAVE_PATH}_final.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Evaluating the Learned Policy\n",
    "\n",
    "The final step is to evaluate how well our trained agent can control the plant. We will load the saved policy and run it on our environment for a fixed number of steps, logging the results to see if it successfully drives the state to the target setpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the best trained model ---\n",
    "try:\n",
    "    trained_model = PPO.load(MODEL_SAVE_PATH + \"_final\")\n",
    "    print(\"Loaded final model\")\n",
    "except:\n",
    "    try:\n",
    "        trained_model = PPO.load(MODEL_SAVE_PATH + \"/best_model\")\n",
    "        print(\"Loaded best model from evaluation callback\")\n",
    "    except:\n",
    "        print(\"No trained model found. Using the current model.\")\n",
    "        trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run evaluation loop ---\n",
    "eval_env_test = GranulationEnv(config=ENV_CONFIG)\n",
    "obs, _ = eval_env_test.reset()\n",
    "\n",
    "log = []\n",
    "cumulative_reward = 0\n",
    "\n",
    "print(\"Running policy evaluation for 500 timesteps...\")\n",
    "\n",
    "for i in range(500):\n",
    "    action, _states = trained_model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env_test.step(action)\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "    # Extract current state information\n",
    "    current_state = obs\n",
    "    log_entry = {\n",
    "        'time': i,\n",
    "        'd50': current_state[0],\n",
    "        'lod': current_state[1],\n",
    "        'd50_target': current_state[2],\n",
    "        'lod_target': current_state[3],\n",
    "        'spray_rate': current_state[4],\n",
    "        'air_flow': current_state[5],\n",
    "        'carousel_speed': current_state[6],\n",
    "        'reward': reward,\n",
    "        'cumulative_reward': cumulative_reward,\n",
    "        'action_spray_rate': action[0],\n",
    "        'action_air_flow': action[1],\n",
    "        'action_carousel_speed': action[2]\n",
    "    }\n",
    "    log.append(log_entry)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        obs, _ = eval_env_test.reset()\n",
    "\n",
    "df_eval = pd.DataFrame(log)\n",
    "print(f\"Evaluation complete. Final cumulative reward: {cumulative_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Visualization ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "fig.suptitle('RL Policy Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. CMA Tracking Performance\n",
    "axes[0,0].plot(df_eval['time'], df_eval['d50'], label='d50 (Actual)', color='blue', linewidth=2)\n",
    "axes[0,0].axhline(y=df_eval['d50_target'].iloc[0], color='blue', linestyle='--', \n",
    "                  label=f'd50 Target ({df_eval[\"d50_target\"].iloc[0]:.0f})', alpha=0.7)\n",
    "\n",
    "ax1b = axes[0,0].twinx()\n",
    "ax1b.plot(df_eval['time'], df_eval['lod'], label='LOD (Actual)', color='red', linewidth=2)\n",
    "ax1b.axhline(y=df_eval['lod_target'].iloc[0], color='red', linestyle='--', \n",
    "             label=f'LOD Target ({df_eval[\"lod_target\"].iloc[0]:.1f})', alpha=0.7)\n",
    "\n",
    "axes[0,0].set_title('CMA Tracking Performance', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Time Steps')\n",
    "axes[0,0].set_ylabel('d50 (μm)', color='blue')\n",
    "ax1b.set_ylabel('LOD (%)', color='red')\n",
    "axes[0,0].legend(loc='upper left')\n",
    "ax1b.legend(loc='upper right')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Control Actions\n",
    "axes[0,1].plot(df_eval['time'], df_eval['spray_rate'], label='Spray Rate', linewidth=2)\n",
    "axes[0,1].plot(df_eval['time'], df_eval['air_flow'], label='Air Flow', linewidth=2)\n",
    "axes[0,1].plot(df_eval['time'], df_eval['carousel_speed'], label='Carousel Speed', linewidth=2)\n",
    "axes[0,1].set_title('Control Actions (CPPs)', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Time Steps')\n",
    "axes[0,1].set_ylabel('CPP Values')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Reward Evolution\n",
    "axes[1,0].plot(df_eval['time'], df_eval['reward'], label='Instantaneous Reward', alpha=0.6)\n",
    "# Rolling average for smoother visualization\n",
    "rolling_reward = df_eval['reward'].rolling(window=20, center=True).mean()\n",
    "axes[1,0].plot(df_eval['time'], rolling_reward, label='Reward (20-step avg)', linewidth=2, color='red')\n",
    "axes[1,0].set_title('Reward Signal Evolution', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Time Steps')\n",
    "axes[1,0].set_ylabel('Reward')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Action Changes (Policy Smoothness)\n",
    "axes[1,1].plot(df_eval['time'], df_eval['action_spray_rate'], label='Δ Spray Rate', alpha=0.8)\n",
    "axes[1,1].plot(df_eval['time'], df_eval['action_air_flow'], label='Δ Air Flow', alpha=0.8)\n",
    "axes[1,1].plot(df_eval['time'], df_eval['action_carousel_speed'], label='Δ Carousel Speed', alpha=0.8)\n",
    "axes[1,1].set_title('Action Changes (Policy Smoothness)', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Time Steps')\n",
    "axes[1,1].set_ylabel('Action Delta')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance Analysis ---\n",
    "print(\"=== RL Policy Performance Analysis ===\")\n",
    "print()\n",
    "\n",
    "# Calculate final tracking errors\n",
    "final_d50_error = abs(df_eval['d50'].iloc[-1] - df_eval['d50_target'].iloc[-1])\n",
    "final_lod_error = abs(df_eval['lod'].iloc[-1] - df_eval['lod_target'].iloc[-1])\n",
    "\n",
    "# Calculate average tracking errors\n",
    "avg_d50_error = abs(df_eval['d50'] - df_eval['d50_target']).mean()\n",
    "avg_lod_error = abs(df_eval['lod'] - df_eval['lod_target']).mean()\n",
    "\n",
    "print(f\"Final Tracking Performance:\")\n",
    "print(f\"  d50 Error: {final_d50_error:.2f} μm (Target: {df_eval['d50_target'].iloc[-1]:.0f}μm)\")\n",
    "print(f\"  LOD Error: {final_lod_error:.3f} % (Target: {df_eval['lod_target'].iloc[-1]:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(f\"Average Tracking Performance:\")\n",
    "print(f\"  d50 Error: {avg_d50_error:.2f} μm\")\n",
    "print(f\"  LOD Error: {avg_lod_error:.3f} %\")\n",
    "print()\n",
    "\n",
    "# Control smoothness analysis\n",
    "action_variability = {\n",
    "    'spray_rate': df_eval['action_spray_rate'].std(),\n",
    "    'air_flow': df_eval['action_air_flow'].std(),\n",
    "    'carousel_speed': df_eval['action_carousel_speed'].std()\n",
    "}\n",
    "\n",
    "print(f\"Control Action Variability (lower is smoother):\")\n",
    "for param, std_val in action_variability.items():\n",
    "    print(f\"  {param}: {std_val:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Learning Metrics:\")\n",
    "print(f\"  Total Cumulative Reward: {cumulative_reward:.2f}\")\n",
    "print(f\"  Average Reward per Step: {cumulative_reward/len(df_eval):.3f}\")\n",
    "print(f\"  Final Reward: {df_eval['reward'].iloc[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis and Conclusions\n",
    "\n",
    "The evaluation plots above demonstrate the performance of our RL policy that was learned entirely through trial and error interaction with the plant simulator. Key observations:\n",
    "\n",
    "**Strengths of the RL Approach:**\n",
    "- **Model-Free Learning**: The agent learns effective control strategies without requiring an explicit process model\n",
    "- **Direct Policy Optimization**: Actions are chosen directly from states, potentially faster than MPC optimization\n",
    "- **Adaptive Behavior**: The policy can adapt to different setpoints and process conditions through training diversity\n",
    "- **Non-Linear Control**: Can discover complex, non-obvious control strategies that linear controllers might miss\n",
    "\n",
    "**Implementation Insights:**\n",
    "- **Reward Engineering**: The reward function design is critical - it must balance tracking performance with control smoothness\n",
    "- **Environment Design**: Proper state representation (including targets) and action space bounds are essential\n",
    "- **Training Time**: RL requires significant computational time but results in a fast inference policy\n",
    "- **Exploration vs. Exploitation**: PPO balances trying new actions with exploiting known good strategies\n",
    "\n",
    "**Comparison with MPC (V1/V2):**\n",
    "- **MPC Advantages**: Explicit constraints, predictable behavior, interpretable optimization\n",
    "- **RL Advantages**: No model required, potentially superior non-linear control, fast execution\n",
    "- **Hybrid Potential**: RL could learn high-level strategies while MPC handles low-level optimization\n",
    "\n",
    "This notebook completes the third and final pillar of the **AutoPharm V3** framework. By implementing Reinforcement Learning, we have opened the door to a fundamentally different class of control strategies that can potentially surpass traditional model-based approaches for complex, non-linear pharmaceutical processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}