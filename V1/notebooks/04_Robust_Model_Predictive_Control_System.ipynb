{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Robust Model Predictive Control System\n",
    "\n",
    "**Project:** `PharmaControl-Pro`\n",
    "**Goal:** Build the 'brain' of our control system. This involves creating a robust Model Predictive Control (MPC) class that encapsulates the logic for real-time decision-making. This controller will use the trained model to find the best control action while respecting process constraints.\n",
    "\n",
    "### Table of Contents\n",
    "1. [The MPC Controller: Architecture and Logic](#1.-The-MPC-Controller:-Architecture-and-Logic)\n",
    "2. [Implementing the MPC Controller Class](#2.-Implementing-the-MPC-Controller-Class)\n",
    "3. [Defining Process Constraints](#3.-Defining-Process-Constraints)\n",
    "4. [Standalone Test of the MPC Controller](#4.-Standalone-Test-of-the-MPC-Controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. The MPC Controller: Architecture and Logic\n",
    "\n",
    "The MPC controller's job is to answer one question at each decision point: **\"Given the current state of the plant, what is the best possible action to take right now to meet our future targets?\"**\n",
    "\n",
    "To do this, it follows a precise, multi-step algorithm based on the principles we discussed in Notebook 1:\n",
    "\n",
    "1.  **Generate Candidate Actions:** Create a 'lattice' or grid of potential future control plans. For example, 'Plan A: increase spray rate by 5%', 'Plan B: hold steady', 'Plan C: decrease air flow by 10 m³/h', and so on. This is our search space.\n",
    "\n",
    "2.  **Enforce Constraints:** This is a critical safety and quality step. The controller must filter out any candidate plans that are physically impossible or would violate process limits (e.g., a negative flow rate, a change that is too rapid, or a value outside the equipment's operational range).\n",
    "\n",
    "3.  **Predict and Evaluate:** For every *valid* candidate plan, use our trained `GranulationPredictor` model to forecast the future CMAs over the horizon `H`. Then, calculate a 'cost' for each prediction. The cost function is key:\n",
    "    *   It heavily penalizes deviations from the target CMAs (`target_error_cost`).\n",
    "    *   It can also lightly penalize large or rapid changes in the control actions themselves to promote smooth, stable operation (`control_effort_cost`).\n",
    "    *   `Total Cost = target_error_cost + λ * control_effort_cost`\n",
    "\n",
    "4.  **Select Optimal Action:** The candidate plan with the lowest total cost is the winner.\n",
    "\n",
    "5.  **Apply Receding Horizon:** The controller returns only the *very first step* of the winning plan. This action is sent to the plant. At the next decision point, the entire process is repeated, ensuring the controller is always reacting to the latest information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Implementing the MPC Controller Class\n",
    "\n",
    "We will encapsulate all this logic into a reusable `MPCController` class. This class will be defined in `src/mpc_controller.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/mpc_controller.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/mpc_controller.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class MPCController:\n",
    "    \"\"\"\n",
    "    Implements a Model Predictive Controller that uses a trained PyTorch model\n",
    "    to find optimal control actions while respecting process constraints.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config, constraints, scalers):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.constraints = constraints\n",
    "        self.scalers = scalers\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _generate_control_lattice(self, current_cpps):\n",
    "        \"\"\"Creates a grid of possible future control sequences.\"\"\"\n",
    "        cpp_names = self.config['cpp_names']\n",
    "        discretization = self.config['discretization_steps']\n",
    "        \n",
    "        # For each CPP, create a set of possible delta (change) values\n",
    "        # Example: [-5.0, 0.0, 5.0] for spray_rate\n",
    "        delta_options = []\n",
    "        for name in cpp_names:\n",
    "            max_change = self.constraints[name]['max_change_per_step']\n",
    "            options = np.linspace(-max_change, max_change, discretization)\n",
    "            delta_options.append(options)\n",
    "        \n",
    "        # Create the Cartesian product of all delta options\n",
    "        # This gives every possible combination of a single-step change\n",
    "        candidate_deltas = list(itertools.product(*delta_options))\n",
    "        \n",
    "        # Assume the change is held constant over the horizon H\n",
    "        candidate_sequences = []\n",
    "        for deltas in candidate_deltas:\n",
    "            # Create a plan by applying the deltas to the current CPPs\n",
    "            new_cpp_step = current_cpps + np.array(deltas)\n",
    "            # Create a full horizon sequence by repeating this step\n",
    "            sequence = np.tile(new_cpp_step, (self.config['horizon'], 1))\n",
    "            candidate_sequences.append(sequence)\n",
    "            \n",
    "        return candidate_sequences\n",
    "\n",
    "    def _filter_by_constraints(self, candidates, current_cpps):\n",
    "        \"\"\"Removes candidate sequences that violate process constraints.\"\"\"\n",
    "        valid_candidates = []\n",
    "        cpp_names = self.config['cpp_names']\n",
    "        \n",
    "        for seq in candidates:\n",
    "            is_valid = True\n",
    "            # We only need to check the first step, as it's held constant\n",
    "            first_step = seq[0]\n",
    "            for i, name in enumerate(cpp_names):\n",
    "                # Check min/max operational limits\n",
    "                if not (self.constraints[name]['min_val'] <= first_step[i] <= self.constraints[name]['max_val']):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "            \n",
    "            if is_valid:\n",
    "                valid_candidates.append(seq)\n",
    "        \n",
    "        return valid_candidates\n",
    "\n",
    "    def _calculate_cost(self, prediction, action, target_cmas):\n",
    "        \"\"\"Calculates the cost of a predicted trajectory.\"\"\"\n",
    "        # Ensure target is on the correct device\n",
    "        target_cmas = target_cmas.to(self.device)\n",
    "        \n",
    "        # 1. Target Error Cost (how far are we from the setpoint?)\n",
    "        # Using L1 loss (Mean Absolute Error) is often more robust to outliers\n",
    "        target_error = torch.mean(torch.abs(prediction - target_cmas))\n",
    "        \n",
    "        # 2. Control Effort Cost (penalize large changes to promote stability)\n",
    "        # This is a placeholder; a more complex version could penalize deviation from a desired steady state\n",
    "        control_effort = torch.mean(torch.abs(action - action[0])) # Penalize non-constant actions\n",
    "        \n",
    "        # Combine costs with a weighting factor (lambda)\n",
    "        total_cost = target_error + self.config['control_effort_lambda'] * control_effort\n",
    "        return total_cost.item()\n",
    "\n",
    "    def suggest_action(self, past_cmas_scaled, past_cpps_scaled, target_cmas_unscaled):\n",
    "        \"\"\"The main MPC loop to find and return the best single control action.\"\"\"\n",
    "        # Get the last known CPPs (unscaled) to base our search on\n",
    "        last_cpps_scaled = past_cpps_scaled[-1, :]\n",
    "        current_cpps_unscaled = np.zeros(len(self.config['cpp_names']))\n",
    "        for i, name in enumerate(self.config['cpp_names']):\n",
    "            current_cpps_unscaled[i] = self.scalers[name].inverse_transform(last_cpps_scaled[i].reshape(-1, 1))\n",
    "        \n",
    "        # 1. Generate all possible actions\n",
    "        candidates_unscaled = self._generate_control_lattice(current_cpps_unscaled)\n",
    "        \n",
    "        # 2. Filter out invalid actions\n",
    "        valid_candidates_unscaled = self._filter_by_constraints(candidates_unscaled, current_cpps_unscaled)\n",
    "        \n",
    "        if not valid_candidates_unscaled:\n",
    "            print(\"Warning: No valid control actions found after applying constraints.\")\n",
    "            return current_cpps_unscaled # Return the last known safe action\n",
    "        \n",
    "        # 3. Predict and Evaluate\n",
    "        best_cost = float('inf')\n",
    "        best_action_sequence = None\n",
    "        \n",
    "        # Prepare scaled target tensor once\n",
    "        target_cmas_scaled = np.zeros_like(target_cmas_unscaled)\n",
    "        for i, name in enumerate(self.config['cma_names']):\n",
    "            target_cmas_scaled[:, i] = self.scalers[name].transform(target_cmas_unscaled[:, i].reshape(-1, 1)).flatten()\n",
    "        target_cmas_tensor = torch.tensor(target_cmas_scaled, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Convert historical data to tensors\n",
    "        past_cmas_tensor = torch.tensor(past_cmas_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        past_cpps_tensor = torch.tensor(past_cpps_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(valid_candidates_unscaled, desc=\"Evaluating MPC Candidates\", leave=False)\n",
    "            for action_seq_unscaled in pbar:\n",
    "                # Scale the candidate action sequence for the model\n",
    "                action_seq_scaled = np.zeros_like(action_seq_unscaled)\n",
    "                for i, name in enumerate(self.config['cpp_names_and_soft_sensors']):\n",
    "                     if name in self.scalers:\n",
    "                        # This part needs to be improved to handle soft sensors correctly.\n",
    "                        # For now, we assume we can scale the base CPPs.\n",
    "                        if i < len(self.config['cpp_names']):\n",
    "                             action_seq_scaled[:, i] = self.scalers[name].transform(action_seq_unscaled[:, i].reshape(-1, 1)).flatten()\n",
    "                \n",
    "                action_tensor = torch.tensor(action_seq_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Get model prediction\n",
    "                prediction = self.model(past_cmas_tensor, past_cpps_tensor, action_tensor)\n",
    "                \n",
    "                # Calculate cost\n",
    "                cost = self._calculate_cost(prediction, action_tensor, target_cmas_tensor)\n",
    "                \n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_action_sequence = action_seq_unscaled\n",
    "        \n",
    "        # 4. Return the first step of the best plan found\n",
    "        return best_action_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Defining Process Constraints\n",
    "\n",
    "A real industrial process has strict limits. The controller must never be allowed to suggest an action that could damage the equipment or ruin the product. We will define these limits in a clear, structured dictionary that our `MPCController` can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_CONSTRAINTS = {\n",
    "    'spray_rate': {\n",
    "        'min_val': 80.0,              # Equipment minimum\n",
    "        'max_val': 180.0,             # Equipment maximum\n",
    "        'max_change_per_step': 10.0   # Max allowed change in one go (for stability)\n",
    "    },\n",
    "    'air_flow': {\n",
    "        'min_val': 400.0,\n",
    "        'max_val': 700.0,\n",
    "        'max_change_per_step': 25.0\n",
    "    },\n",
    "    'carousel_speed': {\n",
    "        'min_val': 20.0,\n",
    "        'max_val': 40.0,\n",
    "        'max_change_per_step': 2.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Standalone Test of the MPC Controller\n",
    "\n",
    "Before we connect the controller to our simulator in the final notebook, let's perform a standalone test. We will create some dummy historical data, define a target, and ask the controller for a single best action. This allows us to debug the controller's logic in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/best_predictor_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     21\u001b[39m CPP_COLS_FULL = [\u001b[33m'\u001b[39m\u001b[33mspray_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mair_flow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcarousel_speed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mspecific_energy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfroude_number_proxy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m model = GranulationPredictor(\n\u001b[32m     24\u001b[39m     cma_features=\u001b[38;5;28mlen\u001b[39m(CMA_COLS),\n\u001b[32m     25\u001b[39m     cpp_features=\u001b[38;5;28mlen\u001b[39m(CPP_COLS_FULL),\n\u001b[32m     26\u001b[39m     **BEST_HPARAMS\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Load scalers\u001b[39;00m\n\u001b[32m     31\u001b[39m scalers = joblib.load(SCALER_FILE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PharmaControl/.venv/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PharmaControl/.venv/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PharmaControl/.venv/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/best_predictor_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import os, sys\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "import pandas as pd\n",
    "from src.model_architecture import GranulationPredictor\n",
    "from src.mpc_controller import MPCController\n",
    "\n",
    "# --- Load all necessary components ---\n",
    "DATA_DIR = '../data'\n",
    "MODEL_SAVE_PATH = os.path.join(DATA_DIR, 'best_predictor_model.pth')\n",
    "SCALER_FILE = os.path.join(DATA_DIR, 'scalers.joblib')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load model (need to know its hyperparameters to instantiate)\n",
    "# For this test, we'll hardcode them from the previous notebook's tuning results.\n",
    "# In a real app, these would be saved in a config file.\n",
    "BEST_HPARAMS = {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.15}\n",
    "CMA_COLS = ['d50', 'lod']\n",
    "CPP_COLS_BASE = ['spray_rate', 'air_flow', 'carousel_speed']\n",
    "CPP_COLS_FULL = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
    "\n",
    "model = GranulationPredictor(\n",
    "    cma_features=len(CMA_COLS),\n",
    "    cpp_features=len(CPP_COLS_FULL),\n",
    "    **BEST_HPARAMS\n",
    ")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "# Load scalers\n",
    "scalers = joblib.load(SCALER_FILE)\n",
    "\n",
    "# --- MPC Configuration ---\n",
    "MPC_CONFIG = {\n",
    "    'horizon': 72,\n",
    "    'cpp_names': CPP_COLS_BASE,\n",
    "    'cma_names': CMA_COLS,\n",
    "    'cpp_names_and_soft_sensors': CPP_COLS_FULL,\n",
    "    'discretization_steps': 3, # Use 3 steps for each CPP delta: [-max, 0, +max]\n",
    "    'control_effort_lambda': 0.05 # Small penalty for control changes\n",
    "}\n",
    "\n",
    "# --- Instantiate the Controller ---\n",
    "mpc = MPCController(model, MPC_CONFIG, PROCESS_CONSTRAINTS, scalers)\n",
    "print(\"MPC Controller instantiated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Dummy Data for the Test ---\n",
    "\n",
    "# Create some fake historical data (36 steps)\n",
    "# In a real scenario, this comes from the live plant data stream.\n",
    "dummy_past_cmas_unscaled = np.array([[450, 1.2]] * 36)\n",
    "dummy_past_cpps_unscaled = np.array([[130, 550, 30]] * 36)\n",
    "\n",
    "# Note: The controller needs the soft sensors, so we must add them.\n",
    "# For this simple test, we will approximate them.\n",
    "dummy_se = (dummy_past_cpps_unscaled[:, 0] * dummy_past_cpps_unscaled[:, 2]) / 1000.0\n",
    "dummy_fr = (dummy_past_cpps_unscaled[:, 2]**2) / 9.81\n",
    "dummy_past_cpps_full_unscaled = np.hstack([\n",
    "    dummy_past_cpps_unscaled,\n",
    "    dummy_se.reshape(-1, 1),\n",
    "    dummy_fr.reshape(-1, 1)\n",
    "])\n",
    "\n",
    "# Scale the dummy data just as we would in a real loop\n",
    "past_cmas_scaled = np.zeros_like(dummy_past_cmas_unscaled)\n",
    "past_cpps_scaled = np.zeros_like(dummy_past_cpps_full_unscaled)\n",
    "\n",
    "for i, col in enumerate(CMA_COLS):\n",
    "    past_cmas_scaled[:, i] = scalers[col].transform(dummy_past_cmas_unscaled[:, i].reshape(-1, 1)).flatten()\n",
    "for i, col in enumerate(CPP_COLS_FULL):\n",
    "    past_cpps_scaled[:, i] = scalers[col].transform(dummy_past_cpps_full_unscaled[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define a target we want to reach\n",
    "# We want smaller, wetter granules.\n",
    "target_cmas_unscaled = np.tile([350.0, 1.8], (MPC_CONFIG['horizon'], 1))\n",
    "\n",
    "# --- Run the MPC Decision Logic ---\n",
    "print(f\"Current State (unscaled): d50={dummy_past_cmas_unscaled[-1, 0]}, lod={dummy_past_cmas_unscaled[-1, 1]}\")\n",
    "print(f\"Target State (unscaled):  d50={target_cmas_unscaled[0, 0]}, lod={target_cmas_unscaled[0, 1]}\")\n",
    "\n",
    "suggested_action = mpc.suggest_action(past_cmas_scaled, past_cpps_scaled, target_cmas_unscaled)\n",
    "\n",
    "print(\"\\n--- MPC Suggestion ---\")\n",
    "print(\"To reach the target, the best immediate action is:\")\n",
    "for i, name in enumerate(MPC_CONFIG['cpp_names']):\n",
    "    print(f\"  {name}: {suggested_action[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Suggestion\n",
    "\n",
    "To reach a target of smaller (`d50`=350) and wetter (`LOD`=1.8) granules from a state of larger, drier ones, we expect the controller to suggest actions that:\n",
    "1.  **Reduce Granule Size:** This is primarily achieved by decreasing the `spray_rate`.\n",
    "2.  **Increase Moisture:** This can be done by decreasing the `air_flow` or increasing the `carousel_speed` (reducing drying time).\n",
    "\n",
    "Check if the controller's suggested action aligns with this physical intuition. This confirms that the model has learned meaningful relationships and the MPC logic is working correctly.\n",
    "\n",
    "We are now ready to put all the pieces together in the final notebook and run a full closed-loop simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
