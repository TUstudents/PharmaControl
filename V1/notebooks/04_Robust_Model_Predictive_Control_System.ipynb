{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Robust Model Predictive Control System\n",
    "\n",
    "**Project:** `PharmaControl-Pro`\n",
    "**Goal:** Build the 'brain' of our control system. This involves creating a robust Model Predictive Control (MPC) class that encapsulates the logic for real-time decision-making. This controller will use the trained model to find the best control action while respecting process constraints.\n",
    "\n",
    "### Table of Contents\n",
    "1. [The MPC Controller: Architecture and Logic](#1.-The-MPC-Controller:-Architecture-and-Logic)\n",
    "2. [Implementing the MPC Controller Class](#2.-Implementing-the-MPC-Controller-Class)\n",
    "3. [Defining Process Constraints](#3.-Defining-Process-Constraints)\n",
    "4. [Standalone Test of the MPC Controller](#4.-Standalone-Test-of-the-MPC-Controller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. The MPC Controller: Architecture and Logic\n",
    "\n",
    "The MPC controller's job is to answer one question at each decision point: **\"Given the current state of the plant, what is the best possible action to take right now to meet our future targets?\"**\n",
    "\n",
    "To do this, it follows a precise, multi-step algorithm based on the principles we discussed in Notebook 1:\n",
    "\n",
    "1.  **Generate Candidate Actions:** Create a 'lattice' or grid of potential future control plans. For example, 'Plan A: increase spray rate by 5%', 'Plan B: hold steady', 'Plan C: decrease air flow by 10 m³/h', and so on. This is our search space.\n",
    "\n",
    "2.  **Enforce Constraints:** This is a critical safety and quality step. The controller must filter out any candidate plans that are physically impossible or would violate process limits (e.g., a negative flow rate, a change that is too rapid, or a value outside the equipment's operational range).\n",
    "\n",
    "3.  **Predict and Evaluate:** For every *valid* candidate plan, use our trained `GranulationPredictor` model to forecast the future CMAs over the horizon `H`. Then, calculate a 'cost' for each prediction. The cost function is key:\n",
    "    *   It heavily penalizes deviations from the target CMAs (`target_error_cost`).\n",
    "    *   It can also lightly penalize large or rapid changes in the control actions themselves to promote smooth, stable operation (`control_effort_cost`).\n",
    "    *   `Total Cost = target_error_cost + λ * control_effort_cost`\n",
    "\n",
    "4.  **Select Optimal Action:** The candidate plan with the lowest total cost is the winner.\n",
    "\n",
    "5.  **Apply Receding Horizon:** The controller returns only the *very first step* of the winning plan. This action is sent to the plant. At the next decision point, the entire process is repeated, ensuring the controller is always reacting to the latest information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Implementing the MPC Controller Class\n",
    "\n",
    "We will encapsulate all this logic into a reusable `MPCController` class. This class will be defined in `src/mpc_controller.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%writefile ../src/mpc_controller.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MPCController:\n",
    "    \"\"\"Model Predictive Controller for pharmaceutical continuous granulation processes.\n",
    "    \n",
    "    This controller implements a discrete optimization-based MPC that uses a trained\n",
    "    neural network predictor to find optimal control actions while respecting \n",
    "    operational constraints and minimizing tracking error.\n",
    "    \n",
    "    The controller performs exhaustive grid search over discretized control changes,\n",
    "    evaluates each candidate using the predictive model, and selects the action\n",
    "    that minimizes a weighted combination of target tracking error and control effort.\n",
    "    \n",
    "    Attributes:\n",
    "        model: Trained PyTorch neural network for process prediction\n",
    "        config: Configuration dictionary containing control parameters\n",
    "        constraints: Process constraint definitions for each control variable\n",
    "        scalers: Data scalers used during model training for consistent preprocessing\n",
    "        device: PyTorch device (CPU/GPU) for model execution\n",
    "    \n",
    "    Example:\n",
    "        >>> controller = MPCController(model, config, constraints, scalers)\n",
    "        >>> optimal_action = controller.suggest_action(past_cmas, past_cpps, targets)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config, constraints, scalers):\n",
    "        \"\"\"Initialize the MPC controller with model and process configuration.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained PyTorch neural network model for process prediction.\n",
    "                Must implement forward(past_cmas, past_cpps, future_cpps) -> predictions\n",
    "            config: Configuration dictionary containing:\n",
    "                - 'cpp_names': List of critical process parameter names\n",
    "                - 'cma_names': List of critical material attribute names  \n",
    "                - 'cpp_names_and_soft_sensors': Extended CPP list including soft sensors\n",
    "                - 'horizon': Prediction and control horizon length\n",
    "                - 'discretization_steps': Number of discrete control options per variable\n",
    "                - 'control_effort_lambda': Weight for control effort penalty term\n",
    "            constraints: Constraint dictionary with structure:\n",
    "                {variable_name: {'min_val': float, 'max_val': float, 'max_change_per_step': float}}\n",
    "            scalers: Dictionary of fitted sklearn scalers for data preprocessing:\n",
    "                {variable_name: fitted_scaler_object}\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If required configuration keys are missing\n",
    "            RuntimeError: If model cannot be moved to available device\n",
    "        \"\"\"\n",
    "        # ADDED: Comprehensive input validation\n",
    "        self._validate_initialization_inputs(model, config, constraints, scalers)\n",
    "        \n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.constraints = constraints\n",
    "        self.scalers = scalers\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        try:\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to move model to device {self.device}: {e}\")\n",
    "\n",
    "    def _validate_initialization_inputs(self, model, config, constraints, scalers):\n",
    "        \"\"\"Validate inputs during controller initialization.\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model to validate\n",
    "            config: Configuration dictionary to validate  \n",
    "            constraints: Constraints dictionary to validate\n",
    "            scalers: Scalers dictionary to validate\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If any validation check fails\n",
    "        \"\"\"\n",
    "        # Validate model\n",
    "        if not hasattr(model, 'forward'):\n",
    "            raise ValueError(\"Model must have a forward method\")\n",
    "        if not hasattr(model, 'eval'):\n",
    "            raise ValueError(\"Model must be a PyTorch model with eval() method\")\n",
    "            \n",
    "        # Validate config structure\n",
    "        required_config_keys = {\n",
    "            'cpp_names', 'cma_names', 'cpp_names_and_soft_sensors', \n",
    "            'horizon', 'discretization_steps', 'control_effort_lambda'\n",
    "        }\n",
    "        missing_keys = required_config_keys - set(config.keys())\n",
    "        if missing_keys:\n",
    "            raise ValueError(f\"Configuration missing required keys: {missing_keys}\")\n",
    "            \n",
    "        # Validate config values\n",
    "        if not isinstance(config['horizon'], int) or config['horizon'] <= 0:\n",
    "            raise ValueError(\"Horizon must be a positive integer\")\n",
    "        if not isinstance(config['discretization_steps'], int) or config['discretization_steps'] <= 0:\n",
    "            raise ValueError(\"Discretization steps must be a positive integer\")\n",
    "        if not isinstance(config['control_effort_lambda'], (int, float)) or config['control_effort_lambda'] < 0:\n",
    "            raise ValueError(\"Control effort lambda must be a non-negative number\")\n",
    "            \n",
    "        # Validate list contents\n",
    "        for key in ['cpp_names', 'cma_names', 'cpp_names_and_soft_sensors']:\n",
    "            if not isinstance(config[key], list) or len(config[key]) == 0:\n",
    "                raise ValueError(f\"{key} must be a non-empty list\")\n",
    "                \n",
    "        # Validate constraints\n",
    "        if not isinstance(constraints, dict):\n",
    "            raise ValueError(\"Constraints must be a dictionary\")\n",
    "            \n",
    "        for cpp_name in config['cpp_names']:\n",
    "            if cpp_name not in constraints:\n",
    "                raise ValueError(f\"Missing constraints for control variable: {cpp_name}\")\n",
    "                \n",
    "            constraint = constraints[cpp_name]\n",
    "            required_constraint_keys = {'min_val', 'max_val', 'max_change_per_step'}\n",
    "            missing_constraint_keys = required_constraint_keys - set(constraint.keys())\n",
    "            if missing_constraint_keys:\n",
    "                raise ValueError(f\"Constraint for {cpp_name} missing keys: {missing_constraint_keys}\")\n",
    "                \n",
    "            # Validate constraint values\n",
    "            if constraint['min_val'] >= constraint['max_val']:\n",
    "                raise ValueError(f\"Invalid constraint for {cpp_name}: min_val must be less than max_val\")\n",
    "            if constraint['max_change_per_step'] <= 0:\n",
    "                raise ValueError(f\"Invalid constraint for {cpp_name}: max_change_per_step must be positive\")\n",
    "        \n",
    "        # Validate scalers\n",
    "        if not isinstance(scalers, dict):\n",
    "            raise ValueError(\"Scalers must be a dictionary\")\n",
    "            \n",
    "        required_scaler_vars = set(config['cpp_names_and_soft_sensors'] + config['cma_names'])\n",
    "        missing_scalers = required_scaler_vars - set(scalers.keys())\n",
    "        if missing_scalers:\n",
    "            raise ValueError(f\"Missing scalers for variables: {missing_scalers}\")\n",
    "            \n",
    "        # Validate each scaler has required methods\n",
    "        for var_name, scaler in scalers.items():\n",
    "            if not hasattr(scaler, 'transform'):\n",
    "                raise ValueError(f\"Scaler for {var_name} must have transform method\")\n",
    "\n",
    "    def _validate_suggest_action_inputs(self, past_cmas_unscaled, past_cpps_unscaled, target_cmas_unscaled):\n",
    "        \"\"\"Validate inputs to suggest_action method.\n",
    "        \n",
    "        Args:\n",
    "            past_cmas_unscaled: Historical CMA data to validate\n",
    "            past_cpps_unscaled: Historical CPP data to validate  \n",
    "            target_cmas_unscaled: Target CMA data to validate\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If any validation check fails\n",
    "        \"\"\"\n",
    "        # Validate DataFrame inputs\n",
    "        if not isinstance(past_cmas_unscaled, pd.DataFrame):\n",
    "            raise ValueError(\"past_cmas_unscaled must be a pandas DataFrame\")\n",
    "        if not isinstance(past_cpps_unscaled, pd.DataFrame):\n",
    "            raise ValueError(\"past_cpps_unscaled must be a pandas DataFrame\")\n",
    "            \n",
    "        # Validate DataFrame columns\n",
    "        missing_cma_cols = set(self.config['cma_names']) - set(past_cmas_unscaled.columns)\n",
    "        if missing_cma_cols:\n",
    "            raise ValueError(f\"past_cmas_unscaled missing columns: {missing_cma_cols}\")\n",
    "            \n",
    "        missing_cpp_cols = set(self.config['cpp_names_and_soft_sensors']) - set(past_cpps_unscaled.columns)\n",
    "        if missing_cpp_cols:\n",
    "            raise ValueError(f\"past_cpps_unscaled missing columns: {missing_cpp_cols}\")\n",
    "            \n",
    "        # Validate DataFrame sizes\n",
    "        if len(past_cmas_unscaled) == 0:\n",
    "            raise ValueError(\"past_cmas_unscaled cannot be empty\")\n",
    "        if len(past_cpps_unscaled) == 0:\n",
    "            raise ValueError(\"past_cpps_unscaled cannot be empty\")\n",
    "        if len(past_cmas_unscaled) != len(past_cpps_unscaled):\n",
    "            raise ValueError(\"past_cmas_unscaled and past_cpps_unscaled must have same length\")\n",
    "            \n",
    "        # Validate target array\n",
    "        if not isinstance(target_cmas_unscaled, np.ndarray):\n",
    "            raise ValueError(\"target_cmas_unscaled must be a numpy array\")\n",
    "        if target_cmas_unscaled.ndim != 2:\n",
    "            raise ValueError(\"target_cmas_unscaled must be 2-dimensional\")\n",
    "        if target_cmas_unscaled.shape[0] != self.config['horizon']:\n",
    "            raise ValueError(f\"target_cmas_unscaled must have {self.config['horizon']} time steps\")\n",
    "        if target_cmas_unscaled.shape[1] != len(self.config['cma_names']):\n",
    "            raise ValueError(f\"target_cmas_unscaled must have {len(self.config['cma_names'])} features\")\n",
    "            \n",
    "        # Validate data for NaN/inf values\n",
    "        if past_cmas_unscaled.isnull().any().any():\n",
    "            raise ValueError(\"past_cmas_unscaled contains NaN values\")\n",
    "        if past_cpps_unscaled.isnull().any().any():\n",
    "            raise ValueError(\"past_cpps_unscaled contains NaN values\")\n",
    "        if np.any(np.isnan(target_cmas_unscaled)) or np.any(np.isinf(target_cmas_unscaled)):\n",
    "            raise ValueError(\"target_cmas_unscaled contains NaN or infinite values\")\n",
    "\n",
    "    def _generate_control_lattice(self, current_cpps):\n",
    "        \"\"\"Generate discrete control action candidates for MPC optimization.\n",
    "        \n",
    "        Creates a grid of possible control sequences by discretizing the allowed\n",
    "        change range for each control variable and generating all combinations.\n",
    "        Each sequence assumes constant control action over the prediction horizon.\n",
    "        \n",
    "        Args:\n",
    "            current_cpps: Current values of critical process parameters as numpy array.\n",
    "                Shape: (num_cpps,)\n",
    "        \n",
    "        Returns:\n",
    "            List of candidate control sequences, each as numpy array of shape\n",
    "            (horizon, num_cpps). Each sequence represents constant control values\n",
    "            applied over the entire prediction horizon.\n",
    "        \n",
    "        Notes:\n",
    "            The discretization creates n^k candidates where n is discretization_steps\n",
    "            and k is the number of control variables. This can become computationally\n",
    "            expensive for high-dimensional control spaces.\n",
    "        \"\"\"\n",
    "        cpp_names = self.config['cpp_names']\n",
    "        discretization = self.config['discretization_steps']\n",
    "\n",
    "        # For each CPP, create a set of possible delta (change) values\n",
    "        # Example: [-5.0, 0.0, 5.0] for spray_rate\n",
    "        delta_options = []\n",
    "        for name in cpp_names:\n",
    "            max_change = self.constraints[name]['max_change_per_step']\n",
    "            options = np.linspace(-max_change, max_change, discretization)\n",
    "            delta_options.append(options)\n",
    "\n",
    "        # Create the Cartesian product of all delta options\n",
    "        # This gives every possible combination of a single-step change\n",
    "        candidate_deltas = list(itertools.product(*delta_options))\n",
    "\n",
    "        # Assume the change is held constant over the horizon H\n",
    "        candidate_sequences = []\n",
    "        for deltas in candidate_deltas:\n",
    "            # Create a plan by applying the deltas to the current CPPs\n",
    "            new_cpp_step = current_cpps + np.array(deltas)\n",
    "            # Create a full horizon sequence by repeating this step\n",
    "            sequence = np.tile(new_cpp_step, (self.config['horizon'], 1))\n",
    "            candidate_sequences.append(sequence)\n",
    "\n",
    "        return candidate_sequences\n",
    "\n",
    "    def _filter_by_constraints(self, candidates, current_cpps):\n",
    "        \"\"\"Filter control candidates to ensure operational constraint compliance.\n",
    "        \n",
    "        Validates each candidate control sequence against defined operational limits\n",
    "        including minimum and maximum allowed values for each control variable.\n",
    "        \n",
    "        Args:\n",
    "            candidates: List of candidate control sequences, each as numpy array\n",
    "                of shape (horizon, num_cpps)\n",
    "            current_cpps: Current control variable values as numpy array.\n",
    "                Shape: (num_cpps,). Used for reference but not directly in filtering.\n",
    "        \n",
    "        Returns:\n",
    "            List of valid candidate control sequences that satisfy all operational\n",
    "            constraints. May be empty if no candidates satisfy constraints.\n",
    "        \n",
    "        Notes:\n",
    "            Only checks the first time step of each sequence since all sequences\n",
    "            assume constant control values. This assumes constraints are time-invariant.\n",
    "        \"\"\"\n",
    "        valid_candidates = []\n",
    "        cpp_names = self.config['cpp_names']\n",
    "\n",
    "        for seq in candidates:\n",
    "            is_valid = True\n",
    "            # We only need to check the first step, as it's held constant\n",
    "            first_step = seq[0]\n",
    "            for i, name in enumerate(cpp_names):\n",
    "                # Check min/max operational limits\n",
    "                if not (self.constraints[name]['min_val'] <= first_step[i] <= self.constraints[name]['max_val']):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "\n",
    "            if is_valid:\n",
    "                valid_candidates.append(seq)\n",
    "\n",
    "        return valid_candidates\n",
    "\n",
    "    def _calculate_cost(self, prediction, action, target_cmas, current_action_scaled):\n",
    "        \"\"\"Calculate the MPC objective function for a candidate control sequence.\n",
    "        \n",
    "        Computes a weighted combination of target tracking error and control effort\n",
    "        penalty to evaluate the quality of a proposed control action.\n",
    "        \n",
    "        Args:\n",
    "            prediction: Model prediction tensor of shape (batch, horizon, num_cmas).\n",
    "                Contains predicted critical material attributes over the horizon.\n",
    "            action: Proposed control action tensor of shape (batch, horizon, num_features).\n",
    "                Contains scaled control variables and soft sensors.\n",
    "            target_cmas: Target setpoint tensor of shape (batch, horizon, num_cmas).\n",
    "                Desired values for critical material attributes.\n",
    "            current_action_scaled: Current control action as scaled tensor of shape (num_cpps,).\n",
    "                Used for calculating control effort penalty.\n",
    "        \n",
    "        Returns:\n",
    "            Scalar cost value as float. Lower values indicate better performance.\n",
    "            Combines L1 tracking error with weighted control effort penalty.\n",
    "        \n",
    "        Notes:\n",
    "            Uses L1 (MAE) loss for tracking error as it's robust to outliers.\n",
    "            Control effort penalizes large changes from current operating point.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure target is on the correct device\n",
    "            target_cmas = target_cmas.to(self.device)\n",
    "            current_action_scaled = current_action_scaled.to(self.device)\n",
    "\n",
    "            # 1. Target Error Cost (how far are we from the setpoint?)\n",
    "            # Using L1 loss (Mean Absolute Error) is often more robust to outliers\n",
    "            target_error = torch.mean(torch.abs(prediction - target_cmas))\n",
    "\n",
    "            # 2. Control Effort Cost (penalize large changes from current state)\n",
    "            # FIXED: Compare scaled proposed action to scaled current action\n",
    "            proposed_first_action_scaled = action[0, 0, :len(self.config['cpp_names'])]  # Only base CPPs, not soft sensors\n",
    "            control_effort = torch.mean(torch.abs(proposed_first_action_scaled - current_action_scaled))\n",
    "\n",
    "            # Combine costs with a weighting factor (lambda)\n",
    "            total_cost = target_error + self.config['control_effort_lambda'] * control_effort\n",
    "            \n",
    "            # ADDED: Validate cost is finite\n",
    "            if torch.isnan(total_cost) or torch.isinf(total_cost):\n",
    "                return float('inf')\n",
    "                \n",
    "            return total_cost.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Cost calculation failed: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def suggest_action(self, past_cmas_unscaled: pd.DataFrame, past_cpps_unscaled: pd.DataFrame, target_cmas_unscaled: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute optimal control action using Model Predictive Control optimization.\n",
    "        \n",
    "        Performs discrete MPC optimization by:\n",
    "        1. Generating candidate control sequences through grid discretization\n",
    "        2. Filtering candidates to ensure constraint satisfaction\n",
    "        3. Evaluating each candidate using the neural network predictor\n",
    "        4. Selecting the action that minimizes the weighted cost function\n",
    "        \n",
    "        The optimization uses exhaustive search over a discretized control space,\n",
    "        making it computationally expensive but guaranteeing the global optimum\n",
    "        within the discretized domain.\n",
    "        \n",
    "        Args:\n",
    "            past_cmas_unscaled: Historical critical material attribute data as DataFrame.\n",
    "                Must contain columns matching config['cma_names']. Data should be\n",
    "                in original engineering units (unscaled). Shape: (lookback, num_cmas)\n",
    "            past_cpps_unscaled: Historical critical process parameter data as DataFrame.\n",
    "                Must contain columns matching config['cpp_names_and_soft_sensors'].\n",
    "                Includes both control variables and calculated soft sensors.\n",
    "                Data in original engineering units. Shape: (lookback, num_features)\n",
    "            target_cmas_unscaled: Target setpoints for critical material attributes.\n",
    "                Array of shape (horizon, num_cmas) in original engineering units.\n",
    "                Typically constant setpoints repeated over the prediction horizon.\n",
    "        \n",
    "        Returns:\n",
    "            Optimal control action as numpy array of shape (num_base_cpps,).\n",
    "            Contains unscaled values for the base critical process parameters\n",
    "            (excluding soft sensors). Values are clipped to operational constraints.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If required variables are missing from configuration or if\n",
    "                input data shapes are incompatible with model requirements.\n",
    "            RuntimeError: If model prediction fails or if all candidates produce\n",
    "                invalid costs.\n",
    "        \n",
    "        Notes:\n",
    "            - Computational complexity scales as O(n^k) where n is discretization\n",
    "              steps and k is number of control variables\n",
    "            - Returns current control values as fallback if optimization fails\n",
    "            - Soft sensors (derived variables) are automatically calculated\n",
    "        \"\"\"\n",
    "        # ADDED: Comprehensive input validation\n",
    "        try:\n",
    "            self._validate_suggest_action_inputs(past_cmas_unscaled, past_cpps_unscaled, target_cmas_unscaled)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Input validation failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract current CPPs directly from unscaled data\n",
    "            current_cpps_unscaled = past_cpps_unscaled.iloc[-1][self.config['cpp_names']].values\n",
    "            \n",
    "            # 1. Generate all possible actions\n",
    "            candidates_unscaled = self._generate_control_lattice(current_cpps_unscaled)\n",
    "\n",
    "            # 2. Filter out invalid actions\n",
    "            valid_candidates_unscaled = self._filter_by_constraints(candidates_unscaled, current_cpps_unscaled)\n",
    "\n",
    "            if not valid_candidates_unscaled:\n",
    "                print(\"Warning: No valid control actions found after applying constraints.\")\n",
    "                return current_cpps_unscaled # Return the last known safe action\n",
    "\n",
    "            # 3. Predict and Evaluate\n",
    "            best_cost = float('inf')\n",
    "            best_action_sequence = None\n",
    "\n",
    "            # Prepare scaled target tensor once using proper DataFrame\n",
    "            target_cmas_scaled = np.zeros_like(target_cmas_unscaled)\n",
    "            for i, name in enumerate(self.config['cma_names']):\n",
    "                try:\n",
    "                    target_df = pd.DataFrame(target_cmas_unscaled[:, i].reshape(-1, 1), columns=[name])\n",
    "                    target_cmas_scaled[:, i] = self.scalers[name].transform(target_df).flatten()\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to scale target for {name}: {e}\")\n",
    "                    \n",
    "            target_cmas_tensor = torch.tensor(target_cmas_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # Scale the historical data for model input\n",
    "            try:\n",
    "                past_cmas_scaled = pd.DataFrame(index=past_cmas_unscaled.index)\n",
    "                for col in self.config['cma_names']:\n",
    "                    past_cmas_scaled[col] = self.scalers[col].transform(past_cmas_unscaled[[col]]).flatten()\n",
    "                    \n",
    "                past_cpps_scaled = pd.DataFrame(index=past_cpps_unscaled.index)\n",
    "                for col in self.config['cpp_names_and_soft_sensors']:\n",
    "                    past_cpps_scaled[col] = self.scalers[col].transform(past_cpps_unscaled[[col]]).flatten()\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to scale historical data: {e}\")\n",
    "            \n",
    "            # Convert DataFrames to numpy arrays for tensor creation\n",
    "            past_cmas_values = past_cmas_scaled.values\n",
    "            past_cpps_values = past_cpps_scaled.values\n",
    "                \n",
    "            # Convert to tensors\n",
    "            past_cmas_tensor = torch.tensor(past_cmas_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            past_cpps_tensor = torch.tensor(past_cpps_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Create scaled tensor for current CPPs (for control effort penalty)\n",
    "            current_cpps_scaled = np.zeros(len(self.config['cpp_names']))\n",
    "            for i, name in enumerate(self.config['cpp_names']):\n",
    "                current_cpp_df = pd.DataFrame(current_cpps_unscaled[i].reshape(-1, 1), columns=[name])\n",
    "                current_cpps_scaled[i] = self.scalers[name].transform(current_cpp_df).flatten()[0]\n",
    "            current_cpps_tensor = torch.tensor(current_cpps_scaled, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Pre-calculate indices for soft sensor calculations (optimization)\n",
    "            try:\n",
    "                spray_rate_idx = self.config['cpp_names'].index('spray_rate')\n",
    "                carousel_speed_idx = self.config['cpp_names'].index('carousel_speed')\n",
    "                specific_energy_idx = self.config['cpp_names_and_soft_sensors'].index('specific_energy')\n",
    "                froude_number_idx = self.config['cpp_names_and_soft_sensors'].index('froude_number_proxy')\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Required variable not found in configuration: {e}. \"\n",
    "                               f\"Expected 'spray_rate' and 'carousel_speed' in cpp_names, \"\n",
    "                               f\"'specific_energy' and 'froude_number_proxy' in cpp_names_and_soft_sensors.\")\n",
    "\n",
    "            valid_costs_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(valid_candidates_unscaled, desc=\"Evaluating MPC Candidates\", leave=False)\n",
    "                for action_seq_unscaled in pbar:\n",
    "                    try:\n",
    "                        # 1. Create the full feature unscaled action sequence\n",
    "                        action_seq_with_sensors = np.zeros((action_seq_unscaled.shape[0], len(self.config['cpp_names_and_soft_sensors'])))\n",
    "                        action_seq_with_sensors[:, :action_seq_unscaled.shape[1]] = action_seq_unscaled\n",
    "                        \n",
    "                        # Calculate soft sensors using pre-calculated indices\n",
    "                        spray_rate = action_seq_unscaled[:, spray_rate_idx]\n",
    "                        carousel_speed = action_seq_unscaled[:, carousel_speed_idx]\n",
    "                        action_seq_with_sensors[:, specific_energy_idx] = (spray_rate * carousel_speed) / 1000.0  # specific_energy\n",
    "                        action_seq_with_sensors[:, froude_number_idx] = (carousel_speed**2) / 9.81             # froude_number_proxy\n",
    "\n",
    "                        # 2. Scale the full feature sequence\n",
    "                        action_df_unscaled = pd.DataFrame(action_seq_with_sensors, columns=self.config['cpp_names_and_soft_sensors'])\n",
    "                        action_df_scaled = pd.DataFrame(index=action_df_unscaled.index)\n",
    "                        for col in self.config['cpp_names_and_soft_sensors']:\n",
    "                            action_df_scaled[col] = self.scalers[col].transform(action_df_unscaled[[col]])\n",
    "\n",
    "                        action_tensor = torch.tensor(action_df_scaled.values, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "                        # Get model prediction\n",
    "                        prediction = self.model(past_cmas_tensor, past_cpps_tensor, action_tensor)\n",
    "\n",
    "                        # Calculate cost\n",
    "                        cost = self._calculate_cost(prediction, action_tensor, target_cmas_tensor, current_cpps_tensor)\n",
    "\n",
    "                        # Only update if cost is valid (finite) and better than current best\n",
    "                        if not (np.isnan(cost) or np.isinf(cost)):\n",
    "                            valid_costs_count += 1\n",
    "                            if cost < best_cost:\n",
    "                                best_cost = cost\n",
    "                                best_action_sequence = action_seq_unscaled\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Failed to evaluate candidate: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # 4. Return the first step of the best plan found\n",
    "            if best_action_sequence is None:\n",
    "                print(f\"Warning: No valid control actions found - all candidates produced invalid costs.\")\n",
    "                print(f\"Evaluated {len(valid_candidates_unscaled)} candidates, {valid_costs_count} produced valid costs\")\n",
    "                print(f\"Returning current CPPs as fallback: {current_cpps_unscaled}\")\n",
    "                return current_cpps_unscaled\n",
    "            \n",
    "            print(f\"✅ MPC optimization successful: {valid_costs_count}/{len(valid_candidates_unscaled)} candidates evaluated, best cost: {best_cost:.6f}\")\n",
    "            return best_action_sequence[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in MPC optimization: {e}\")\n",
    "            # Fallback to current action\n",
    "            current_cpps_unscaled = past_cpps_unscaled.iloc[-1][self.config['cpp_names']].values\n",
    "            return current_cpps_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Defining Process Constraints\n",
    "\n",
    "A real industrial process has strict limits. The controller must never be allowed to suggest an action that could damage the equipment or ruin the product. We will define these limits in a clear, structured dictionary that our `MPCController` can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_CONSTRAINTS = {\n",
    "    'spray_rate': {\n",
    "        'min_val': 80.0,              # Equipment minimum\n",
    "        'max_val': 180.0,             # Equipment maximum\n",
    "        'max_change_per_step': 10.0   # Max allowed change in one go (for stability)\n",
    "    },\n",
    "    'air_flow': {\n",
    "        'min_val': 400.0,\n",
    "        'max_val': 700.0,\n",
    "        'max_change_per_step': 25.0\n",
    "    },\n",
    "    'carousel_speed': {\n",
    "        'min_val': 20.0,\n",
    "        'max_val': 40.0,\n",
    "        'max_change_per_step': 2.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Standalone Test of the MPC Controller\n",
    "\n",
    "Before we connect the controller to our simulator in the final notebook, let's perform a standalone test. We will create some dummy historical data, define a target, and ask the controller for a single best action. This allows us to debug the controller's logic in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model and configuration...\n",
      "Model architecture: 64-dim transformer\n",
      "  Encoder layers: 4\n",
      "  Decoder layers: 4\n",
      "  Attention heads: 4\n",
      "  Dropout: 0.172\n",
      "Loaded scalers for 7 variables\n",
      "✅ MPC Controller instantiated successfully with correct model configuration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from V1.src.model_architecture import GranulationPredictor\n",
    "from V1.src.mpc_controller import MPCController\n",
    "\n",
    "# --- Load all necessary components ---\n",
    "DATA_DIR = '../data'\n",
    "MODEL_SAVE_PATH = os.path.join(DATA_DIR, 'best_predictor_model.pth')\n",
    "SCALER_FILE = os.path.join(DATA_DIR, 'model_scalers.joblib')  # FIXED: Correct scaler file name\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# FIXED: Load model configuration from saved checkpoint\n",
    "print(\"Loading trained model and configuration...\")\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "\n",
    "# Extract saved configuration and hyperparameters\n",
    "saved_config = checkpoint['config']\n",
    "saved_hparams = checkpoint['hyperparameters']\n",
    "\n",
    "CMA_COLS = saved_config['CMA_COLS']\n",
    "CPP_COLS_BASE = ['spray_rate', 'air_flow', 'carousel_speed']  # Base control variables\n",
    "CPP_COLS_FULL = saved_config['CPP_COLS']  # Includes soft sensors\n",
    "\n",
    "print(f\"Model architecture: {saved_hparams['d_model']}-dim transformer\")\n",
    "print(f\"  Encoder layers: {saved_hparams['num_encoder_layers']}\")\n",
    "print(f\"  Decoder layers: {saved_hparams['num_decoder_layers']}\")\n",
    "print(f\"  Attention heads: {saved_hparams['nhead']}\")\n",
    "print(f\"  Dropout: {saved_hparams['dropout']:.3f}\")\n",
    "\n",
    "# Create model with correct hyperparameters from checkpoint\n",
    "model = GranulationPredictor(\n",
    "    cma_features=len(CMA_COLS),\n",
    "    cpp_features=len(CPP_COLS_FULL),\n",
    "    d_model=saved_hparams['d_model'],\n",
    "    nhead=saved_hparams['nhead'],\n",
    "    num_encoder_layers=saved_hparams['num_encoder_layers'],\n",
    "    num_decoder_layers=saved_hparams['num_decoder_layers'],\n",
    "    dropout=saved_hparams['dropout']\n",
    ")\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Load scalers\n",
    "scalers = joblib.load(SCALER_FILE)\n",
    "print(f\"Loaded scalers for {len(scalers)} variables\")\n",
    "\n",
    "# --- MPC Configuration ---\n",
    "MPC_CONFIG = {\n",
    "    'horizon': saved_config['HORIZON'],  # Use same horizon as training\n",
    "    'cpp_names': CPP_COLS_BASE,\n",
    "    'cma_names': CMA_COLS,\n",
    "    'cpp_names_and_soft_sensors': CPP_COLS_FULL,\n",
    "    'discretization_steps': 3, # Use 3 steps for each CPP delta: [-max, 0, +max]\n",
    "    'control_effort_lambda': 0.05 # Small penalty for control changes\n",
    "}\n",
    "\n",
    "# --- Instantiate the Controller ---\n",
    "mpc = MPCController(model, MPC_CONFIG, PROCESS_CONSTRAINTS, scalers)\n",
    "print(\"✅ MPC Controller instantiated successfully with correct model configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating realistic test data with process dynamics...\n",
      "\n",
      "📊 Process History Summary:\n",
      "  d50 range: 450.1 - 466.4 μm (trend: +11.4)\n",
      "  LOD range: 1.189 - 1.535 % (trend: +0.323)\n",
      "\n",
      "🎯 Control Challenge:\n",
      "  Current State: d50=465.3 μm, LOD=1.512 %\n",
      "  Target State:  d50=350.0 μm, LOD=1.800 %\n",
      "  Required Change: d50 +115.3 μm, LOD +0.288 %\n",
      "\n",
      "⚙️ Current Operating Point:\n",
      "  Spray Rate: 132.1 g/min\n",
      "  Air Flow: 547.7 m³/h\n",
      "  Carousel Speed: 30.7 rpm\n",
      "\n",
      "🔄 Running MPC optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a4c604373e4e1a9235737f34cfcd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating MPC Candidates:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 MPC Recommendation:\n",
      "To reach the target, the optimal immediate action is:\n",
      "  spray_rate     :  132.1 →  122.1 ( -10.0)\n",
      "  air_flow       :  547.7 →  547.7 (  +0.0)\n",
      "  carousel_speed :   30.7 →   32.7 (  +2.0)\n",
      "\n",
      "🧠 Physical Interpretation:\n",
      "  ↓ Reduce spray rate by 10.0 g/min → smaller particles\n",
      "  → Maintain air flow (minimal change: +0.0 m³/h)\n",
      "  ↑ Increase carousel speed by 2.0 rpm → less residence time, higher moisture\n",
      "\n",
      "✅ MPC controller test completed successfully with realistic process data!\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Realistic Test Data ---\n",
    "\n",
    "# Create realistic historical data (36 steps) with process dynamics\n",
    "# Simulate a process that has been slowly drifting from target\n",
    "print(\"Generating realistic test data with process dynamics...\")\n",
    "\n",
    "# Initialize with steady state, then add realistic variations\n",
    "base_d50 = 450.0\n",
    "base_lod = 1.2\n",
    "base_spray = 130.0\n",
    "base_air = 550.0\n",
    "base_carousel = 30.0\n",
    "\n",
    "# Generate realistic time series with trends and noise\n",
    "np.random.seed(42)  # For reproducible test results\n",
    "time_steps = 36\n",
    "\n",
    "# Create slow process drift (simulating equipment aging/fouling)\n",
    "drift_d50 = np.linspace(0, 15, time_steps)  # Gradual increase in particle size\n",
    "drift_lod = np.linspace(0, 0.3, time_steps)  # Gradual increase in moisture\n",
    "\n",
    "# Add realistic process noise\n",
    "noise_scale_d50 = 8.0\n",
    "noise_scale_lod = 0.08\n",
    "noise_scale_spray = 3.0\n",
    "noise_scale_air = 15.0\n",
    "noise_scale_carousel = 0.8\n",
    "\n",
    "# Generate realistic historical data with autocorrelation\n",
    "d50_history = []\n",
    "lod_history = []\n",
    "spray_history = []\n",
    "air_history = []\n",
    "carousel_history = []\n",
    "\n",
    "for i in range(time_steps):\n",
    "    # Add autocorrelated noise (process has memory)\n",
    "    if i == 0:\n",
    "        d50_noise = np.random.normal(0, noise_scale_d50)\n",
    "        lod_noise = np.random.normal(0, noise_scale_lod)\n",
    "        spray_noise = np.random.normal(0, noise_scale_spray)\n",
    "        air_noise = np.random.normal(0, noise_scale_air)\n",
    "        carousel_noise = np.random.normal(0, noise_scale_carousel)\n",
    "    else:\n",
    "        # Autocorrelated noise (0.7 correlation with previous step)\n",
    "        d50_noise = 0.7 * d50_noise + 0.3 * np.random.normal(0, noise_scale_d50)\n",
    "        lod_noise = 0.7 * lod_noise + 0.3 * np.random.normal(0, noise_scale_lod)\n",
    "        spray_noise = 0.7 * spray_noise + 0.3 * np.random.normal(0, noise_scale_spray)\n",
    "        air_noise = 0.7 * air_noise + 0.3 * np.random.normal(0, noise_scale_air)\n",
    "        carousel_noise = 0.7 * carousel_noise + 0.3 * np.random.normal(0, noise_scale_carousel)\n",
    "    \n",
    "    # Apply process constraints and realistic bounds\n",
    "    d50_val = max(300, min(600, base_d50 + drift_d50[i] + d50_noise))\n",
    "    lod_val = max(0.5, min(3.0, base_lod + drift_lod[i] + lod_noise))\n",
    "    spray_val = max(85, min(175, base_spray + spray_noise))\n",
    "    air_val = max(420, min(680, base_air + air_noise))\n",
    "    carousel_val = max(22, min(38, base_carousel + carousel_noise))\n",
    "    \n",
    "    d50_history.append(d50_val)\n",
    "    lod_history.append(lod_val)\n",
    "    spray_history.append(spray_val)\n",
    "    air_history.append(air_val)\n",
    "    carousel_history.append(carousel_val)\n",
    "\n",
    "# Convert to arrays\n",
    "dummy_past_cmas_unscaled = np.column_stack([d50_history, lod_history])\n",
    "dummy_past_cpps_unscaled = np.column_stack([spray_history, air_history, carousel_history])\n",
    "\n",
    "# Calculate soft sensors for the complete feature set\n",
    "dummy_se = (dummy_past_cpps_unscaled[:, 0] * dummy_past_cpps_unscaled[:, 2]) / 1000.0\n",
    "dummy_fr = (dummy_past_cpps_unscaled[:, 2]**2) / 9.81\n",
    "dummy_past_cpps_full_unscaled = np.hstack([\n",
    "    dummy_past_cpps_unscaled,\n",
    "    dummy_se.reshape(-1, 1),\n",
    "    dummy_fr.reshape(-1, 1)\n",
    "])\n",
    "\n",
    "# Convert to DataFrames as expected by suggest_action method\n",
    "past_cmas_df = pd.DataFrame(dummy_past_cmas_unscaled, columns=CMA_COLS)\n",
    "past_cpps_df = pd.DataFrame(dummy_past_cpps_full_unscaled, columns=CPP_COLS_FULL)\n",
    "\n",
    "# Define a challenging but achievable target - smaller, wetter granules\n",
    "target_d50 = 350.0  # Reduce particle size by ~100 μm\n",
    "target_lod = 1.8    # Increase moisture by ~0.3%\n",
    "target_cmas_unscaled = np.tile([target_d50, target_lod], (MPC_CONFIG['horizon'], 1))\n",
    "\n",
    "# Display current state and target\n",
    "current_d50 = past_cmas_df.iloc[-1]['d50']\n",
    "current_lod = past_cmas_df.iloc[-1]['lod']\n",
    "current_spray = past_cpps_df.iloc[-1]['spray_rate']\n",
    "current_air = past_cpps_df.iloc[-1]['air_flow']\n",
    "current_carousel = past_cpps_df.iloc[-1]['carousel_speed']\n",
    "\n",
    "print(f\"\\n📊 Process History Summary:\")\n",
    "print(f\"  d50 range: {past_cmas_df['d50'].min():.1f} - {past_cmas_df['d50'].max():.1f} μm (trend: +{past_cmas_df['d50'].iloc[-1] - past_cmas_df['d50'].iloc[0]:.1f})\")\n",
    "print(f\"  LOD range: {past_cmas_df['lod'].min():.3f} - {past_cmas_df['lod'].max():.3f} % (trend: +{past_cmas_df['lod'].iloc[-1] - past_cmas_df['lod'].iloc[0]:.3f})\")\n",
    "\n",
    "print(f\"\\n🎯 Control Challenge:\")\n",
    "print(f\"  Current State: d50={current_d50:.1f} μm, LOD={current_lod:.3f} %\")\n",
    "print(f\"  Target State:  d50={target_d50:.1f} μm, LOD={target_lod:.3f} %\")\n",
    "print(f\"  Required Change: d50 {current_d50-target_d50:+.1f} μm, LOD {target_lod-current_lod:+.3f} %\")\n",
    "\n",
    "print(f\"\\n⚙️ Current Operating Point:\")\n",
    "print(f\"  Spray Rate: {current_spray:.1f} g/min\")\n",
    "print(f\"  Air Flow: {current_air:.1f} m³/h\") \n",
    "print(f\"  Carousel Speed: {current_carousel:.1f} rpm\")\n",
    "\n",
    "# --- Run the MPC Decision Logic ---\n",
    "print(f\"\\n🔄 Running MPC optimization...\")\n",
    "suggested_action = mpc.suggest_action(past_cmas_df, past_cpps_df, target_cmas_unscaled)\n",
    "\n",
    "print(\"\\n🎯 MPC Recommendation:\")\n",
    "print(\"To reach the target, the optimal immediate action is:\")\n",
    "for i, name in enumerate(MPC_CONFIG['cpp_names']):\n",
    "    current_val = past_cpps_df.iloc[-1][name]\n",
    "    suggested_val = suggested_action[i]\n",
    "    change = suggested_val - current_val\n",
    "    print(f\"  {name:15}: {current_val:6.1f} → {suggested_val:6.1f} ({change:+6.1f})\")\n",
    "\n",
    "# Physical interpretation\n",
    "print(\"\\n🧠 Physical Interpretation:\")\n",
    "spray_change = suggested_action[0] - current_spray\n",
    "air_change = suggested_action[1] - current_air\n",
    "carousel_change = suggested_action[2] - current_carousel\n",
    "\n",
    "if spray_change < -1:\n",
    "    print(f\"  ↓ Reduce spray rate by {abs(spray_change):.1f} g/min → smaller particles\")\n",
    "elif spray_change > 1:\n",
    "    print(f\"  ↑ Increase spray rate by {spray_change:.1f} g/min → larger particles\")\n",
    "else:\n",
    "    print(f\"  → Maintain spray rate (minimal change: {spray_change:+.1f} g/min)\")\n",
    "\n",
    "if air_change < -5:\n",
    "    print(f\"  ↓ Reduce air flow by {abs(air_change):.1f} m³/h → less drying, higher moisture\")\n",
    "elif air_change > 5:\n",
    "    print(f\"  ↑ Increase air flow by {air_change:.1f} m³/h → more drying, lower moisture\")\n",
    "else:\n",
    "    print(f\"  → Maintain air flow (minimal change: {air_change:+.1f} m³/h)\")\n",
    "\n",
    "if carousel_change > 0.5:\n",
    "    print(f\"  ↑ Increase carousel speed by {carousel_change:.1f} rpm → less residence time, higher moisture\")\n",
    "elif carousel_change < -0.5:\n",
    "    print(f\"  ↓ Reduce carousel speed by {abs(carousel_change):.1f} rpm → more residence time, lower moisture\")\n",
    "else:\n",
    "    print(f\"  → Maintain carousel speed (minimal change: {carousel_change:+.1f} rpm)\")\n",
    "\n",
    "print(f\"\\n✅ MPC controller test completed successfully with realistic process data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Suggestion\n",
    "\n",
    "To reach a target of smaller (`d50`=350) and wetter (`LOD`=1.8) granules from a state of larger, drier ones, we expect the controller to suggest actions that:\n",
    "1.  **Reduce Granule Size:** This is primarily achieved by decreasing the `spray_rate`.\n",
    "2.  **Increase Moisture:** This can be done by decreasing the `air_flow` or increasing the `carousel_speed` (reducing drying time).\n",
    "\n",
    "Check if the controller's suggested action aligns with this physical intuition. This confirms that the model has learned meaningful relationships and the MPC logic is working correctly.\n",
    "\n",
    "We are now ready to put all the pieces together in the final notebook and run a full closed-loop simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
