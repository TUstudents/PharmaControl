{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Data Wrangling and Hybrid Preprocessing\n",
    "\n",
    "**Project:** `PharmaControl-Pro`\n",
    "**Goal:** Use our advanced simulator to generate a large, diverse dataset suitable for training a dynamic model. We will then perform the crucial preprocessing steps: chronological splitting, feature scaling, and creating a PyTorch `Dataset` to serve data in the correct sequence format.\n",
    "\n",
    "### Table of Contents\n",
    "1. [System Identification: Generating Rich Data](#1.-System-Identification:-Generating-Rich-Data)\n",
    "2. [Time-Series Splitting: Avoiding Data Leakage](#2.-Time-Series-Splitting:-Avoiding-Data-Leakage)\n",
    "3. [Hybrid Modeling: Engineering Soft Sensors](#3.-Hybrid-Modeling:-Engineering-Soft-Sensors)\n",
    "4. [Data Scaling](#4.-Data-Scaling)\n",
    "5. [Creating a PyTorch Time-Series Dataset](#5.-Creating-a-PyTorch-Time-Series-Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. System Identification: Generating Rich Data\n",
    "\n",
    "To train a model that understands process dynamics, we need to show it a wide variety of conditions. A simple step-change experiment is not enough. We need to 'excite' the system by changing the inputs (CPPs) frequently and randomly within their operating ranges.\n",
    "\n",
    "This process is called **System Identification**. We will run our simulator for a long duration, randomly adjusting the CPPs at regular intervals. This will generate a rich time-series dataset that captures how the system responds to a wide range of inputs and transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset '../data/granulation_data.csv' already exists. Skipping generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spray_rate</th>\n",
       "      <th>air_flow</th>\n",
       "      <th>carousel_speed</th>\n",
       "      <th>d50</th>\n",
       "      <th>lod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118.028124</td>\n",
       "      <td>425.265825</td>\n",
       "      <td>24.669616</td>\n",
       "      <td>400.398937</td>\n",
       "      <td>1.585391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.028124</td>\n",
       "      <td>425.265825</td>\n",
       "      <td>24.669616</td>\n",
       "      <td>419.201792</td>\n",
       "      <td>1.578726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118.028124</td>\n",
       "      <td>425.265825</td>\n",
       "      <td>24.669616</td>\n",
       "      <td>406.536867</td>\n",
       "      <td>1.611260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.028124</td>\n",
       "      <td>425.265825</td>\n",
       "      <td>24.669616</td>\n",
       "      <td>415.168112</td>\n",
       "      <td>1.589788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118.028124</td>\n",
       "      <td>425.265825</td>\n",
       "      <td>24.669616</td>\n",
       "      <td>416.203793</td>\n",
       "      <td>1.689158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spray_rate    air_flow  carousel_speed         d50       lod\n",
       "0  118.028124  425.265825       24.669616  400.398937  1.585391\n",
       "1  118.028124  425.265825       24.669616  419.201792  1.578726\n",
       "2  118.028124  425.265825       24.669616  406.536867  1.611260\n",
       "3  118.028124  425.265825       24.669616  415.168112  1.589788\n",
       "4  118.028124  425.265825       24.669616  416.203793  1.689158"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from V1.src.plant_simulator import AdvancedPlantSimulator\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = '../data'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'granulation_data.csv')\n",
    "SIMULATION_STEPS = 15000\n",
    "CPP_CHANGE_INTERVAL = 75 # Change CPPs every 75 steps\n",
    "\n",
    "# Define operating ranges for our CPPs\n",
    "CPP_RANGES = {\n",
    "    'spray_rate': (80.0, 180.0),\n",
    "    'air_flow': (400.0, 700.0),\n",
    "    'carousel_speed': (20.0, 40.0)\n",
    "}\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Data Generation ---\n",
    "if os.path.exists(DATA_FILE):\n",
    "    print(f\"Dataset '{DATA_FILE}' already exists. Skipping generation.\")\n",
    "    df_raw = pd.read_csv(DATA_FILE)\n",
    "else:\n",
    "    print(\"Generating new dataset...\")\n",
    "    plant = AdvancedPlantSimulator()\n",
    "    \n",
    "    # Initialize with a random valid state\n",
    "    current_cpps = {key: np.random.uniform(min_v, max_v) for key, (min_v, max_v) in CPP_RANGES.items()}\n",
    "\n",
    "    log = []\n",
    "    for t in tqdm(range(SIMULATION_STEPS)):\n",
    "        # Randomly change CPPs at specified intervals\n",
    "        if t % CPP_CHANGE_INTERVAL == 0:\n",
    "            current_cpps = {key: np.random.uniform(min_v, max_v) for key, (min_v, max_v) in CPP_RANGES.items()}\n",
    "        \n",
    "        state = plant.step(current_cpps)\n",
    "        record = {**current_cpps, **state}\n",
    "        log.append(record)\n",
    "\n",
    "    df_raw = pd.DataFrame(log)\n",
    "    df_raw.to_csv(DATA_FILE, index=False)\n",
    "    print(f\"Dataset with {len(df_raw)} records saved to '{DATA_FILE}'.\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Time-Series Splitting: Avoiding Data Leakage\n",
    "\n",
    "This is one of the most critical steps in any time-series modeling project. **You cannot use a random split (like `sklearn.model_selection.train_test_split`) for time-series data.**\n",
    "\n",
    "Why? A random split would shuffle the data points, meaning the model could be trained on data from time `t` and tested on data from time `t-1`. This is 'cheating' because it has seen the future. This **data leakage** leads to overly optimistic performance metrics and models that fail catastrophically in real-world deployment.\n",
    "\n",
    "The correct approach is a **chronological split**. We must train the model on the past and validate/test it on the future, mimicking how it will be used in production.\n",
    "\n",
    "We will split our data as follows:\n",
    "*   **Training Set (70%):** The earliest data, used for model training.\n",
    "*   **Validation Set (15%):** The next block of data, used for hyperparameter tuning.\n",
    "*   **Test Set (15%):** The most recent data, held out for final, unbiased performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:   (10500, 5)\n",
      "Validation set shape: (2250, 5)\n",
      "Test set shape:       (2250, 5)\n"
     ]
    }
   ],
   "source": [
    "# Calculate split indices\n",
    "n = len(df_raw)\n",
    "train_end_idx = int(n * 0.7)\n",
    "val_end_idx = int(n * 0.85)\n",
    "\n",
    "# Perform the chronological split\n",
    "df_train = df_raw.iloc[:train_end_idx].copy()\n",
    "df_val = df_raw.iloc[train_end_idx:val_end_idx].copy()\n",
    "df_test = df_raw.iloc[val_end_idx:].copy()\n",
    "\n",
    "print(f\"Training set shape:   {df_train.shape}\")\n",
    "print(f\"Validation set shape: {df_val.shape}\")\n",
    "print(f\"Test set shape:       {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Hybrid Modeling: Engineering Soft Sensors\n",
    "\n",
    "As outlined in the paper, a purely data-driven model can be improved by injecting domain knowledge. We can compute 'soft sensors'—features derived from physical principles or mechanistic models—and add them to our dataset.\n",
    "\n",
    "This helps the model by:\n",
    "*   **Providing Context:** A feature like `specific_energy` is more informative than raw `spray_rate` and `carousel_speed` values alone.\n",
    "*   **Improving Generalization:** Models trained with physics-informed features tend to perform better on unseen data.\n",
    "\n",
    "We will create two simplified soft sensors as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft sensors added. New training set columns:\n",
      "['spray_rate', 'air_flow', 'carousel_speed', 'd50', 'lod', 'specific_energy', 'froude_number_proxy']\n"
     ]
    }
   ],
   "source": [
    "def add_soft_sensors(df):\n",
    "    \"\"\"Calculates and adds soft sensor columns to the DataFrame.\"\"\"\n",
    "    # Proxy for Specific Energy (SE): relates energy input to material throughput\n",
    "    # A more complex model would use torque, but we use a proxy.\n",
    "    df['specific_energy'] = (df['spray_rate'] * df['carousel_speed']) / 1000.0\n",
    "    \n",
    "    # Proxy for Froude Number (Fr): dimensionless number characterizing mixing intensity\n",
    "    # Fr is proportional to (speed^2) / diameter. We use a simplified version.\n",
    "    df['froude_number_proxy'] = (df['carousel_speed']**2) / 9.81\n",
    "    return df\n",
    "\n",
    "df_train = add_soft_sensors(df_train)\n",
    "df_val = add_soft_sensors(df_val)\n",
    "df_test = add_soft_sensors(df_test)\n",
    "\n",
    "print(\"Soft sensors added. New training set columns:\")\n",
    "print(df_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Data Scaling\n",
    "\n",
    "Neural networks perform best when input features are on a similar scale, typically between 0 and 1 or with a mean of 0 and standard deviation of 1. We will use `MinMaxScaler` to scale our data to the `[0, 1]` range.\n",
    "\n",
    "**Crucial Rule:** The scaler must be **fitted ONLY on the training data**. We then use this fitted scaler to transform the validation and test sets. This prevents any information from the validation/test sets from 'leaking' into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled successfully. Scalers saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spray_rate</th>\n",
       "      <th>air_flow</th>\n",
       "      <th>carousel_speed</th>\n",
       "      <th>d50</th>\n",
       "      <th>lod</th>\n",
       "      <th>specific_energy</th>\n",
       "      <th>froude_number_proxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.463489</td>\n",
       "      <td>0.535058</td>\n",
       "      <td>0.503027</td>\n",
       "      <td>0.501117</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.425269</td>\n",
       "      <td>0.450857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.293077</td>\n",
       "      <td>0.286889</td>\n",
       "      <td>0.304963</td>\n",
       "      <td>0.234866</td>\n",
       "      <td>0.232319</td>\n",
       "      <td>0.241163</td>\n",
       "      <td>0.304186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.207271</td>\n",
       "      <td>0.328495</td>\n",
       "      <td>0.237472</td>\n",
       "      <td>0.312424</td>\n",
       "      <td>0.321943</td>\n",
       "      <td>0.226837</td>\n",
       "      <td>0.177303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.431652</td>\n",
       "      <td>0.520181</td>\n",
       "      <td>0.506139</td>\n",
       "      <td>0.489725</td>\n",
       "      <td>0.523881</td>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.423088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.703208</td>\n",
       "      <td>0.795711</td>\n",
       "      <td>0.759350</td>\n",
       "      <td>0.706873</td>\n",
       "      <td>0.723336</td>\n",
       "      <td>0.620675</td>\n",
       "      <td>0.698626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         spray_rate      air_flow  carousel_speed           d50           lod  \\\n",
       "count  10500.000000  10500.000000    10500.000000  10500.000000  10500.000000   \n",
       "mean       0.463489      0.535058        0.503027      0.501117      0.523148   \n",
       "std        0.293077      0.286889        0.304963      0.234866      0.232319   \n",
       "min        0.000000      0.000000        0.000000      0.000000      0.000000   \n",
       "25%        0.207271      0.328495        0.237472      0.312424      0.321943   \n",
       "50%        0.431652      0.520181        0.506139      0.489725      0.523881   \n",
       "75%        0.703208      0.795711        0.759350      0.706873      0.723336   \n",
       "max        1.000000      1.000000        1.000000      1.000000      1.000000   \n",
       "\n",
       "       specific_energy  froude_number_proxy  \n",
       "count     10500.000000         10500.000000  \n",
       "mean          0.425269             0.450857  \n",
       "std           0.241163             0.304186  \n",
       "min           0.000000             0.000000  \n",
       "25%           0.226837             0.177303  \n",
       "50%           0.370006             0.423088  \n",
       "75%           0.620675             0.698626  \n",
       "max           1.000000             1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "FEATURE_COLUMNS = df_train.columns.tolist()\n",
    "SCALER_FILE = os.path.join(DATA_DIR, 'scalers.joblib')\n",
    "\n",
    "# Initialize a dictionary to hold a scaler for each column\n",
    "scalers = {col: MinMaxScaler() for col in FEATURE_COLUMNS}\n",
    "\n",
    "# Fit the scalers ONLY on the training data\n",
    "for col, scaler in scalers.items():\n",
    "    df_train[col] = scaler.fit_transform(df_train[[col]])\n",
    "\n",
    "# Transform the validation and test data using the FITTED scalers\n",
    "for col, scaler in scalers.items():\n",
    "    df_val[col] = scaler.transform(df_val[[col]])\n",
    "    df_test[col] = scaler.transform(df_test[[col]])\n",
    "\n",
    "# Save the fitted scalers for later use (e.g., in deployment)\n",
    "joblib.dump(scalers, SCALER_FILE)\n",
    "\n",
    "print(\"Data scaled successfully. Scalers saved.\")\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to '../data/train_data.csv'.\n",
      "Validation data saved to '../data/validation_data.csv'.\n",
      "Test data saved to '../data/test_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'train_data.csv')\n",
    "df_train.to_csv(TRAIN_DATA_FILE, index=False)\n",
    "print(f\"Training data saved to '{TRAIN_DATA_FILE}'.\")\n",
    "VALIDATION_DATA_FILE = os.path.join(DATA_DIR, 'validation_data.csv')\n",
    "df_val.to_csv(VALIDATION_DATA_FILE, index=False)\n",
    "print(f\"Validation data saved to '{VALIDATION_DATA_FILE}'.\")\n",
    "TEST_DATA_FILE = os.path.join(DATA_DIR, 'test_data.csv')\n",
    "df_test.to_csv(TEST_DATA_FILE, index=False)\n",
    "print(f\"Test data saved to '{TEST_DATA_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Creating a PyTorch Time-Series Dataset\n",
    "\n",
    "Our model needs data in a specific format: sequences of past and future information. A standard PyTorch `DataLoader` expects to receive data from a `Dataset` object. We will create a custom `Dataset` class that, given an index `i`, returns a complete sample tuple:\n",
    "\n",
    "`(past_CMAs, past_CPPs, future_CPPs, future_CMAs_target)`\n",
    "\n",
    "We will define this class in `src/dataset.py` for reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class GranulationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for creating time-series sequences for the \n",
    "    granulation process predictive model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, cma_cols, cpp_cols, lookback, horizon):\n",
    "        self.df = df\n",
    "        self.cma_cols = cma_cols\n",
    "        self.cpp_cols = cpp_cols\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        # Convert to numpy for faster slicing\n",
    "        self.cma_data = df[cma_cols].to_numpy()\n",
    "        self.cpp_data = df[cpp_cols].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of possible start points for a complete sequence\n",
    "        return len(self.df) - self.lookback - self.horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Define the slice boundaries for the sample\n",
    "        past_start = idx\n",
    "        past_end = idx + self.lookback\n",
    "        future_end = past_end + self.horizon\n",
    "        \n",
    "        # --- Extract sequences ---\n",
    "        # Historical CMAs (what we observed)\n",
    "        past_cmas = self.cma_data[past_start:past_end, :]\n",
    "        \n",
    "        # Historical CPPs (what we did)\n",
    "        past_cpps = self.cpp_data[past_start:past_end, :]\n",
    "        \n",
    "        # Future CPPs (what we plan to do)\n",
    "        future_cpps = self.cpp_data[past_end:future_end, :]\n",
    "        \n",
    "        # Future CMAs (the ground truth we want to predict)\n",
    "        future_cmas_target = self.cma_data[past_end:future_end, :]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        return (\n",
    "            torch.tensor(past_cmas, dtype=torch.float32),\n",
    "            torch.tensor(past_cpps, dtype=torch.float32),\n",
    "            torch.tensor(future_cpps, dtype=torch.float32),\n",
    "            torch.tensor(future_cmas_target, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 163 training batches of size 64.\n",
      "Created 34 validation batches of size 64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying batch shapes ---\n",
      "Past CMAs shape:        torch.Size([64, 36, 2])\n",
      "Past CPPs shape:        torch.Size([64, 36, 5])\n",
      "Future CPPs shape:      torch.Size([64, 72, 5])\n",
      "Future CMAs (Target) shape: torch.Size([64, 72, 2])\n",
      "\n",
      "Expected Past CMAs shape:        (batch_size, lookback, num_cmas) -> (64, 36, 2)\n",
      "Expected Future CMAs (Target) shape: (batch_size, horizon, num_cmas) -> (64, 72, 2)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from V1.src.dataset import GranulationDataset\n",
    "\n",
    "# --- Configuration ---\n",
    "LOOKBACK = 36 # L: Use 36 past steps (e.g., 3 minutes of data)\n",
    "HORIZON = 72  # H: Predict 72 future steps (e.g., 6 minutes)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define column groups\n",
    "CMA_COLS = ['d50', 'lod']\n",
    "CPP_COLS = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = GranulationDataset(df_train, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "val_dataset = GranulationDataset(df_val, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "test_dataset = GranulationDataset(df_test, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Created {len(train_loader)} training batches of size {BATCH_SIZE}.\")\n",
    "print(f\"Created {len(val_loader)} validation batches of size {BATCH_SIZE}.\")\n",
    "\n",
    "# --- Verify a sample ---\n",
    "past_cmas, past_cpps, future_cpps, future_cmas_target = next(iter(train_loader))\n",
    "\n",
    "print(\"\\n--- Verifying batch shapes ---\")\n",
    "print(f\"Past CMAs shape:        {past_cmas.shape}\")\n",
    "print(f\"Past CPPs shape:        {past_cpps.shape}\")\n",
    "print(f\"Future CPPs shape:      {future_cpps.shape}\")\n",
    "print(f\"Future CMAs (Target) shape: {future_cmas_target.shape}\")\n",
    "\n",
    "print(\"\\nExpected Past CMAs shape:        (batch_size, lookback, num_cmas) -> (64, 36, 2)\")\n",
    "print(\"Expected Future CMAs (Target) shape: (batch_size, horizon, num_cmas) -> (64, 72, 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have successfully transformed our raw simulated data into a clean, structured, and properly formatted input for our machine learning model. We have:\n",
    "1. Generated a diverse dataset.\n",
    "2. Performed a chronologically correct train/validation/test split.\n",
    "3. Enriched the data with physics-informed soft sensors.\n",
    "4. Scaled all features appropriately.\n",
    "5. Encapsulated the complex sequencing logic into a reusable PyTorch `Dataset`.\n",
    "\n",
    "We are now ready to move on to the most exciting part: building and training the predictive model in Notebook 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
