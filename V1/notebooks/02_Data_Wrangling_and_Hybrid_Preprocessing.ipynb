{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Data Wrangling and Hybrid Preprocessing\n",
    "\n",
    "**Project:** `PharmaControl-Pro`\n",
    "**Goal:** Use our advanced simulator to generate a large, diverse dataset suitable for training a dynamic model. We will then perform the crucial preprocessing steps: chronological splitting, feature scaling, and creating a PyTorch `Dataset` to serve data in the correct sequence format.\n",
    "\n",
    "### Table of Contents\n",
    "1. [System Identification: Generating Rich Data](#1.-System-Identification:-Generating-Rich-Data)\n",
    "2. [Time-Series Splitting: Avoiding Data Leakage](#2.-Time-Series-Splitting:-Avoiding-Data-Leakage)\n",
    "3. [Hybrid Modeling: Engineering Soft Sensors](#3.-Hybrid-Modeling:-Engineering-Soft-Sensors)\n",
    "4. [Data Scaling](#4.-Data-Scaling)\n",
    "5. [Creating a PyTorch Time-Series Dataset](#5.-Creating-a-PyTorch-Time-Series-Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. System Identification: Generating Rich Data\n",
    "\n",
    "To train a model that understands process dynamics, we need to show it a wide variety of conditions. A simple step-change experiment is not enough. We need to 'excite' the system by changing the inputs (CPPs) frequently and randomly within their operating ranges.\n",
    "\n",
    "This process is called **System Identification**. We will run our simulator for a long duration, randomly adjusting the CPPs at regular intervals. This will generate a rich time-series dataset that captures how the system responds to a wide range of inputs and transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data directories created: ../data\n",
      "Dataset '../data/granulation_data_raw.csv' already exists. Loading...\n",
      "✓ Loaded existing dataset with 15000 records\n",
      "✓ Data validation passed: 15000 rows, 7 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spray_rate</th>\n",
       "      <th>air_flow</th>\n",
       "      <th>carousel_speed</th>\n",
       "      <th>d50</th>\n",
       "      <th>lod</th>\n",
       "      <th>specific_energy</th>\n",
       "      <th>froude_number_proxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139.865848</td>\n",
       "      <td>446.805592</td>\n",
       "      <td>23.11989</td>\n",
       "      <td>418.117522</td>\n",
       "      <td>1.563103</td>\n",
       "      <td>3.233683</td>\n",
       "      <td>54.488209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139.865848</td>\n",
       "      <td>446.805592</td>\n",
       "      <td>23.11989</td>\n",
       "      <td>418.095544</td>\n",
       "      <td>1.576609</td>\n",
       "      <td>3.233683</td>\n",
       "      <td>54.488209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139.865848</td>\n",
       "      <td>446.805592</td>\n",
       "      <td>23.11989</td>\n",
       "      <td>428.347286</td>\n",
       "      <td>1.551784</td>\n",
       "      <td>3.233683</td>\n",
       "      <td>54.488209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139.865848</td>\n",
       "      <td>446.805592</td>\n",
       "      <td>23.11989</td>\n",
       "      <td>442.095644</td>\n",
       "      <td>1.506116</td>\n",
       "      <td>3.233683</td>\n",
       "      <td>54.488209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139.865848</td>\n",
       "      <td>446.805592</td>\n",
       "      <td>23.11989</td>\n",
       "      <td>442.482701</td>\n",
       "      <td>1.600427</td>\n",
       "      <td>3.233683</td>\n",
       "      <td>54.488209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spray_rate    air_flow  carousel_speed         d50       lod  \\\n",
       "0  139.865848  446.805592        23.11989  418.117522  1.563103   \n",
       "1  139.865848  446.805592        23.11989  418.095544  1.576609   \n",
       "2  139.865848  446.805592        23.11989  428.347286  1.551784   \n",
       "3  139.865848  446.805592        23.11989  442.095644  1.506116   \n",
       "4  139.865848  446.805592        23.11989  442.482701  1.600427   \n",
       "\n",
       "   specific_energy  froude_number_proxy  \n",
       "0         3.233683            54.488209  \n",
       "1         3.233683            54.488209  \n",
       "2         3.233683            54.488209  \n",
       "3         3.233683            54.488209  \n",
       "4         3.233683            54.488209  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from V1.src.plant_simulator import AdvancedPlantSimulator\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "# Data generation configuration\n",
    "SIMULATION_STEPS = 15000      # Total simulation time steps\n",
    "CPP_CHANGE_INTERVAL = 75      # Steps between control parameter changes  \n",
    "RANDOM_SEED = 42              # For reproducible data generation\n",
    "\n",
    "# Time series configuration  \n",
    "LOOKBACK = 36                 # Historical context window (steps)\n",
    "HORIZON = 72                  # Prediction horizon (steps)\n",
    "BATCH_SIZE = 64               # Training batch size\n",
    "\n",
    "# Data management\n",
    "DATA_DIR = '../data'\n",
    "RAW_DATA_FILE = os.path.join(DATA_DIR, 'granulation_data_raw.csv')  # Unscaled data\n",
    "\n",
    "# Define column groups\n",
    "CMA_COLS = ['d50', 'lod']  # Critical Material Attributes\n",
    "CPP_COLS = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']  # Critical Process Parameters\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create data directories if they don't exist\n",
    "try:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    #os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    print(f\"✓ Data directories created: {DATA_DIR}\")\n",
    "except OSError as e:\n",
    "    raise RuntimeError(f\"Failed to create data directories: {e}\")\n",
    "\n",
    "# --- Data Generation ---\n",
    "if os.path.exists(RAW_DATA_FILE):\n",
    "    print(f\"Dataset '{RAW_DATA_FILE}' already exists. Loading...\")\n",
    "    try:\n",
    "        df_raw = pd.read_csv(RAW_DATA_FILE)\n",
    "        print(f\"✓ Loaded existing dataset with {len(df_raw)} records\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load existing dataset: {e}\")\n",
    "else:\n",
    "    print(\"Generating new dataset...\")\n",
    "    \n",
    "    # Initialize plant simulator with reproducible seed\n",
    "    plant = AdvancedPlantSimulator(random_seed=RANDOM_SEED)\n",
    "    \n",
    "    # Use CPP ranges from plant simulator for consistency\n",
    "    cpp_ranges = plant.CPP_RANGES\n",
    "    print(f\"✓ Using CPP ranges from plant simulator: {cpp_ranges}\")\n",
    "    \n",
    "    # Initialize with a random valid state\n",
    "    current_cpps = {key: np.random.uniform(min_v, max_v) for key, (min_v, max_v) in cpp_ranges.items()}\n",
    "\n",
    "    log = []\n",
    "    try:\n",
    "        for t in tqdm(range(SIMULATION_STEPS), desc=\"Generating data\"):\n",
    "            # Randomly change CPPs at specified intervals\n",
    "            if t % CPP_CHANGE_INTERVAL == 0:\n",
    "                current_cpps = {key: np.random.uniform(min_v, max_v) for key, (min_v, max_v) in cpp_ranges.items()}\n",
    "            \n",
    "            state = plant.step(current_cpps)\n",
    "            record = {**current_cpps, **state}\n",
    "            log.append(record)\n",
    "\n",
    "        df_raw = pd.DataFrame(log)\n",
    "        \n",
    "        # Validate generated data\n",
    "        expected_cols = set(list(cpp_ranges.keys()) + ['d50', 'lod'])\n",
    "        actual_cols = set(df_raw.columns)\n",
    "        if not expected_cols.issubset(actual_cols):\n",
    "            missing = expected_cols - actual_cols\n",
    "            raise ValueError(f\"Generated data missing expected columns: {missing}\")\n",
    "        \n",
    "        # Save raw unscaled data\n",
    "        df_raw.to_csv(RAW_DATA_FILE, index=False)\n",
    "        print(f\"✓ Dataset with {len(df_raw)} records saved to '{RAW_DATA_FILE}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Data generation failed: {e}\")\n",
    "\n",
    "# Validate data integrity\n",
    "if df_raw.empty:\n",
    "    raise ValueError(\"Generated dataset is empty\")\n",
    "if len(df_raw) < LOOKBACK + HORIZON:\n",
    "    raise ValueError(f\"Dataset too small ({len(df_raw)} rows) for lookback={LOOKBACK} + horizon={HORIZON}\")\n",
    "\n",
    "print(f\"✓ Data validation passed: {len(df_raw)} rows, {len(df_raw.columns)} columns\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Time-Series Splitting: Avoiding Data Leakage\n",
    "\n",
    "This is one of the most critical steps in any time-series modeling project. **You cannot use a random split (like `sklearn.model_selection.train_test_split`) for time-series data.**\n",
    "\n",
    "Why? A random split would shuffle the data points, meaning the model could be trained on data from time `t` and tested on data from time `t-1`. This is 'cheating' because it has seen the future. This **data leakage** leads to overly optimistic performance metrics and models that fail catastrophically in real-world deployment.\n",
    "\n",
    "The correct approach is a **chronological split**. We must train the model on the past and validate/test it on the future, mimicking how it will be used in production.\n",
    "\n",
    "We will split our data as follows:\n",
    "*   **Training Set (70%):** The earliest data, used for model training.\n",
    "*   **Validation Set (15%):** The next block of data, used for hyperparameter tuning.\n",
    "*   **Test Set (15%):** The most recent data, held out for final, unbiased performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting Configuration\n",
    "TRAIN_SPLIT = 0.7    # 70% for training\n",
    "VAL_SPLIT = 0.15     # 15% for validation  \n",
    "TEST_SPLIT = 0.15    # 15% for testing\n",
    "\n",
    "# Validate split ratios\n",
    "if abs(TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT - 1.0) > 1e-6:\n",
    "    raise ValueError(f\"Split ratios must sum to 1.0, got {TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT}\")\n",
    "\n",
    "# Calculate split indices\n",
    "n = len(df_raw)\n",
    "train_end_idx = int(n * TRAIN_SPLIT)\n",
    "val_end_idx = int(n * (TRAIN_SPLIT + VAL_SPLIT))\n",
    "\n",
    "# Validate indices create meaningful datasets\n",
    "min_dataset_size = LOOKBACK + HORIZON\n",
    "if train_end_idx < min_dataset_size:\n",
    "    raise ValueError(f\"Training set too small ({train_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "if (val_end_idx - train_end_idx) < min_dataset_size:\n",
    "    raise ValueError(f\"Validation set too small ({val_end_idx - train_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "if (n - val_end_idx) < min_dataset_size:\n",
    "    raise ValueError(f\"Test set too small ({n - val_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "\n",
    "# Actual split is perfomed in next step after adding soft sensor data\n",
    "# Perform the chronological split\n",
    "# try:\n",
    "#     df_train_raw = df_raw.iloc[:train_end_idx].copy()\n",
    "#     df_val_raw = df_raw.iloc[train_end_idx:val_end_idx].copy()\n",
    "#     df_test_raw = df_raw.iloc[val_end_idx:].copy()\n",
    "    \n",
    "#     print(f\"✓ Chronological split completed:\")\n",
    "#     print(f\"  Training set:   {df_train_raw.shape[0]:,} rows ({TRAIN_SPLIT:.1%})\")\n",
    "#     print(f\"  Validation set: {df_val_raw.shape[0]:,} rows ({VAL_SPLIT:.1%})\")\n",
    "#     print(f\"  Test set:       {df_test_raw.shape[0]:,} rows ({TEST_SPLIT:.1%})\")\n",
    "    \n",
    "#     # Validate no data overlap\n",
    "#     assert len(set(df_train_raw.index) & set(df_val_raw.index)) == 0, \"Training/validation overlap detected\"\n",
    "#     assert len(set(df_val_raw.index) & set(df_test_raw.index)) == 0, \"Validation/test overlap detected\"\n",
    "#     assert len(set(df_train_raw.index) & set(df_test_raw.index)) == 0, \"Training/test overlap detected\"\n",
    "    \n",
    "#     print(\"✓ Data integrity validation passed - no temporal leakage detected\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     raise RuntimeError(f\"Data splitting failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Hybrid Modeling: Engineering Soft Sensors\n",
    "\n",
    "A purely data-driven model can be improved by injecting domain knowledge. We can compute 'soft sensors'—features derived from physical principles or mechanistic models—and add them to our dataset.\n",
    "\n",
    "This helps the model by:\n",
    "*   **Providing Context:** A feature like `specific_energy` is more informative than raw `spray_rate` and `carousel_speed` values alone.\n",
    "*   **Improving Generalization:** Models trained with physics-informed features tend to perform better on unseen data.\n",
    "\n",
    "We will create two simplified soft sensors as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Soft sensors added successfully: specific_energy, froude_number_proxy\n",
      "✓ Soft sensors added to raw data\n",
      "Updated columns: ['spray_rate', 'air_flow', 'carousel_speed', 'd50', 'lod', 'specific_energy', 'froude_number_proxy']\n",
      "✓ Updated raw data with soft sensors saved to '../data/granulation_data_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "def add_soft_sensors(df):\n",
    "    \"\"\"Calculate and add soft sensor columns to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing CPP columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added soft sensor columns\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing\n",
    "    \"\"\"\n",
    "    # Validate required columns exist\n",
    "    required_cols = {'spray_rate', 'carousel_speed'}\n",
    "    missing_cols = required_cols - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns for soft sensors: {missing_cols}\")\n",
    "    \n",
    "    df = df.copy()  # Avoid modifying original DataFrame\n",
    "    \n",
    "    try:\n",
    "        # Proxy for Specific Energy (SE): relates energy input to material throughput\n",
    "        # A more complex model would use torque, but we use a proxy.\n",
    "        df['specific_energy'] = (df['spray_rate'] * df['carousel_speed']) / 1000.0\n",
    "        \n",
    "        # Proxy for Froude Number (Fr): dimensionless number characterizing mixing intensity\n",
    "        # Fr is proportional to (speed^2) / diameter. We use a simplified version.\n",
    "        df['froude_number_proxy'] = (df['carousel_speed']**2) / 9.81\n",
    "        \n",
    "        # Validate soft sensor calculations\n",
    "        if df['specific_energy'].isnull().any() or df['froude_number_proxy'].isnull().any():\n",
    "            raise ValueError(\"Soft sensor calculations resulted in null values\")\n",
    "            \n",
    "        print(f\"✓ Soft sensors added successfully: specific_energy, froude_number_proxy\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Soft sensor calculation failed: {e}\")\n",
    "\n",
    "# Add soft sensors to raw data BEFORE splitting\n",
    "try:\n",
    "    df_raw = add_soft_sensors(df_raw)\n",
    "    print(\"✓ Soft sensors added to raw data\")\n",
    "    print(f\"Updated columns: {df_raw.columns.tolist()}\")\n",
    "    \n",
    "    # Save the updated raw data with soft sensors\n",
    "    df_raw.to_csv(RAW_DATA_FILE, index=False)\n",
    "    print(f\"✓ Updated raw data with soft sensors saved to '{RAW_DATA_FILE}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to add soft sensors: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chronological split completed (with soft sensors):\n",
      "  Training set:   10,500 rows (70.0%)\n",
      "  Validation set: 2,250 rows (15.0%)\n",
      "  Test set:       2,250 rows (15.0%)\n",
      "✓ Data integrity validation passed - no temporal leakage detected\n",
      "✓ All splits contain required soft sensor columns\n",
      "✓ Final raw datasets with soft sensors saved:\n",
      "  Training: '../data/train_data_raw.csv' (7 columns)\n",
      "  Validation: '../data/validation_data_raw.csv' (7 columns)\n",
      "  Test: '../data/test_data_raw.csv' (7 columns)\n",
      "✓ File integrity and column verification passed\n",
      "\n",
      "============================================================\n",
      "DATA PREPROCESSING SUMMARY\n",
      "============================================================\n",
      "📊 Raw data generated: 15,000 total records\n",
      "🧪 Soft sensors added: specific_energy, froude_number_proxy\n",
      "📈 Train/Val/Test split: 70%/15%/15%\n",
      "💾 Unscaled data with soft sensors saved to: ../data\n",
      "🎯 Ready for model training with LOOKBACK=36, HORIZON=72\n",
      "📋 All columns: ['spray_rate', 'air_flow', 'carousel_speed', 'd50', 'lod', 'specific_energy', 'froude_number_proxy']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Now re-split the data WITH soft sensors included\n",
    "# Data Splitting Configuration  \n",
    "TRAIN_SPLIT = 0.7    # 70% for training\n",
    "VAL_SPLIT = 0.15     # 15% for validation  \n",
    "TEST_SPLIT = 0.15    # 15% for testing\n",
    "\n",
    "# Validate split ratios\n",
    "if abs(TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT - 1.0) > 1e-6:\n",
    "    raise ValueError(f\"Split ratios must sum to 1.0, got {TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT}\")\n",
    "\n",
    "# Calculate split indices\n",
    "n = len(df_raw)\n",
    "train_end_idx = int(n * TRAIN_SPLIT)\n",
    "val_end_idx = int(n * (TRAIN_SPLIT + VAL_SPLIT))\n",
    "\n",
    "# Validate indices create meaningful datasets\n",
    "min_dataset_size = LOOKBACK + HORIZON\n",
    "if train_end_idx < min_dataset_size:\n",
    "    raise ValueError(f\"Training set too small ({train_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "if (val_end_idx - train_end_idx) < min_dataset_size:\n",
    "    raise ValueError(f\"Validation set too small ({val_end_idx - train_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "if (n - val_end_idx) < min_dataset_size:\n",
    "    raise ValueError(f\"Test set too small ({n - val_end_idx}) for sequence requirements ({min_dataset_size})\")\n",
    "\n",
    "# Perform the chronological split WITH soft sensors\n",
    "try:\n",
    "    df_train_raw_final = df_raw.iloc[:train_end_idx].copy()\n",
    "    df_val_raw_final = df_raw.iloc[train_end_idx:val_end_idx].copy()\n",
    "    df_test_raw_final = df_raw.iloc[val_end_idx:].copy()\n",
    "    \n",
    "    print(f\"✓ Chronological split completed (with soft sensors):\")\n",
    "    print(f\"  Training set:   {df_train_raw_final.shape[0]:,} rows ({TRAIN_SPLIT:.1%})\")\n",
    "    print(f\"  Validation set: {df_val_raw_final.shape[0]:,} rows ({VAL_SPLIT:.1%})\")\n",
    "    print(f\"  Test set:       {df_test_raw_final.shape[0]:,} rows ({TEST_SPLIT:.1%})\")\n",
    "    \n",
    "    # Validate no data overlap\n",
    "    assert len(set(df_train_raw_final.index) & set(df_val_raw_final.index)) == 0, \"Training/validation overlap detected\"\n",
    "    assert len(set(df_val_raw_final.index) & set(df_test_raw_final.index)) == 0, \"Validation/test overlap detected\"\n",
    "    assert len(set(df_train_raw_final.index) & set(df_test_raw_final.index)) == 0, \"Training/test overlap detected\"\n",
    "    \n",
    "    # Validate soft sensors are present in all splits\n",
    "    required_cols = set(CMA_COLS + CPP_COLS)\n",
    "    for name, df in [(\"training\", df_train_raw_final), (\"validation\", df_val_raw_final), (\"test\", df_test_raw_final)]:\n",
    "        missing_cols = required_cols - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"{name} split missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(\"✓ Data integrity validation passed - no temporal leakage detected\")\n",
    "    print(\"✓ All splits contain required soft sensor columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Data splitting failed: {e}\")\n",
    "\n",
    "# Save the final splits with soft sensors\n",
    "RAW_TRAIN_FILE = os.path.join(DATA_DIR, 'train_data_raw.csv')\n",
    "RAW_VAL_FILE = os.path.join(DATA_DIR, 'validation_data_raw.csv') \n",
    "RAW_TEST_FILE = os.path.join(DATA_DIR, 'test_data_raw.csv')\n",
    "\n",
    "try:\n",
    "    # Save final datasets with soft sensors included\n",
    "    df_train_raw_final.to_csv(RAW_TRAIN_FILE, index=False)\n",
    "    df_val_raw_final.to_csv(RAW_VAL_FILE, index=False)\n",
    "    df_test_raw_final.to_csv(RAW_TEST_FILE, index=False)\n",
    "    \n",
    "    print(f\"✓ Final raw datasets with soft sensors saved:\")\n",
    "    print(f\"  Training: '{RAW_TRAIN_FILE}' ({len(df_train_raw_final.columns)} columns)\")\n",
    "    print(f\"  Validation: '{RAW_VAL_FILE}' ({len(df_val_raw_final.columns)} columns)\")\n",
    "    print(f\"  Test: '{RAW_TEST_FILE}' ({len(df_test_raw_final.columns)} columns)\")\n",
    "    \n",
    "    # Verify file integrity and column presence\n",
    "    for name, filepath in [(\"Training\", RAW_TRAIN_FILE), (\"Validation\", RAW_VAL_FILE), (\"Test\", RAW_TEST_FILE)]:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Failed to create {filepath}\")\n",
    "        if os.path.getsize(filepath) == 0:\n",
    "            raise ValueError(f\"Created empty file: {filepath}\")\n",
    "        \n",
    "        # Verify all required columns are present\n",
    "        test_df = pd.read_csv(filepath, nrows=1)\n",
    "        required_columns = set(CMA_COLS + CPP_COLS)\n",
    "        missing_columns = required_columns - set(test_df.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"{name} dataset missing required columns: {missing_columns}\")\n",
    "    \n",
    "    print(\"✓ File integrity and column verification passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to save final datasets: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Raw data generated: {len(df_raw):,} total records\")\n",
    "print(f\"🧪 Soft sensors added: specific_energy, froude_number_proxy\") \n",
    "print(f\"📈 Train/Val/Test split: {TRAIN_SPLIT:.0%}/{VAL_SPLIT:.0%}/{TEST_SPLIT:.0%}\")\n",
    "print(f\"💾 Unscaled data with soft sensors saved to: {DATA_DIR}\")\n",
    "print(f\"🎯 Ready for model training with LOOKBACK={LOOKBACK}, HORIZON={HORIZON}\")\n",
    "print(f\"📋 All columns: {df_raw.columns.tolist()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Creating a PyTorch Time-Series Dataset\n",
    "\n",
    "Our model needs data in a specific format: sequences of past and future information. A standard PyTorch `DataLoader` expects to receive data from a `Dataset` object. We will create a custom `Dataset` class that, given an index `i`, returns a complete sample tuple:\n",
    "\n",
    "`(past_CMAs, past_CPPs, future_CPPs, future_CMAs_target)`\n",
    "\n",
    "We will define this class in `src/dataset.py` for reusability."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%writefile ../src/dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class GranulationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for pharmaceutical granulation process time series modeling.\n",
    "    \n",
    "    This dataset class implements efficient sliding window sequence extraction from\n",
    "    continuous granulation process data for training transformer-based predictive models.\n",
    "    It creates overlapping temporal sequences suitable for supervised learning of\n",
    "    multi-step-ahead process predictions.\n",
    "    \n",
    "    The dataset supports the sequence-to-sequence learning paradigm required for\n",
    "    Model Predictive Control applications, where the model learns to predict future\n",
    "    Critical Material Attributes (CMAs) based on historical process data and\n",
    "    planned future control actions (CPPs).\n",
    "    \n",
    "    Data Structure:\n",
    "    - Historical Context: (lookback, features) window of past CMAs and CPPs\n",
    "    - Control Plan: (horizon, features) sequence of future planned CPPs  \n",
    "    - Target Predictions: (horizon, features) ground truth future CMAs to predict\n",
    "    \n",
    "    The sliding window approach maximizes data utilization from continuous process\n",
    "    datasets while maintaining temporal causality for realistic MPC training.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame containing complete process time series data.\n",
    "            Must include all columns specified in cma_cols and cpp_cols.\n",
    "            Data should be chronologically ordered and preprocessed (scaled).\n",
    "        cma_cols: List of column names for Critical Material Attributes (outputs).\n",
    "            Typically includes variables like 'd50' (particle size), 'lod' (moisture).\n",
    "        cpp_cols: List of column names for Critical Process Parameters (inputs).\n",
    "            Includes control variables and soft sensors like 'spray_rate', 'air_flow',\n",
    "            'carousel_speed', 'specific_energy', 'froude_number_proxy'.\n",
    "        lookback: Number of historical time steps to include in model input.\n",
    "            Determines the temporal context window for prediction. Typical range: 20-50.\n",
    "        horizon: Number of future time steps to predict.\n",
    "            Defines the prediction horizon for MPC applications. Typical range: 5-20.\n",
    "    \n",
    "    Attributes:\n",
    "        df: Original DataFrame reference (kept for metadata access)\n",
    "        cma_cols: CMA column names for output variable identification\n",
    "        cpp_cols: CPP column names for input variable identification\n",
    "        lookback: Historical window length parameter\n",
    "        horizon: Prediction horizon length parameter\n",
    "        cma_data: Numpy array of CMA data for efficient indexing\n",
    "        cpp_data: Numpy array of CPP data for efficient indexing\n",
    "    \n",
    "    Returns:\n",
    "        Each __getitem__ call returns a 4-tuple of torch.float32 tensors:\n",
    "        - past_cmas: (lookback, num_cmas) historical material attributes\n",
    "        - past_cpps: (lookback, num_cpps) historical process parameters\n",
    "        - future_cpps: (horizon, num_cpps) planned future control actions\n",
    "        - future_cmas_target: (horizon, num_cmas) ground truth targets for training\n",
    "    \n",
    "    Example:\n",
    "        >>> df = pd.read_csv('granulation_data.csv')  # Preprocessed time series\n",
    "        >>> dataset = GranulationDataset(\n",
    "        ...     df=df,\n",
    "        ...     cma_cols=['d50', 'lod'],\n",
    "        ...     cpp_cols=['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy'],\n",
    "        ...     lookback=36,\n",
    "        ...     horizon=10\n",
    "        ... )\n",
    "        >>> dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        >>> for batch in dataloader:\n",
    "        ...     past_cmas, past_cpps, future_cpps, targets = batch\n",
    "        ...     predictions = model(past_cmas, past_cpps, future_cpps)\n",
    "        ...     loss = criterion(predictions, targets)\n",
    "    \n",
    "    Notes:\n",
    "        - Data conversion to numpy arrays improves indexing performance\n",
    "        - Temporal sequences maintain strict chronological order\n",
    "        - Dataset length accounts for required lookback and horizon windows\n",
    "        - All tensors are returned as float32 for GPU compatibility\n",
    "    \"\"\"\n",
    "    def __init__(self, df, cma_cols, cpp_cols, lookback, horizon):\n",
    "        self.df = df\n",
    "        self.cma_cols = cma_cols\n",
    "        self.cpp_cols = cpp_cols\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "\n",
    "        # Convert to numpy for faster slicing\n",
    "        self.cma_data = df[cma_cols].to_numpy()\n",
    "        self.cpp_data = df[cpp_cols].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of valid sequences in the dataset.\n",
    "        \n",
    "        Calculates the maximum number of complete sequences that can be extracted\n",
    "        from the time series data given the lookback and horizon requirements.\n",
    "        Each sequence needs sufficient historical data (lookback) and future data\n",
    "        (horizon) to be valid for training.\n",
    "        \n",
    "        Returns:\n",
    "            Integer count of valid sequences. Formula: total_rows - lookback - horizon + 1\n",
    "            \n",
    "        Notes:\n",
    "            - Accounts for the fact that sequences near the end cannot provide full horizon\n",
    "            - Sequences near the beginning lack sufficient historical context\n",
    "            - Used by PyTorch DataLoader for batching and iteration control\n",
    "        \"\"\"\n",
    "        # The number of possible start points for a complete sequence\n",
    "        return len(self.df) - self.lookback - self.horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Extract a single training sequence from the dataset.\n",
    "        \n",
    "        Implements the core sliding window logic to create a complete training sample\n",
    "        with historical context, control plan, and prediction targets. This method\n",
    "        defines the temporal structure that the transformer model will learn to map\n",
    "        from inputs to outputs.\n",
    "        \n",
    "        Sequence Structure:\n",
    "        - Historical window: [idx : idx + lookback] for context building\n",
    "        - Future window: [idx + lookback : idx + lookback + horizon] for predictions\n",
    "        - No temporal overlap between historical context and prediction targets\n",
    "        - Maintains strict chronological ordering within each sequence\n",
    "        \n",
    "        Args:\n",
    "            idx: Integer index specifying the starting position of the sequence\n",
    "                in the overall time series. Must be in range [0, len(dataset)-1].\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of four torch.float32 tensors representing a complete training sample:\n",
    "            \n",
    "            past_cmas: Historical CMA observations, shape (lookback, num_cmas)\n",
    "                Contains the process output measurements that provide context for prediction.\n",
    "                Represents \"what we observed\" in terms of material quality attributes.\n",
    "            \n",
    "            past_cpps: Historical CPP measurements, shape (lookback, num_cpps) \n",
    "                Contains the control input and soft sensor values that influenced past CMAs.\n",
    "                Represents \"what we did\" in terms of process control actions.\n",
    "            \n",
    "            future_cpps: Planned control sequence, shape (horizon, num_cpps)\n",
    "                Contains the control actions for which predictions are desired.\n",
    "                Represents \"what we plan to do\" - the MPC control sequence to evaluate.\n",
    "            \n",
    "            future_cmas_target: Ground truth CMA values, shape (horizon, num_cmas)\n",
    "                Contains the actual measured CMA values that the model should predict.\n",
    "                Used as supervised learning targets during training.\n",
    "        \n",
    "        Notes:\n",
    "            - All array slicing uses numpy for performance (converted to tensors at end)\n",
    "            - Temporal boundaries are carefully managed to prevent data leakage\n",
    "            - Float32 tensors ensure compatibility with most PyTorch models and GPUs\n",
    "            - Sequence extraction maintains the causal structure required for MPC training\n",
    "        \"\"\"\n",
    "        # Define the slice boundaries for the sample\n",
    "        past_start = idx\n",
    "        past_end = idx + self.lookback\n",
    "        future_end = past_end + self.horizon\n",
    "\n",
    "        # --- Extract sequences ---\n",
    "        # Historical CMAs (what we observed)\n",
    "        past_cmas = self.cma_data[past_start:past_end, :]\n",
    "\n",
    "        # Historical CPPs (what we did)\n",
    "        past_cpps = self.cpp_data[past_start:past_end, :]\n",
    "\n",
    "        # Future CPPs (what we plan to do)\n",
    "        future_cpps = self.cpp_data[past_end:future_end, :]\n",
    "\n",
    "        # Future CMAs (the ground truth we want to predict)\n",
    "        future_cmas_target = self.cma_data[past_end:future_end, :]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        return (\n",
    "            torch.tensor(past_cmas, dtype=torch.float32),\n",
    "            torch.tensor(past_cpps, dtype=torch.float32),\n",
    "            torch.tensor(future_cpps, dtype=torch.float32),\n",
    "            torch.tensor(future_cmas_target, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded final datasets for PyTorch processing\n",
      "  Training dataset columns: ['spray_rate', 'air_flow', 'carousel_speed', 'd50', 'lod', 'specific_energy', 'froude_number_proxy']\n",
      "✓ Column validation passed for all datasets\n",
      "✓ All datasets contain required columns: ['d50', 'lod', 'spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
      "✓ Created PyTorch datasets and dataloaders:\n",
      "  Training: 163 batches of size 64\n",
      "  Validation: 34 batches of size 64\n",
      "  Test: 34 batches of size 64\n",
      "\n",
      "--- Batch Shape Verification ---\n",
      "Past CMAs shape:         torch.Size([64, 36, 2])\n",
      "Past CPPs shape:         torch.Size([64, 36, 5])\n",
      "Future CPPs shape:       torch.Size([64, 72, 5])\n",
      "Future CMAs target shape: torch.Size([64, 72, 2])\n",
      "✓ All tensor shapes match expected dimensions\n",
      "\n",
      "--- Data Range Verification (Unscaled) ---\n",
      "d50 range: [291.8, 623.5] μm\n",
      "lod range: [0.503, 7.613] %\n",
      "spray_rate range: [80.5, 179.7] g/min\n",
      "specific_energy range: [2.022, 6.435]\n",
      "froude_number_proxy range: [43.677, 156.598]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from V1.src.dataset import GranulationDataset\n",
    "\n",
    "# Load the final datasets with soft sensors included\n",
    "try:\n",
    "    df_train_raw = pd.read_csv(RAW_TRAIN_FILE)\n",
    "    df_val_raw = pd.read_csv(RAW_VAL_FILE) \n",
    "    df_test_raw = pd.read_csv(RAW_TEST_FILE)\n",
    "    \n",
    "    print(f\"✓ Loaded final datasets for PyTorch processing\")\n",
    "    \n",
    "    # Display column information for verification\n",
    "    print(f\"  Training dataset columns: {df_train_raw.columns.tolist()}\")\n",
    "    \n",
    "    # Validate all required columns exist\n",
    "    all_required_cols = set(CMA_COLS + CPP_COLS)\n",
    "    for name, df in [(\"training\", df_train_raw), (\"validation\", df_val_raw), (\"test\", df_test_raw)]:\n",
    "        missing_cols = all_required_cols - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"{name} dataset missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(f\"✓ Column validation passed for all datasets\")\n",
    "    print(f\"✓ All datasets contain required columns: {CMA_COLS + CPP_COLS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load datasets for PyTorch processing: {e}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "try:\n",
    "    # Note: Scaling will be handled in the dataset class or training loop\n",
    "    # This preserves the original unscaled data for analysis\n",
    "    train_dataset = GranulationDataset(df_train_raw, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "    val_dataset = GranulationDataset(df_val_raw, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "    test_dataset = GranulationDataset(df_test_raw, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    print(f\"✓ Created PyTorch datasets and dataloaders:\")\n",
    "    print(f\"  Training: {len(train_loader)} batches of size {BATCH_SIZE}\")\n",
    "    print(f\"  Validation: {len(val_loader)} batches of size {BATCH_SIZE}\")\n",
    "    print(f\"  Test: {len(test_loader)} batches of size {BATCH_SIZE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to create PyTorch datasets: {e}\")\n",
    "\n",
    "# --- Verify a sample ---\n",
    "try:\n",
    "    past_cmas, past_cpps, future_cpps, future_cmas_target = next(iter(train_loader))\n",
    "\n",
    "    print(f\"\\n--- Batch Shape Verification ---\")\n",
    "    print(f\"Past CMAs shape:         {past_cmas.shape}\")\n",
    "    print(f\"Past CPPs shape:         {past_cpps.shape}\")\n",
    "    print(f\"Future CPPs shape:       {future_cpps.shape}\")\n",
    "    print(f\"Future CMAs target shape: {future_cmas_target.shape}\")\n",
    "\n",
    "    # Validate expected shapes\n",
    "    expected_shapes = {\n",
    "        'past_cmas': (BATCH_SIZE, LOOKBACK, len(CMA_COLS)),\n",
    "        'past_cpps': (BATCH_SIZE, LOOKBACK, len(CPP_COLS)), \n",
    "        'future_cpps': (BATCH_SIZE, HORIZON, len(CPP_COLS)),\n",
    "        'future_cmas_target': (BATCH_SIZE, HORIZON, len(CMA_COLS))\n",
    "    }\n",
    "    \n",
    "    actual_shapes = {\n",
    "        'past_cmas': tuple(past_cmas.shape),\n",
    "        'past_cpps': tuple(past_cpps.shape),\n",
    "        'future_cpps': tuple(future_cpps.shape), \n",
    "        'future_cmas_target': tuple(future_cmas_target.shape)\n",
    "    }\n",
    "    \n",
    "    for tensor_name, expected_shape in expected_shapes.items():\n",
    "        actual_shape = actual_shapes[tensor_name]\n",
    "        if actual_shape != expected_shape:\n",
    "            raise ValueError(f\"{tensor_name} shape mismatch: expected {expected_shape}, got {actual_shape}\")\n",
    "    \n",
    "    print(f\"✓ All tensor shapes match expected dimensions\")\n",
    "    \n",
    "    # Check data value ranges (should be unscaled)\n",
    "    print(f\"\\n--- Data Range Verification (Unscaled) ---\")\n",
    "    print(f\"d50 range: [{past_cmas[:,:,0].min():.1f}, {past_cmas[:,:,0].max():.1f}] μm\")\n",
    "    print(f\"lod range: [{past_cmas[:,:,1].min():.3f}, {past_cmas[:,:,1].max():.3f}] %\")\n",
    "    print(f\"spray_rate range: [{past_cpps[:,:,0].min():.1f}, {past_cpps[:,:,0].max():.1f}] g/min\")\n",
    "    \n",
    "    # Check soft sensor ranges\n",
    "    specific_energy_idx = CPP_COLS.index('specific_energy')\n",
    "    froude_idx = CPP_COLS.index('froude_number_proxy')\n",
    "    print(f\"specific_energy range: [{past_cpps[:,:,specific_energy_idx].min():.3f}, {past_cpps[:,:,specific_energy_idx].max():.3f}]\")\n",
    "    print(f\"froude_number_proxy range: [{past_cpps[:,:,froude_idx].min():.3f}, {past_cpps[:,:,froude_idx].max():.3f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Batch verification failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
