{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Predictive Model Training and Validation\n",
    "\n",
    "**Project:** `PharmaControl-Pro`\n",
    "**Goal:** Build, train, and validate the predictive 'ML kernel' that will power our MPC controller. This involves defining a sophisticated Transformer-based architecture, creating a custom loss function, and using a systematic approach for hyperparameter tuning.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Model Architecture: A Transformer for Time-Series](#1.-Model-Architecture:-A-Transformer-for-Time-Series)\n",
    "2. [Implementing a Custom Loss Function](#2.-Implementing-a-Custom-Loss-Function)\n",
    "3. [Hyperparameter Tuning with Optuna](#3.-Hyperparameter-Tuning-with-Optuna)\n",
    "4. [Training the Final Model](#4.-Training-the-Final-Model)\n",
    "5. [Model Validation and Baseline Comparison](#5.-Model-Validation-and-Baseline-Comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Model Architecture: A Transformer for Time-Series\n",
    "\n",
    "The paper mentions a \"Transformer-inspired\" architecture. We will implement a robust **Encoder-Decoder** model using PyTorch's `nn.Transformer` components. This architecture is well-suited for sequence-to-sequence tasks like ours.\n",
    "\n",
    "*   **Encoder:** Its job is to read the historical data (the last `L` steps of CMAs and CPPs) and compress this information into a rich, contextualized memory. It uses self-attention to understand the relationships within the historical sequence.\n",
    "*   **Decoder:** Its job is to generate the future prediction. At each future time step `t` (from 1 to `H`), it looks at the entire encoded memory (via cross-attention) and combines that context with the *planned* control action for that future step (`future_U[t]`) to make a prediction. This structure explicitly models the relationship between future actions and future outcomes.\n",
    "\n",
    "We will define this model in `src/model_architecture.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%writefile ../src/model_architecture.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for transformer-based sequence models.\n",
    "    \n",
    "    Implements the standard sinusoidal positional encoding from \"Attention Is All You Need\"\n",
    "    to provide position-dependent information to the transformer architecture. This enables\n",
    "    the model to understand temporal relationships in process time series data without\n",
    "    relying on recurrent connections.\n",
    "    \n",
    "    The encoding uses sine and cosine functions of different frequencies to create\n",
    "    unique positional representations that allow the model to extrapolate to\n",
    "    sequence lengths not seen during training.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension (embedding size) that must match transformer architecture\n",
    "        dropout: Dropout probability applied after adding positional encodings (default: 0.1)\n",
    "        max_len: Maximum sequence length supported (default: 5000)\n",
    "    \n",
    "    Attributes:\n",
    "        dropout: Dropout layer for regularization\n",
    "        pe: Registered buffer containing precomputed positional encodings\n",
    "    \n",
    "    Notes:\n",
    "        - Uses sine for even dimensions, cosine for odd dimensions\n",
    "        - Encodings are cached as registered buffers for efficiency\n",
    "        - Compatible with variable-length sequences up to max_len\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (seq_len, batch_size, d_model) containing\n",
    "               embedded sequence data (e.g., process variables or control actions)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of same shape as input with positional information added\n",
    "            and dropout applied for regularization during training\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class GranulationPredictor(nn.Module):\n",
    "    \"\"\"Transformer-based neural network for pharmaceutical granulation process prediction.\n",
    "    \n",
    "    This encoder-decoder transformer architecture is specifically designed for predicting\n",
    "    Critical Material Attributes (CMAs) in continuous granulation processes based on\n",
    "    historical process data and planned future control actions.\n",
    "    \n",
    "    Architecture Overview:\n",
    "    - Encoder: Processes historical CMA and CPP time series to build process context\n",
    "    - Decoder: Generates future CMA predictions conditioned on planned CPP sequences\n",
    "    - Multi-head attention: Captures complex temporal dependencies and interactions\n",
    "    - Positional encoding: Maintains temporal order information\n",
    "    \n",
    "    The model implements a sequence-to-sequence prediction paradigm where:\n",
    "    - Input: Historical CMAs + CPPs (lookback window) + Future CPPs (control plan)\n",
    "    - Output: Future CMAs (prediction horizon)\n",
    "    \n",
    "    This architecture enables Model Predictive Control by providing multi-step-ahead\n",
    "    predictions necessary for optimization-based control strategies.\n",
    "    \n",
    "    Args:\n",
    "        cma_features: Number of Critical Material Attributes (output variables)\n",
    "            Typically 2 for granulation: particle size (d50) and moisture (LOD)\n",
    "        cpp_features: Number of Critical Process Parameters including soft sensors\n",
    "            Typically 5: spray_rate, air_flow, carousel_speed, specific_energy, froude_number\n",
    "        d_model: Transformer model dimension for embeddings and hidden states (default: 64)\n",
    "        nhead: Number of attention heads in multi-head attention (default: 4)\n",
    "        num_encoder_layers: Number of transformer encoder layers (default: 2)\n",
    "        num_decoder_layers: Number of transformer decoder layers (default: 2)\n",
    "        dim_feedforward: Hidden dimension in feedforward networks (default: 256)\n",
    "        dropout: Dropout probability for regularization (default: 0.1)\n",
    "    \n",
    "    Attributes:\n",
    "        cma_encoder_embedding: Linear layer projecting CMA features to d_model\n",
    "        cpp_encoder_embedding: Linear layer projecting CPP features to d_model (encoder)\n",
    "        cpp_decoder_embedding: Linear layer projecting CPP features to d_model (decoder)\n",
    "        pos_encoder: Positional encoding module for temporal information\n",
    "        transformer: Core transformer architecture with encoder-decoder structure\n",
    "        output_linear: Final projection layer mapping d_model back to CMA features\n",
    "    \n",
    "    Example:\n",
    "        >>> model = GranulationPredictor(cma_features=2, cpp_features=5)\n",
    "        >>> past_cmas = torch.randn(32, 36, 2)  # (batch, lookback, features)\n",
    "        >>> past_cpps = torch.randn(32, 36, 5)\n",
    "        >>> future_cpps = torch.randn(32, 10, 5)  # (batch, horizon, features)\n",
    "        >>> predictions = model(past_cmas, past_cpps, future_cpps)\n",
    "        >>> print(predictions.shape)  # torch.Size([32, 10, 2])\n",
    "    \"\"\"\n",
    "    def __init__(self, cma_features, cpp_features, d_model=64, nhead=4, \n",
    "                 num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Input Embeddings ---\n",
    "        self.cma_encoder_embedding = nn.Linear(cma_features, d_model)\n",
    "        self.cpp_encoder_embedding = nn.Linear(cpp_features, d_model)\n",
    "        self.cpp_decoder_embedding = nn.Linear(cpp_features, d_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # --- Transformer --- \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        # Maps the decoder output back to the desired number of CMA features\n",
    "        self.output_linear = nn.Linear(d_model, cma_features)\n",
    "\n",
    "    def forward(self, past_cmas, past_cpps, future_cpps):\n",
    "        \"\"\"Execute forward pass for granulation process prediction.\n",
    "        \n",
    "        Implements the sequence-to-sequence transformer architecture for multi-step-ahead\n",
    "        prediction of critical material attributes based on historical process data\n",
    "        and planned future control actions.\n",
    "        \n",
    "        Forward Pass Architecture:\n",
    "        1. Embed historical CMAs and CPPs separately, then combine with addition\n",
    "        2. Apply positional encoding to maintain temporal sequence information\n",
    "        3. Process through transformer encoder to build process context representation\n",
    "        4. Embed future CPPs and apply positional encoding for decoder input\n",
    "        5. Generate causal attention mask to prevent information leakage\n",
    "        6. Decode predictions using transformer decoder with attention to encoder context\n",
    "        7. Project decoder output to CMA space using final linear layer\n",
    "        \n",
    "        Args:\n",
    "            past_cmas: Historical critical material attributes tensor of shape\n",
    "                (batch_size, lookback_length, cma_features). Contains time series\n",
    "                of process outputs (e.g., particle size, moisture content)\n",
    "            past_cpps: Historical critical process parameters tensor of shape\n",
    "                (batch_size, lookback_length, cpp_features). Contains time series\n",
    "                of control inputs and soft sensors\n",
    "            future_cpps: Planned future control actions tensor of shape\n",
    "                (batch_size, prediction_horizon, cpp_features). Control sequence\n",
    "                for which predictions are desired\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, prediction_horizon, cma_features) containing\n",
    "            predicted critical material attributes over the specified horizon.\n",
    "            Values are in the same scale/units as the training data.\n",
    "        \n",
    "        Notes:\n",
    "            - Uses causal masking in decoder to prevent future information leakage\n",
    "            - Additive combination of CMA and CPP embeddings assumes feature alignment\n",
    "            - Positional encoding enables attention mechanism to utilize temporal order\n",
    "            - All tensors must be on the same device for computation\n",
    "        \"\"\"\n",
    "        # src: source sequence to the encoder (historical data)\n",
    "        # tgt: target sequence to the decoder (planned future actions)\n",
    "\n",
    "        # Embed and combine historical inputs for the encoder\n",
    "        past_cma_emb = self.cma_encoder_embedding(past_cmas)\n",
    "        past_cpp_emb = self.cpp_encoder_embedding(past_cpps)\n",
    "        src = self.pos_encoder(past_cma_emb + past_cpp_emb)\n",
    "\n",
    "        # Embed future control actions for the decoder\n",
    "        tgt = self.pos_encoder(self.cpp_decoder_embedding(future_cpps))\n",
    "\n",
    "        # The decoder needs a target mask to prevent it from seeing future positions\n",
    "        # when making a prediction at the current position.\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        # Pass through the transformer\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "        # Final linear layer to get CMA predictions\n",
    "        prediction = self.output_linear(output)\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Implementing a Custom Loss Function\n",
    "\n",
    "The paper mentions a custom loss function designed to \"prevent fitting irrelevant short-time dynamics.\" This implies that errors further out in the prediction horizon are more important than immediate, transient errors.\n",
    "\n",
    "We can implement this by creating a **weighted Mean Squared Error (MSE)** loss. We'll assign a weight to each of the `H` steps in the horizon, with weights increasing over time. This forces the model to prioritize long-term accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedHorizonMSELoss(nn.Module):\n",
    "    \"\"\"Calculates MSE with a linearly increasing weight over the horizon.\"\"\"\n",
    "    def __init__(self, horizon: int, start_weight: float = 0.5, end_weight: float = 1.5):\n",
    "        super().__init__()\n",
    "        # Create a weight tensor of shape (1, horizon, 1) for broadcasting\n",
    "        weights = torch.linspace(start_weight, end_weight, horizon).view(1, -1, 1)\n",
    "        self.register_buffer('weights', weights)\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        # prediction and target shape: (batch_size, horizon, features)\n",
    "        loss = (prediction - target) ** 2\n",
    "        weighted_loss = loss * self.weights\n",
    "        return torch.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Hyperparameter Tuning with Optuna\n",
    "\n",
    "Choosing the right hyperparameters (like learning rate, model size, etc.) is critical for performance. Manually guessing these values is inefficient. We will use **Optuna**, a powerful hyperparameter optimization framework, to systematically search for the best combination.\n",
    "\n",
    "We'll define an `objective` function that takes a trial, builds a model with the suggested hyperparameters, trains it for a few epochs, and returns the validation loss. Optuna will then intelligently choose the next set of hyperparameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Configuration loaded:\n",
      "  Device: cuda\n",
      "  Data shape: 36 → 72 steps\n",
      "  Features: 2 CMAs, 5 CPPs\n",
      "  Hyperparameter trials: 800\n",
      "  Final training epochs: 50\n",
      "\n",
      "✓ Loaded unscaled datasets:\n",
      "  Training: (10500, 7)\n",
      "  Validation: (2250, 7)\n",
      "  Test: (2250, 7)\n",
      "✓ All datasets contain required columns: 7 features\n",
      "\n",
      "📊 Applying MinMax scaling to 7 features...\n",
      "✓ Scalers saved to: ../data/model_scalers.joblib\n",
      "\n",
      "📈 Scaling Summary:\n",
      "  d50: [291.17, 645.98] → [0.000, 1.000]\n",
      "  lod: [0.50, 8.02] → [0.000, 1.000]\n",
      "  spray_rate: [80.51, 179.65] → [0.000, 1.000]\n",
      "  air_flow: [405.97, 698.82] → [0.000, 1.000]\n",
      "  carousel_speed: [20.04, 39.73] → [0.000, 1.000]\n",
      "\n",
      "✅ Data preparation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Import our models and dataset\n",
    "sys.path.append('..')  # Add parent directory to Python path\n",
    "from V1.src.model_architecture import GranulationPredictor\n",
    "from V1.src.dataset import GranulationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "# Data and model configuration\n",
    "DATA_DIR = '../data'\n",
    "LOOKBACK = 36                    # Historical context window (steps)\n",
    "HORIZON = 72                     # Prediction horizon (steps)\n",
    "CMA_COLS = ['d50', 'lod']       # Critical Material Attributes\n",
    "CPP_COLS = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RANDOM_SEED = 42                 # For reproducible results\n",
    "\n",
    "# Hyperparameter tuning configuration\n",
    "OPTUNA_CONFIG = {\n",
    "    'n_trials': 800,             # Number of hyperparameter trials\n",
    "    'tuning_epochs': 8,          # Epochs per trial (faster tuning)\n",
    "    'tuning_batch_size': 128,    # Batch size for hyperparameter search\n",
    "    'direction': 'minimize',     # Objective direction\n",
    "    'timeout': 3600,             # 1 hour timeout\n",
    "    'gc_after_trial': True       # Garbage collection # Minimize validation loss\n",
    "}\n",
    "\n",
    "# Final training configuration  \n",
    "TRAINING_CONFIG = {\n",
    "    'final_epochs': 50,          # Full training epochs\n",
    "    'final_batch_size': 64,      # Production batch size\n",
    "    'patience': 8,              # Early stopping patience\n",
    "    'gradient_clip_value': 1.0,  # Gradient clipping threshold\n",
    "    'min_delta': 1e-6           # Minimum improvement for early stopping\n",
    "}\n",
    "\n",
    "# Model architecture search space\n",
    "MODEL_SEARCH_SPACE = {\n",
    "    'd_model': [32, 64, 128],           # Transformer model dimensions\n",
    "    'nhead': [2, 4, 8],                 # Number of attention heads\n",
    "    'num_encoder_layers': (1, 4),       # Encoder layer range\n",
    "    'num_decoder_layers': (1, 4),       # Decoder layer range\n",
    "    'lr': (1e-5, 1e-2),                # Learning rate range (log scale)\n",
    "    'dropout': (0.05, 0.15),            # Dropout probability range\n",
    "    'weight_decay': (1e-6, 1e-3)       # L2 regularization range\n",
    "}\n",
    "\n",
    "# Loss function configuration\n",
    "LOSS_CONFIG = {\n",
    "    'horizon_weighting': True,          # Enable horizon-based weighting\n",
    "    'start_weight': 0.5,               # Weight for early horizon steps\n",
    "    'end_weight': 1.5,                 # Weight for late horizon steps\n",
    "    'loss_type': 'mse'                 # Loss function type\n",
    "}\n",
    "\n",
    "# File paths\n",
    "PATHS = {\n",
    "    'model_save': os.path.join(DATA_DIR, 'best_predictor_model.pth'),\n",
    "    'scalers_save': os.path.join(DATA_DIR, 'model_scalers.joblib'),\n",
    "    'optuna_study': os.path.join(DATA_DIR, 'optuna_study.pkl'),\n",
    "    'training_log': os.path.join(DATA_DIR, 'training_log.csv')\n",
    "}\n",
    "\n",
    "# Validation configuration\n",
    "VALIDATION_CONFIG = {\n",
    "    'eval_batch_size': 32,             # Batch size for evaluation\n",
    "    'metrics': ['mae', 'rmse', 'mape'], # Evaluation metrics\n",
    "    'save_predictions': True,           # Save test predictions\n",
    "    'plot_samples': 3                  # Number of prediction plots\n",
    "}\n",
    "\n",
    "print(f\"🔧 Configuration loaded:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Data shape: {LOOKBACK} → {HORIZON} steps\")\n",
    "print(f\"  Features: {len(CMA_COLS)} CMAs, {len(CPP_COLS)} CPPs\")\n",
    "print(f\"  Hyperparameter trials: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(f\"  Final training epochs: {TRAINING_CONFIG['final_epochs']}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import torch\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# --- Load Unscaled Data from Updated Pipeline ---\n",
    "try:\n",
    "    # Load raw unscaled data from notebook 02 pipeline\n",
    "    df_train_raw = pd.read_csv(os.path.join(DATA_DIR, 'train_data_raw.csv'))\n",
    "    df_val_raw = pd.read_csv(os.path.join(DATA_DIR, 'validation_data_raw.csv'))\n",
    "    df_test_raw = pd.read_csv(os.path.join(DATA_DIR, 'test_data_raw.csv'))\n",
    "    \n",
    "    print(f\"\\n✓ Loaded unscaled datasets:\")\n",
    "    print(f\"  Training: {df_train_raw.shape}\")\n",
    "    print(f\"  Validation: {df_val_raw.shape}\")\n",
    "    print(f\"  Test: {df_test_raw.shape}\")\n",
    "    \n",
    "    # Validate all required columns exist\n",
    "    all_required_cols = set(CMA_COLS + CPP_COLS)\n",
    "    for name, df in [(\"train\", df_train_raw), (\"validation\", df_val_raw), (\"test\", df_test_raw)]:\n",
    "        missing_cols = all_required_cols - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"{name} dataset missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(f\"✓ All datasets contain required columns: {len(all_required_cols)} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load datasets: {e}\")\n",
    "\n",
    "# --- Data Scaling for Model Training ---\n",
    "def create_scalers_and_scale_data(df_train, df_val, df_test, feature_cols):\n",
    "    \"\"\"Create scalers fitted on training data and apply to all datasets.\n",
    "    \n",
    "    Args:\n",
    "        df_train: Training dataframe (unscaled)\n",
    "        df_val: Validation dataframe (unscaled)\n",
    "        df_test: Test dataframe (unscaled)\n",
    "        feature_cols: List of column names to scale\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scaled_train, scaled_val, scaled_test, scalers_dict)\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    \n",
    "    # Create copies to avoid modifying original data\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_val_scaled = df_val.copy()\n",
    "    df_test_scaled = df_test.copy()\n",
    "    \n",
    "    # Fit scalers on training data only and transform all datasets\n",
    "    for col in feature_cols:\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Fit only on training data\n",
    "        df_train_scaled[col] = scaler.fit_transform(df_train[[col]])\n",
    "        \n",
    "        # Transform validation and test data using fitted scaler\n",
    "        df_val_scaled[col] = scaler.transform(df_val[[col]])\n",
    "        df_test_scaled[col] = scaler.transform(df_test[[col]])\n",
    "        \n",
    "        scalers[col] = scaler\n",
    "    \n",
    "    return df_train_scaled, df_val_scaled, df_test_scaled, scalers\n",
    "\n",
    "# Apply scaling\n",
    "print(f\"\\n📊 Applying MinMax scaling to {len(CMA_COLS + CPP_COLS)} features...\")\n",
    "all_feature_cols = CMA_COLS + CPP_COLS\n",
    "df_train, df_val, df_test, scalers = create_scalers_and_scale_data(\n",
    "    df_train_raw, df_val_raw, df_test_raw, all_feature_cols\n",
    ")\n",
    "\n",
    "# Save scalers for later use (MPC controller, evaluation)\n",
    "joblib.dump(scalers, PATHS['scalers_save'])\n",
    "print(f\"✓ Scalers saved to: {PATHS['scalers_save']}\")\n",
    "\n",
    "# Display scaling results\n",
    "print(f\"\\n📈 Scaling Summary:\")\n",
    "for col in CMA_COLS + CPP_COLS[:3]:  # Show first few columns\n",
    "    orig_min, orig_max = df_train_raw[col].min(), df_train_raw[col].max()\n",
    "    scaled_min, scaled_max = df_train[col].min(), df_train[col].max()\n",
    "    print(f\"  {col}: [{orig_min:.2f}, {orig_max:.2f}] → [{scaled_min:.3f}, {scaled_max:.3f}]\")\n",
    "\n",
    "print(f\"\\n✅ Data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def objective(trial):\n    \"\"\"Optuna objective function for hyperparameter tuning with configurable parameters.\n    \n    Note: No try/except wrapper is used around this function. Optuna is designed to handle\n    exceptions from individual trials gracefully and will report full tracebacks for \n    failed trials, which is essential for debugging hyperparameter search issues.\n    \"\"\"\n    # --- Sample Hyperparameters from Search Space ---\n    d_model = trial.suggest_categorical('d_model', MODEL_SEARCH_SPACE['d_model'])\n    nhead = trial.suggest_categorical('nhead', MODEL_SEARCH_SPACE['nhead'])\n    num_encoder_layers = trial.suggest_int('num_encoder_layers', \n                                         MODEL_SEARCH_SPACE['num_encoder_layers'][0],\n                                         MODEL_SEARCH_SPACE['num_encoder_layers'][1])\n    num_decoder_layers = trial.suggest_int('num_decoder_layers',\n                                         MODEL_SEARCH_SPACE['num_decoder_layers'][0], \n                                         MODEL_SEARCH_SPACE['num_decoder_layers'][1])\n    lr = trial.suggest_float('lr', \n                           MODEL_SEARCH_SPACE['lr'][0], \n                           MODEL_SEARCH_SPACE['lr'][1], log=True)\n    dropout = trial.suggest_float('dropout',\n                                MODEL_SEARCH_SPACE['dropout'][0],\n                                MODEL_SEARCH_SPACE['dropout'][1])\n    weight_decay = trial.suggest_float('weight_decay',\n                                     MODEL_SEARCH_SPACE['weight_decay'][0],\n                                     MODEL_SEARCH_SPACE['weight_decay'][1], log=True)\n    \n    # Validate nhead compatibility with d_model\n    if d_model % nhead != 0:\n        # Skip invalid combinations\n        raise optuna.exceptions.TrialPruned()\n    \n    # --- Model, Loss, Optimizer ---\n    model = GranulationPredictor(\n        cma_features=len(CMA_COLS),\n        cpp_features=len(CPP_COLS),\n        d_model=d_model, \n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dropout=dropout\n    ).to(DEVICE)\n    \n    criterion = WeightedHorizonMSELoss(\n        horizon=HORIZON,\n        start_weight=LOSS_CONFIG['start_weight'],\n        end_weight=LOSS_CONFIG['end_weight']\n    ).to(DEVICE)\n    \n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # --- DataLoaders with Configured Batch Size ---\n    train_dataset = GranulationDataset(df_train, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n    val_dataset = GranulationDataset(df_val, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=OPTUNA_CONFIG['tuning_batch_size'], \n        shuffle=True,\n        num_workers=2\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=OPTUNA_CONFIG['tuning_batch_size'], \n        shuffle=False,\n        num_workers=2\n    )\n\n    # --- Training Loop with Configuration ---\n    best_val_loss = float('inf')\n    \n    for epoch in range(OPTUNA_CONFIG['tuning_epochs']):\n        model.train()\n        epoch_loss = 0.0\n        \n        for batch_idx, batch in enumerate(train_loader):\n            past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n            \n            optimizer.zero_grad()\n            prediction = model(past_cmas, past_cpps, future_cpps)\n            loss = criterion(prediction, future_cmas_target)\n            loss.backward()\n            \n            # Apply gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), TRAINING_CONFIG['gradient_clip_value'])\n            \n            optimizer.step()\n            epoch_loss += loss.item()\n            \n            # Report intermediate values for pruning\n            if batch_idx % 10 == 0:\n                trial.report(loss.item(), epoch * len(train_loader) + batch_idx)\n                \n                # Prune unpromising trials\n                if trial.should_prune():\n                    raise optuna.exceptions.TrialPruned()\n        \n        # Validation evaluation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n                prediction = model(past_cmas, past_cpps, future_cpps)\n                val_loss += criterion(prediction, future_cmas_target).item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        best_val_loss = min(best_val_loss, avg_val_loss)\n        \n        # Report for pruning (offset to avoid duplicate step reporting)\n        trial.report(avg_val_loss, (epoch + 1) * len(train_loader))\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    \n    return best_val_loss\n    "
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 16:27:54,431] A new study created in memory with name: no-name-4690209d-52cc-41d7-895d-6008c87a3a16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Starting hyperparameter optimization...\n",
      "  Trials: 800\n",
      "  Epochs per trial: 8\n",
      "  Batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:28:06,851] Trial 0 finished with value: 0.002294103539658382 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.003995970573176855, 'dropout': 0.07695508786883604, 'weight_decay': 0.0005122079744190295}. Best is trial 0 with value: 0.002294103539658382.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:28:36,725] Trial 1 finished with value: 0.0029117292108233363 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'lr': 4.4001433385444076e-05, 'dropout': 0.09710785691557478, 'weight_decay': 0.000866296161649678}. Best is trial 0 with value: 0.002294103539658382.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:29:00,151] Trial 2 finished with value: 0.0011570863836609265 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 4, 'num_decoder_layers': 2, 'lr': 7.33659763909268e-05, 'dropout': 0.05653689514690714, 'weight_decay': 2.364091515022092e-06}. Best is trial 2 with value: 0.0011570863836609265.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:29:12,226] Trial 3 finished with value: 0.011905489037470782 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 1.66759918811895e-05, 'dropout': 0.12311285463819267, 'weight_decay': 3.917438843937377e-06}. Best is trial 2 with value: 0.0011570863836609265.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:29:32,284] Trial 4 finished with value: 0.0019942973569199883 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'lr': 0.00195329995487069, 'dropout': 0.13498165282718866, 'weight_decay': 1.5610664606437525e-06}. Best is trial 2 with value: 0.0011570863836609265.\n",
      "[I 2025-08-16 16:29:32,632] Trial 5 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:29:49,305] Trial 6 finished with value: 0.0009970407768645708 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'lr': 0.0002686090604585737, 'dropout': 0.09821309922305474, 'weight_decay': 8.906490680104052e-05}. Best is trial 6 with value: 0.0009970407768645708.\n",
      "[I 2025-08-16 16:29:50,300] Trial 7 pruned. \n",
      "[I 2025-08-16 16:29:50,626] Trial 8 pruned. \n",
      "[I 2025-08-16 16:29:51,127] Trial 9 pruned. \n",
      "[I 2025-08-16 16:29:51,849] Trial 10 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:29:54,539] Trial 11 pruned. \n",
      "[I 2025-08-16 16:29:54,961] Trial 12 pruned. \n",
      "[I 2025-08-16 16:29:55,397] Trial 13 pruned. \n",
      "[I 2025-08-16 16:29:55,750] Trial 14 pruned. \n",
      "[I 2025-08-16 16:29:56,134] Trial 15 pruned. \n",
      "[I 2025-08-16 16:29:56,602] Trial 16 pruned. \n",
      "[I 2025-08-16 16:29:56,922] Trial 17 pruned. \n",
      "[I 2025-08-16 16:29:57,724] Trial 18 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:30:13,575] Trial 19 finished with value: 0.0010737269117003854 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 3, 'num_decoder_layers': 1, 'lr': 0.00024228418028540583, 'dropout': 0.05078237788252506, 'weight_decay': 5.078991035165185e-05}. Best is trial 6 with value: 0.0009970407768645708.\n",
      "[I 2025-08-16 16:30:13,891] Trial 20 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:30:31,380] Trial 21 finished with value: 0.0022019913166706614 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 4, 'num_decoder_layers': 1, 'lr': 0.0006628051679102563, 'dropout': 0.052002623307939456, 'weight_decay': 0.00022520257201218347}. Best is trial 6 with value: 0.0009970407768645708.\n",
      "[I 2025-08-16 16:30:32,009] Trial 22 pruned. \n",
      "[I 2025-08-16 16:30:33,512] Trial 23 pruned. \n",
      "[I 2025-08-16 16:30:33,835] Trial 24 pruned. \n",
      "[I 2025-08-16 16:30:34,228] Trial 25 pruned. \n",
      "[I 2025-08-16 16:30:34,591] Trial 26 pruned. \n",
      "[I 2025-08-16 16:30:34,907] Trial 27 pruned. \n",
      "[I 2025-08-16 16:30:35,348] Trial 28 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:30:38,071] Trial 29 pruned. \n",
      "[I 2025-08-16 16:30:38,862] Trial 30 pruned. \n",
      "[I 2025-08-16 16:30:39,518] Trial 31 pruned. \n",
      "[I 2025-08-16 16:30:39,976] Trial 32 pruned. \n",
      "[I 2025-08-16 16:30:40,366] Trial 33 pruned. \n",
      "[I 2025-08-16 16:30:41,659] Trial 34 pruned. \n",
      "[I 2025-08-16 16:30:42,374] Trial 35 pruned. \n",
      "[I 2025-08-16 16:30:42,760] Trial 36 pruned. \n",
      "[I 2025-08-16 16:30:43,415] Trial 37 pruned. \n",
      "[I 2025-08-16 16:30:43,675] Trial 38 pruned. \n",
      "[I 2025-08-16 16:30:44,204] Trial 39 pruned. \n",
      "[I 2025-08-16 16:30:44,551] Trial 40 pruned. \n",
      "[I 2025-08-16 16:30:44,909] Trial 41 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:31:02,461] Trial 42 finished with value: 0.001401158799554276 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 4, 'num_decoder_layers': 1, 'lr': 0.0006979738711736775, 'dropout': 0.05400977616454715, 'weight_decay': 0.00030122889255491215}. Best is trial 6 with value: 0.0009970407768645708.\n",
      "[I 2025-08-16 16:31:02,826] Trial 43 pruned. \n",
      "[I 2025-08-16 16:31:03,190] Trial 44 pruned. \n",
      "[I 2025-08-16 16:31:03,602] Trial 45 pruned. \n",
      "[I 2025-08-16 16:31:04,032] Trial 46 pruned. \n",
      "[I 2025-08-16 16:31:04,371] Trial 47 pruned. \n",
      "[I 2025-08-16 16:31:04,835] Trial 48 pruned. \n",
      "[I 2025-08-16 16:31:05,235] Trial 49 pruned. \n",
      "[I 2025-08-16 16:31:05,607] Trial 50 pruned. \n",
      "[I 2025-08-16 16:31:06,861] Trial 51 pruned. \n",
      "[I 2025-08-16 16:31:07,230] Trial 52 pruned. \n",
      "[I 2025-08-16 16:31:08,161] Trial 53 pruned. \n",
      "[I 2025-08-16 16:31:08,672] Trial 54 pruned. \n",
      "[I 2025-08-16 16:31:09,074] Trial 55 pruned. \n",
      "[I 2025-08-16 16:31:09,457] Trial 56 pruned. \n",
      "[I 2025-08-16 16:31:11,160] Trial 57 pruned. \n",
      "[I 2025-08-16 16:31:11,948] Trial 58 pruned. \n",
      "[I 2025-08-16 16:31:12,281] Trial 59 pruned. \n",
      "[I 2025-08-16 16:31:12,638] Trial 60 pruned. \n",
      "[I 2025-08-16 16:31:13,054] Trial 61 pruned. \n",
      "[I 2025-08-16 16:31:13,329] Trial 62 pruned. \n",
      "[I 2025-08-16 16:31:13,882] Trial 63 pruned. \n",
      "[I 2025-08-16 16:31:14,148] Trial 64 pruned. \n",
      "[I 2025-08-16 16:31:15,293] Trial 65 pruned. \n",
      "[I 2025-08-16 16:31:16,119] Trial 66 pruned. \n",
      "[I 2025-08-16 16:31:16,834] Trial 67 pruned. \n",
      "[I 2025-08-16 16:31:17,099] Trial 68 pruned. \n",
      "[I 2025-08-16 16:31:17,561] Trial 69 pruned. \n",
      "[I 2025-08-16 16:31:19,233] Trial 70 pruned. \n",
      "[I 2025-08-16 16:31:19,807] Trial 71 pruned. \n",
      "[I 2025-08-16 16:31:20,373] Trial 72 pruned. \n",
      "[I 2025-08-16 16:31:20,939] Trial 73 pruned. \n",
      "[I 2025-08-16 16:31:21,475] Trial 74 pruned. \n",
      "[I 2025-08-16 16:31:21,837] Trial 75 pruned. \n",
      "[I 2025-08-16 16:31:22,130] Trial 76 pruned. \n",
      "[I 2025-08-16 16:31:22,554] Trial 77 pruned. \n",
      "[I 2025-08-16 16:31:23,035] Trial 78 pruned. \n",
      "[I 2025-08-16 16:31:23,471] Trial 79 pruned. \n",
      "[I 2025-08-16 16:31:24,002] Trial 80 pruned. \n",
      "[I 2025-08-16 16:31:24,265] Trial 81 pruned. \n",
      "[I 2025-08-16 16:31:24,524] Trial 82 pruned. \n",
      "[I 2025-08-16 16:31:24,787] Trial 83 pruned. \n",
      "[I 2025-08-16 16:31:25,047] Trial 84 pruned. \n",
      "[I 2025-08-16 16:31:25,651] Trial 85 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:31:45,341] Trial 86 finished with value: 0.000553528470096781 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'lr': 0.0005229537921419063, 'dropout': 0.05274518738050758, 'weight_decay': 1.1222662238974691e-06}. Best is trial 86 with value: 0.000553528470096781.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:32:04,795] Trial 87 finished with value: 0.0007819640398819876 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'lr': 0.0005296491323957605, 'dropout': 0.05258393416215182, 'weight_decay': 2.7158294533128535e-05}. Best is trial 86 with value: 0.000553528470096781.\n",
      "[I 2025-08-16 16:32:05,186] Trial 88 pruned. \n",
      "[I 2025-08-16 16:32:05,552] Trial 89 pruned. \n",
      "[I 2025-08-16 16:32:06,132] Trial 90 pruned. \n",
      "[I 2025-08-16 16:32:06,526] Trial 91 pruned. \n",
      "[I 2025-08-16 16:32:06,925] Trial 92 pruned. \n",
      "[I 2025-08-16 16:32:07,575] Trial 93 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:32:10,683] Trial 94 pruned. \n",
      "[I 2025-08-16 16:32:11,111] Trial 95 pruned. \n",
      "[I 2025-08-16 16:32:11,569] Trial 96 pruned. \n",
      "[I 2025-08-16 16:32:11,945] Trial 97 pruned. \n",
      "[I 2025-08-16 16:32:12,333] Trial 98 pruned. \n",
      "[I 2025-08-16 16:32:13,264] Trial 99 pruned. \n",
      "[I 2025-08-16 16:32:13,629] Trial 100 pruned. \n",
      "[I 2025-08-16 16:32:13,899] Trial 101 pruned. \n",
      "[I 2025-08-16 16:32:14,181] Trial 102 pruned. \n",
      "[I 2025-08-16 16:32:14,454] Trial 103 pruned. \n",
      "[I 2025-08-16 16:32:14,823] Trial 104 pruned. \n",
      "[I 2025-08-16 16:32:15,462] Trial 105 pruned. \n",
      "[I 2025-08-16 16:32:15,887] Trial 106 pruned. \n",
      "[I 2025-08-16 16:32:16,164] Trial 107 pruned. \n",
      "[I 2025-08-16 16:32:16,555] Trial 108 pruned. \n",
      "[I 2025-08-16 16:32:18,037] Trial 109 pruned. \n",
      "[I 2025-08-16 16:32:18,929] Trial 110 pruned. \n",
      "[I 2025-08-16 16:32:19,207] Trial 111 pruned. \n",
      "[I 2025-08-16 16:32:19,482] Trial 112 pruned. \n",
      "[I 2025-08-16 16:32:19,913] Trial 113 pruned. \n",
      "[I 2025-08-16 16:32:20,188] Trial 114 pruned. \n",
      "[I 2025-08-16 16:32:20,549] Trial 115 pruned. \n",
      "[I 2025-08-16 16:32:21,028] Trial 116 pruned. \n",
      "[I 2025-08-16 16:32:22,186] Trial 117 pruned. \n",
      "[I 2025-08-16 16:32:22,530] Trial 118 pruned. \n",
      "[I 2025-08-16 16:32:23,093] Trial 119 pruned. \n",
      "[I 2025-08-16 16:32:23,477] Trial 120 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:32:26,622] Trial 121 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:32:29,487] Trial 122 pruned. \n",
      "[I 2025-08-16 16:32:29,880] Trial 123 pruned. \n",
      "[I 2025-08-16 16:32:30,275] Trial 124 pruned. \n",
      "[I 2025-08-16 16:32:30,666] Trial 125 pruned. \n",
      "[I 2025-08-16 16:32:31,017] Trial 126 pruned. \n",
      "[I 2025-08-16 16:32:31,982] Trial 127 pruned. \n",
      "[I 2025-08-16 16:32:32,247] Trial 128 pruned. \n",
      "[I 2025-08-16 16:32:32,795] Trial 129 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:32:52,509] Trial 130 finished with value: 0.0009284551180077388 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'lr': 0.0005501898764973646, 'dropout': 0.11866491191235307, 'weight_decay': 3.533982286828651e-05}. Best is trial 86 with value: 0.000553528470096781.\n",
      "[I 2025-08-16 16:32:52,909] Trial 131 pruned. \n",
      "[I 2025-08-16 16:32:53,303] Trial 132 pruned. \n",
      "[I 2025-08-16 16:32:53,711] Trial 133 pruned. \n",
      "[I 2025-08-16 16:32:54,110] Trial 134 pruned. \n",
      "[I 2025-08-16 16:32:54,499] Trial 135 pruned. \n",
      "[I 2025-08-16 16:32:54,864] Trial 136 pruned. \n",
      "[I 2025-08-16 16:32:55,128] Trial 137 pruned. \n",
      "[I 2025-08-16 16:32:55,482] Trial 138 pruned. \n",
      "[I 2025-08-16 16:32:56,548] Trial 139 pruned. \n",
      "[I 2025-08-16 16:32:57,239] Trial 140 pruned. \n",
      "[I 2025-08-16 16:32:57,637] Trial 141 pruned. \n",
      "[I 2025-08-16 16:32:58,032] Trial 142 pruned. \n",
      "[I 2025-08-16 16:32:59,981] Trial 143 pruned. \n",
      "[I 2025-08-16 16:33:00,380] Trial 144 pruned. \n",
      "[I 2025-08-16 16:33:00,780] Trial 145 pruned. \n",
      "[I 2025-08-16 16:33:02,558] Trial 146 pruned. \n",
      "[I 2025-08-16 16:33:02,886] Trial 147 pruned. \n",
      "[I 2025-08-16 16:33:03,250] Trial 148 pruned. \n",
      "[I 2025-08-16 16:33:03,650] Trial 149 pruned. \n",
      "[I 2025-08-16 16:33:03,986] Trial 150 pruned. \n",
      "[I 2025-08-16 16:33:04,378] Trial 151 pruned. \n",
      "[I 2025-08-16 16:33:06,051] Trial 152 pruned. \n",
      "[I 2025-08-16 16:33:06,442] Trial 153 pruned. \n",
      "[I 2025-08-16 16:33:06,834] Trial 154 pruned. \n",
      "[I 2025-08-16 16:33:07,189] Trial 155 pruned. \n",
      "[I 2025-08-16 16:33:07,504] Trial 156 pruned. \n",
      "[I 2025-08-16 16:33:07,814] Trial 157 pruned. \n",
      "[I 2025-08-16 16:33:08,834] Trial 158 pruned. \n",
      "[I 2025-08-16 16:33:09,386] Trial 159 pruned. \n",
      "[I 2025-08-16 16:33:09,781] Trial 160 pruned. \n",
      "[I 2025-08-16 16:33:10,176] Trial 161 pruned. \n",
      "[I 2025-08-16 16:33:10,563] Trial 162 pruned. \n",
      "[I 2025-08-16 16:33:11,528] Trial 163 pruned. \n",
      "[I 2025-08-16 16:33:11,922] Trial 164 pruned. \n",
      "[I 2025-08-16 16:33:13,635] Trial 165 pruned. \n",
      "[I 2025-08-16 16:33:13,941] Trial 166 pruned. \n",
      "[I 2025-08-16 16:33:14,263] Trial 167 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:16,979] Trial 168 pruned. \n",
      "[I 2025-08-16 16:33:18,973] Trial 169 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:21,732] Trial 170 pruned. \n",
      "[I 2025-08-16 16:33:22,562] Trial 171 pruned. \n",
      "[I 2025-08-16 16:33:24,286] Trial 172 pruned. \n",
      "[I 2025-08-16 16:33:24,658] Trial 173 pruned. \n",
      "[I 2025-08-16 16:33:25,023] Trial 174 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:28,016] Trial 175 pruned. \n",
      "[I 2025-08-16 16:33:28,848] Trial 176 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:32,428] Trial 177 pruned. \n",
      "[I 2025-08-16 16:33:33,440] Trial 178 pruned. \n",
      "[I 2025-08-16 16:33:33,879] Trial 179 pruned. \n",
      "[I 2025-08-16 16:33:34,319] Trial 180 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:37,774] Trial 181 pruned. \n",
      "[I 2025-08-16 16:33:38,180] Trial 182 pruned. \n",
      "[I 2025-08-16 16:33:39,898] Trial 183 pruned. \n",
      "[I 2025-08-16 16:33:40,924] Trial 184 pruned. \n",
      "[I 2025-08-16 16:33:41,366] Trial 185 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:44,633] Trial 186 pruned. \n",
      "[I 2025-08-16 16:33:46,192] Trial 187 pruned. \n",
      "[I 2025-08-16 16:33:48,111] Trial 188 pruned. \n",
      "[I 2025-08-16 16:33:48,550] Trial 189 pruned. \n",
      "[I 2025-08-16 16:33:49,060] Trial 190 pruned. \n",
      "[I 2025-08-16 16:33:49,448] Trial 191 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:33:52,052] Trial 192 pruned. \n",
      "[I 2025-08-16 16:33:53,753] Trial 193 pruned. \n",
      "[I 2025-08-16 16:33:54,574] Trial 194 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:34:12,194] Trial 195 finished with value: 0.001485480457845637 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'num_decoder_layers': 1, 'lr': 0.0008481250593740041, 'dropout': 0.055245690422608594, 'weight_decay': 8.434428787687775e-05}. Best is trial 86 with value: 0.000553528470096781.\n",
      "[I 2025-08-16 16:34:12,560] Trial 196 pruned. \n",
      "[I 2025-08-16 16:34:14,035] Trial 197 pruned. \n",
      "[I 2025-08-16 16:34:15,189] Trial 198 pruned. \n",
      "[I 2025-08-16 16:34:15,574] Trial 199 pruned. \n",
      "[I 2025-08-16 16:34:16,158] Trial 200 pruned. \n",
      "[I 2025-08-16 16:34:16,527] Trial 201 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:34:19,926] Trial 202 pruned. \n",
      "[I 2025-08-16 16:34:20,292] Trial 203 pruned. \n",
      "[I 2025-08-16 16:34:22,255] Trial 204 pruned. \n",
      "[I 2025-08-16 16:34:23,378] Trial 205 pruned. \n",
      "[I 2025-08-16 16:34:23,753] Trial 206 pruned. \n",
      "[I 2025-08-16 16:34:24,115] Trial 207 pruned. \n",
      "[I 2025-08-16 16:34:24,910] Trial 208 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:34:37,707] Trial 209 finished with value: 0.0004921747553025317 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0009048239991361718, 'dropout': 0.05236957215248933, 'weight_decay': 1.6849095117585615e-06}. Best is trial 209 with value: 0.0004921747553025317.\n",
      "[I 2025-08-16 16:34:37,983] Trial 210 pruned. \n",
      "[I 2025-08-16 16:34:38,266] Trial 211 pruned. \n",
      "[I 2025-08-16 16:34:38,544] Trial 212 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:34:51,722] Trial 213 finished with value: 0.0004531779747648055 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0009059439748286063, 'dropout': 0.050044940567043526, 'weight_decay': 1.6434038497210028e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:34:52,012] Trial 214 pruned. \n",
      "[I 2025-08-16 16:34:52,296] Trial 215 pruned. \n",
      "[I 2025-08-16 16:34:52,726] Trial 216 pruned. \n",
      "[I 2025-08-16 16:34:52,994] Trial 217 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:35:05,357] Trial 218 finished with value: 0.0007705374497591573 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0006704543042516208, 'dropout': 0.05374874619402885, 'weight_decay': 3.2449003291371476e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:35:06,617] Trial 219 pruned. \n",
      "[I 2025-08-16 16:35:06,891] Trial 220 pruned. \n",
      "[I 2025-08-16 16:35:07,152] Trial 221 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:35:19,455] Trial 222 finished with value: 0.0007668487087357789 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0009726168400205374, 'dropout': 0.055504022331321935, 'weight_decay': 3.298414316967828e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:35:31,646] Trial 223 finished with value: 0.0008519426491522394 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0012004278491785484, 'dropout': 0.055988234713114184, 'weight_decay': 4.642667745047222e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:35:31,931] Trial 224 pruned. \n",
      "[I 2025-08-16 16:35:32,202] Trial 225 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:35:44,447] Trial 226 finished with value: 0.001252155319568427 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0009552594685339809, 'dropout': 0.06116030726226511, 'weight_decay': 4.1459038208967575e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:35:46,735] Trial 227 pruned. \n",
      "[I 2025-08-16 16:35:47,296] Trial 228 pruned. \n",
      "[I 2025-08-16 16:35:48,571] Trial 229 pruned. \n",
      "[I 2025-08-16 16:35:48,862] Trial 230 pruned. \n",
      "[I 2025-08-16 16:35:49,282] Trial 231 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:01,641] Trial 232 finished with value: 0.001096107909688726 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0008639621679978399, 'dropout': 0.05348230381524627, 'weight_decay': 3.983547585793949e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:36:02,699] Trial 233 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:05,067] Trial 234 pruned. \n",
      "[I 2025-08-16 16:36:05,343] Trial 235 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:17,573] Trial 236 finished with value: 0.0007282342960345833 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.001040856312889875, 'dropout': 0.06807547450187681, 'weight_decay': 3.9698995877747905e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:29,657] Trial 237 finished with value: 0.0007484826235347154 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0014641273019684968, 'dropout': 0.06654254405457262, 'weight_decay': 5.228090788452883e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:36:29,927] Trial 238 pruned. \n",
      "[I 2025-08-16 16:36:30,198] Trial 239 pruned. \n",
      "[I 2025-08-16 16:36:30,622] Trial 240 pruned. \n",
      "[I 2025-08-16 16:36:30,893] Trial 241 pruned. \n",
      "[I 2025-08-16 16:36:31,159] Trial 242 pruned. \n",
      "[I 2025-08-16 16:36:31,990] Trial 243 pruned. \n",
      "[I 2025-08-16 16:36:32,404] Trial 244 pruned. \n",
      "[I 2025-08-16 16:36:32,674] Trial 245 pruned. \n",
      "[I 2025-08-16 16:36:33,288] Trial 246 pruned. \n",
      "[I 2025-08-16 16:36:33,562] Trial 247 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:45,647] Trial 248 finished with value: 0.0012935992528903572 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0013194839535900858, 'dropout': 0.06062028549167006, 'weight_decay': 4.791139236420797e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:36:45,916] Trial 249 pruned. \n",
      "[I 2025-08-16 16:36:46,179] Trial 250 pruned. \n",
      "[I 2025-08-16 16:36:46,616] Trial 251 pruned. \n",
      "[I 2025-08-16 16:36:46,896] Trial 252 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:36:59,295] Trial 253 finished with value: 0.0008446740753510419 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0014760665285054341, 'dropout': 0.06539686117776711, 'weight_decay': 3.788165982746286e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:37:00,042] Trial 254 pruned. \n",
      "[I 2025-08-16 16:37:00,478] Trial 255 pruned. \n",
      "[I 2025-08-16 16:37:00,796] Trial 256 pruned. \n",
      "[I 2025-08-16 16:37:01,084] Trial 257 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:37:13,381] Trial 258 finished with value: 0.0010831544127808336 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.003031831474201717, 'dropout': 0.05682531057340446, 'weight_decay': 3.848039928428127e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:37:13,990] Trial 259 pruned. \n",
      "[I 2025-08-16 16:37:14,258] Trial 260 pruned. \n",
      "[I 2025-08-16 16:37:14,525] Trial 261 pruned. \n",
      "[I 2025-08-16 16:37:14,796] Trial 262 pruned. \n",
      "[I 2025-08-16 16:37:15,064] Trial 263 pruned. \n",
      "[I 2025-08-16 16:37:15,474] Trial 264 pruned. \n",
      "[I 2025-08-16 16:37:15,745] Trial 265 pruned. \n",
      "[I 2025-08-16 16:37:16,156] Trial 266 pruned. \n",
      "[I 2025-08-16 16:37:16,424] Trial 267 pruned. \n",
      "[I 2025-08-16 16:37:16,695] Trial 268 pruned. \n",
      "[I 2025-08-16 16:37:16,962] Trial 269 pruned. \n",
      "[I 2025-08-16 16:37:17,233] Trial 270 pruned. \n",
      "[I 2025-08-16 16:37:17,947] Trial 271 pruned. \n",
      "[I 2025-08-16 16:37:18,373] Trial 272 pruned. \n",
      "[I 2025-08-16 16:37:18,793] Trial 273 pruned. \n",
      "[I 2025-08-16 16:37:19,060] Trial 274 pruned. \n",
      "[I 2025-08-16 16:37:19,347] Trial 275 pruned. \n",
      "[I 2025-08-16 16:37:19,737] Trial 276 pruned. \n",
      "[I 2025-08-16 16:37:20,014] Trial 277 pruned. \n",
      "[I 2025-08-16 16:37:20,286] Trial 278 pruned. \n",
      "[I 2025-08-16 16:37:20,555] Trial 279 pruned. \n",
      "[I 2025-08-16 16:37:20,901] Trial 280 pruned. \n",
      "[I 2025-08-16 16:37:21,184] Trial 281 pruned. \n",
      "[I 2025-08-16 16:37:21,508] Trial 282 pruned. \n",
      "[I 2025-08-16 16:37:21,781] Trial 283 pruned. \n",
      "[I 2025-08-16 16:37:22,065] Trial 284 pruned. \n",
      "[I 2025-08-16 16:37:22,395] Trial 285 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:37:35,365] Trial 286 finished with value: 0.0006095539925702135 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0010526094348346632, 'dropout': 0.056387273801870155, 'weight_decay': 1.657140750027313e-05}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:37:35,645] Trial 287 pruned. \n",
      "[I 2025-08-16 16:37:35,924] Trial 288 pruned. \n",
      "[I 2025-08-16 16:37:36,269] Trial 289 pruned. \n",
      "[I 2025-08-16 16:37:36,733] Trial 290 pruned. \n",
      "[I 2025-08-16 16:37:37,054] Trial 291 pruned. \n",
      "[I 2025-08-16 16:37:37,382] Trial 292 pruned. \n",
      "[I 2025-08-16 16:37:37,870] Trial 293 pruned. \n",
      "[I 2025-08-16 16:37:38,165] Trial 294 pruned. \n",
      "[I 2025-08-16 16:37:38,438] Trial 295 pruned. \n",
      "[I 2025-08-16 16:37:38,749] Trial 296 pruned. \n",
      "[I 2025-08-16 16:37:39,119] Trial 297 pruned. \n",
      "[I 2025-08-16 16:37:39,396] Trial 298 pruned. \n",
      "[I 2025-08-16 16:37:39,672] Trial 299 pruned. \n",
      "[I 2025-08-16 16:37:40,244] Trial 300 pruned. \n",
      "[I 2025-08-16 16:37:40,525] Trial 301 pruned. \n",
      "[I 2025-08-16 16:37:41,355] Trial 302 pruned. \n",
      "[I 2025-08-16 16:37:41,649] Trial 303 pruned. \n",
      "[I 2025-08-16 16:37:41,949] Trial 304 pruned. \n",
      "[I 2025-08-16 16:37:42,234] Trial 305 pruned. \n",
      "[I 2025-08-16 16:37:42,555] Trial 306 pruned. \n",
      "[I 2025-08-16 16:37:42,838] Trial 307 pruned. \n",
      "[I 2025-08-16 16:37:43,330] Trial 308 pruned. \n",
      "[I 2025-08-16 16:37:43,873] Trial 309 pruned. \n",
      "[I 2025-08-16 16:37:44,293] Trial 310 pruned. \n",
      "[I 2025-08-16 16:37:45,398] Trial 311 pruned. \n",
      "[I 2025-08-16 16:37:45,669] Trial 312 pruned. \n",
      "[I 2025-08-16 16:37:45,948] Trial 313 pruned. \n",
      "[I 2025-08-16 16:37:46,246] Trial 314 pruned. \n",
      "[I 2025-08-16 16:37:46,638] Trial 315 pruned. \n",
      "[I 2025-08-16 16:37:46,916] Trial 316 pruned. \n",
      "[I 2025-08-16 16:37:47,277] Trial 317 pruned. \n",
      "[I 2025-08-16 16:37:47,551] Trial 318 pruned. \n",
      "[I 2025-08-16 16:37:48,927] Trial 319 pruned. \n",
      "[I 2025-08-16 16:37:49,462] Trial 320 pruned. \n",
      "[I 2025-08-16 16:37:49,785] Trial 321 pruned. \n",
      "[I 2025-08-16 16:37:50,077] Trial 322 pruned. \n",
      "[I 2025-08-16 16:37:50,374] Trial 323 pruned. \n",
      "[I 2025-08-16 16:37:50,721] Trial 324 pruned. \n",
      "[I 2025-08-16 16:37:51,277] Trial 325 pruned. \n",
      "[I 2025-08-16 16:37:51,723] Trial 326 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:38:07,732] Trial 327 finished with value: 0.000787514030782725 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'lr': 0.0012102285460172317, 'dropout': 0.053931169975908466, 'weight_decay': 3.093267473967706e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:38:08,070] Trial 328 pruned. \n",
      "[I 2025-08-16 16:38:08,399] Trial 329 pruned. \n",
      "[I 2025-08-16 16:38:08,740] Trial 330 pruned. \n",
      "[I 2025-08-16 16:38:09,277] Trial 331 pruned. \n",
      "[I 2025-08-16 16:38:10,409] Trial 332 pruned. \n",
      "[I 2025-08-16 16:38:10,746] Trial 333 pruned. \n",
      "[I 2025-08-16 16:38:11,027] Trial 334 pruned. \n",
      "[I 2025-08-16 16:38:11,298] Trial 335 pruned. \n",
      "[I 2025-08-16 16:38:11,575] Trial 336 pruned. \n",
      "[I 2025-08-16 16:38:11,912] Trial 337 pruned. \n",
      "[I 2025-08-16 16:38:12,343] Trial 338 pruned. \n",
      "[I 2025-08-16 16:38:12,617] Trial 339 pruned. \n",
      "[I 2025-08-16 16:38:12,892] Trial 340 pruned. \n",
      "[I 2025-08-16 16:38:13,233] Trial 341 pruned. \n",
      "[I 2025-08-16 16:38:13,505] Trial 342 pruned. \n",
      "[I 2025-08-16 16:38:13,784] Trial 343 pruned. \n",
      "[I 2025-08-16 16:38:14,069] Trial 344 pruned. \n",
      "[I 2025-08-16 16:38:14,998] Trial 345 pruned. \n",
      "[I 2025-08-16 16:38:15,288] Trial 346 pruned. \n",
      "[I 2025-08-16 16:38:15,620] Trial 347 pruned. \n",
      "[I 2025-08-16 16:38:15,903] Trial 348 pruned. \n",
      "[I 2025-08-16 16:38:16,609] Trial 349 pruned. \n",
      "[I 2025-08-16 16:38:17,039] Trial 350 pruned. \n",
      "[I 2025-08-16 16:38:17,373] Trial 351 pruned. \n",
      "[I 2025-08-16 16:38:17,643] Trial 352 pruned. \n",
      "[I 2025-08-16 16:38:18,179] Trial 353 pruned. \n",
      "[I 2025-08-16 16:38:18,877] Trial 354 pruned. \n",
      "[I 2025-08-16 16:38:19,210] Trial 355 pruned. \n",
      "[I 2025-08-16 16:38:19,491] Trial 356 pruned. \n",
      "[I 2025-08-16 16:38:19,765] Trial 357 pruned. \n",
      "[I 2025-08-16 16:38:20,150] Trial 358 pruned. \n",
      "[I 2025-08-16 16:38:20,517] Trial 359 pruned. \n",
      "[I 2025-08-16 16:38:20,909] Trial 360 pruned. \n",
      "[I 2025-08-16 16:38:21,189] Trial 361 pruned. \n",
      "[I 2025-08-16 16:38:21,568] Trial 362 pruned. \n",
      "[I 2025-08-16 16:38:21,856] Trial 363 pruned. \n",
      "[I 2025-08-16 16:38:22,259] Trial 364 pruned. \n",
      "[I 2025-08-16 16:38:22,525] Trial 365 pruned. \n",
      "[I 2025-08-16 16:38:22,804] Trial 366 pruned. \n",
      "[I 2025-08-16 16:38:23,119] Trial 367 pruned. \n",
      "[I 2025-08-16 16:38:23,465] Trial 368 pruned. \n",
      "[I 2025-08-16 16:38:23,848] Trial 369 pruned. \n",
      "[I 2025-08-16 16:38:24,124] Trial 370 pruned. \n",
      "[I 2025-08-16 16:38:24,437] Trial 371 pruned. \n",
      "[I 2025-08-16 16:38:24,777] Trial 372 pruned. \n",
      "[I 2025-08-16 16:38:25,052] Trial 373 pruned. \n",
      "[I 2025-08-16 16:38:25,423] Trial 374 pruned. \n",
      "[I 2025-08-16 16:38:26,148] Trial 375 pruned. \n",
      "[I 2025-08-16 16:38:26,643] Trial 376 pruned. \n",
      "[I 2025-08-16 16:38:26,917] Trial 377 pruned. \n",
      "[I 2025-08-16 16:38:27,211] Trial 378 pruned. \n",
      "[I 2025-08-16 16:38:27,624] Trial 379 pruned. \n",
      "[I 2025-08-16 16:38:27,999] Trial 380 pruned. \n",
      "[I 2025-08-16 16:38:28,277] Trial 381 pruned. \n",
      "[I 2025-08-16 16:38:28,581] Trial 382 pruned. \n",
      "[I 2025-08-16 16:38:28,913] Trial 383 pruned. \n",
      "[I 2025-08-16 16:38:30,197] Trial 384 pruned. \n",
      "[I 2025-08-16 16:38:30,665] Trial 385 pruned. \n",
      "[I 2025-08-16 16:38:30,943] Trial 386 pruned. \n",
      "[I 2025-08-16 16:38:31,245] Trial 387 pruned. \n",
      "[I 2025-08-16 16:38:31,554] Trial 388 pruned. \n",
      "[I 2025-08-16 16:38:31,891] Trial 389 pruned. \n",
      "[I 2025-08-16 16:38:32,182] Trial 390 pruned. \n",
      "[I 2025-08-16 16:38:33,144] Trial 391 pruned. \n",
      "[I 2025-08-16 16:38:33,484] Trial 392 pruned. \n",
      "[I 2025-08-16 16:38:33,925] Trial 393 pruned. \n",
      "[I 2025-08-16 16:38:34,317] Trial 394 pruned. \n",
      "[I 2025-08-16 16:38:34,752] Trial 395 pruned. \n",
      "[I 2025-08-16 16:38:35,030] Trial 396 pruned. \n",
      "[I 2025-08-16 16:38:35,318] Trial 397 pruned. \n",
      "[I 2025-08-16 16:38:35,700] Trial 398 pruned. \n",
      "[I 2025-08-16 16:38:36,042] Trial 399 pruned. \n",
      "[I 2025-08-16 16:38:36,345] Trial 400 pruned. \n",
      "[I 2025-08-16 16:38:36,628] Trial 401 pruned. \n",
      "[I 2025-08-16 16:38:36,996] Trial 402 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:38:53,197] Trial 403 finished with value: 0.0014089205634424135 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 1, 'lr': 0.0017074979125976109, 'dropout': 0.0517098257184708, 'weight_decay': 5.585852446447122e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:38:53,740] Trial 404 pruned. \n",
      "[I 2025-08-16 16:38:54,109] Trial 405 pruned. \n",
      "[I 2025-08-16 16:38:54,458] Trial 406 pruned. \n",
      "[I 2025-08-16 16:38:54,864] Trial 407 pruned. \n",
      "[I 2025-08-16 16:38:55,216] Trial 408 pruned. \n",
      "[I 2025-08-16 16:38:55,764] Trial 409 pruned. \n",
      "[I 2025-08-16 16:38:56,301] Trial 410 pruned. \n",
      "[I 2025-08-16 16:38:57,126] Trial 411 pruned. \n",
      "[I 2025-08-16 16:38:57,442] Trial 412 pruned. \n",
      "[I 2025-08-16 16:38:57,799] Trial 413 pruned. \n",
      "[I 2025-08-16 16:38:58,190] Trial 414 pruned. \n",
      "[I 2025-08-16 16:38:58,514] Trial 415 pruned. \n",
      "[I 2025-08-16 16:38:58,907] Trial 416 pruned. \n",
      "[I 2025-08-16 16:38:59,261] Trial 417 pruned. \n",
      "[I 2025-08-16 16:38:59,809] Trial 418 pruned. \n",
      "[I 2025-08-16 16:39:00,098] Trial 419 pruned. \n",
      "[I 2025-08-16 16:39:00,433] Trial 420 pruned. \n",
      "[I 2025-08-16 16:39:00,749] Trial 421 pruned. \n",
      "[I 2025-08-16 16:39:01,473] Trial 422 pruned. \n",
      "[I 2025-08-16 16:39:01,759] Trial 423 pruned. \n",
      "[I 2025-08-16 16:39:02,457] Trial 424 pruned. \n",
      "[I 2025-08-16 16:39:02,774] Trial 425 pruned. \n",
      "[I 2025-08-16 16:39:03,435] Trial 426 pruned. \n",
      "[I 2025-08-16 16:39:03,717] Trial 427 pruned. \n",
      "[I 2025-08-16 16:39:04,138] Trial 428 pruned. \n",
      "[I 2025-08-16 16:39:04,605] Trial 429 pruned. \n",
      "[I 2025-08-16 16:39:04,881] Trial 430 pruned. \n",
      "[I 2025-08-16 16:39:05,235] Trial 431 pruned. \n",
      "[I 2025-08-16 16:39:05,518] Trial 432 pruned. \n",
      "[I 2025-08-16 16:39:05,834] Trial 433 pruned. \n",
      "[I 2025-08-16 16:39:06,108] Trial 434 pruned. \n",
      "[I 2025-08-16 16:39:06,646] Trial 435 pruned. \n",
      "[I 2025-08-16 16:39:07,018] Trial 436 pruned. \n",
      "[I 2025-08-16 16:39:07,295] Trial 437 pruned. \n",
      "[I 2025-08-16 16:39:07,618] Trial 438 pruned. \n",
      "[I 2025-08-16 16:39:08,705] Trial 439 pruned. \n",
      "[I 2025-08-16 16:39:09,048] Trial 440 pruned. \n",
      "[I 2025-08-16 16:39:09,412] Trial 441 pruned. \n",
      "[I 2025-08-16 16:39:09,823] Trial 442 pruned. \n",
      "[I 2025-08-16 16:39:10,241] Trial 443 pruned. \n",
      "[I 2025-08-16 16:39:10,613] Trial 444 pruned. \n",
      "[I 2025-08-16 16:39:11,765] Trial 445 pruned. \n",
      "[I 2025-08-16 16:39:12,083] Trial 446 pruned. \n",
      "[I 2025-08-16 16:39:12,410] Trial 447 pruned. \n",
      "[I 2025-08-16 16:39:12,791] Trial 448 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:39:25,209] Trial 449 finished with value: 0.0008840965490569087 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0011248008811555988, 'dropout': 0.052931754312952134, 'weight_decay': 1.743780293811558e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:39:25,486] Trial 450 pruned. \n",
      "[I 2025-08-16 16:39:25,828] Trial 451 pruned. \n",
      "[I 2025-08-16 16:39:26,112] Trial 452 pruned. \n",
      "[I 2025-08-16 16:39:26,397] Trial 453 pruned. \n",
      "[I 2025-08-16 16:39:26,677] Trial 454 pruned. \n",
      "[I 2025-08-16 16:39:26,954] Trial 455 pruned. \n",
      "[I 2025-08-16 16:39:27,235] Trial 456 pruned. \n",
      "[I 2025-08-16 16:39:27,568] Trial 457 pruned. \n",
      "[I 2025-08-16 16:39:27,846] Trial 458 pruned. \n",
      "[I 2025-08-16 16:39:28,128] Trial 459 pruned. \n",
      "[I 2025-08-16 16:39:28,468] Trial 460 pruned. \n",
      "[I 2025-08-16 16:39:28,814] Trial 461 pruned. \n",
      "[I 2025-08-16 16:39:29,131] Trial 462 pruned. \n",
      "[I 2025-08-16 16:39:29,409] Trial 463 pruned. \n",
      "[I 2025-08-16 16:39:29,945] Trial 464 pruned. \n",
      "[I 2025-08-16 16:39:30,345] Trial 465 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:39:32,680] Trial 466 pruned. \n",
      "[I 2025-08-16 16:39:33,241] Trial 467 pruned. \n",
      "[I 2025-08-16 16:39:34,255] Trial 468 pruned. \n",
      "[I 2025-08-16 16:39:34,633] Trial 469 pruned. \n",
      "[I 2025-08-16 16:39:35,001] Trial 470 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:39:47,452] Trial 471 finished with value: 0.000897967756506713 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.001695201754574508, 'dropout': 0.0513045584079612, 'weight_decay': 2.9186443466911943e-05}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:39:47,730] Trial 472 pruned. \n",
      "[I 2025-08-16 16:39:48,007] Trial 473 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:39:50,831] Trial 474 pruned. \n",
      "[I 2025-08-16 16:39:51,106] Trial 475 pruned. \n",
      "[I 2025-08-16 16:39:51,381] Trial 476 pruned. \n",
      "[I 2025-08-16 16:39:52,243] Trial 477 pruned. \n",
      "[I 2025-08-16 16:39:52,517] Trial 478 pruned. \n",
      "[I 2025-08-16 16:39:52,940] Trial 479 pruned. \n",
      "[I 2025-08-16 16:39:53,249] Trial 480 pruned. \n",
      "[I 2025-08-16 16:39:53,565] Trial 481 pruned. \n",
      "[I 2025-08-16 16:39:54,028] Trial 482 pruned. \n",
      "[I 2025-08-16 16:39:54,339] Trial 483 pruned. \n",
      "[I 2025-08-16 16:39:54,619] Trial 484 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:40:06,793] Trial 485 finished with value: 0.0007329690937801977 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0008881903997482037, 'dropout': 0.05315556663841068, 'weight_decay': 6.545701524057609e-05}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:40:07,235] Trial 486 pruned. \n",
      "[I 2025-08-16 16:40:07,670] Trial 487 pruned. \n",
      "[I 2025-08-16 16:40:07,959] Trial 488 pruned. \n",
      "[I 2025-08-16 16:40:08,314] Trial 489 pruned. \n",
      "[I 2025-08-16 16:40:08,606] Trial 490 pruned. \n",
      "[I 2025-08-16 16:40:08,902] Trial 491 pruned. \n",
      "[I 2025-08-16 16:40:09,264] Trial 492 pruned. \n",
      "[I 2025-08-16 16:40:09,548] Trial 493 pruned. \n",
      "[I 2025-08-16 16:40:09,829] Trial 494 pruned. \n",
      "[I 2025-08-16 16:40:10,111] Trial 495 pruned. \n",
      "[I 2025-08-16 16:40:10,453] Trial 496 pruned. \n",
      "[I 2025-08-16 16:40:10,798] Trial 497 pruned. \n",
      "[I 2025-08-16 16:40:11,083] Trial 498 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:40:23,675] Trial 499 finished with value: 0.0007378159412730704 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0007908762524729964, 'dropout': 0.05155525349574858, 'weight_decay': 3.700094751500491e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:40:24,023] Trial 500 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:40:36,598] Trial 501 finished with value: 0.0005120816982953864 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.0007523874597008274, 'dropout': 0.054379959386939916, 'weight_decay': 2.9580175242559716e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:40:36,893] Trial 502 pruned. \n",
      "[I 2025-08-16 16:40:37,189] Trial 503 pruned. \n",
      "[I 2025-08-16 16:40:37,493] Trial 504 pruned. \n",
      "[I 2025-08-16 16:40:37,838] Trial 505 pruned. \n",
      "[I 2025-08-16 16:40:38,136] Trial 506 pruned. \n",
      "[I 2025-08-16 16:40:38,467] Trial 507 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:40:40,545] Trial 508 pruned. \n",
      "[I 2025-08-16 16:40:40,890] Trial 509 pruned. \n",
      "[I 2025-08-16 16:40:41,176] Trial 510 pruned. \n",
      "[I 2025-08-16 16:40:41,462] Trial 511 pruned. \n",
      "[I 2025-08-16 16:40:41,753] Trial 512 pruned. \n",
      "[I 2025-08-16 16:40:42,035] Trial 513 pruned. \n",
      "[I 2025-08-16 16:40:42,331] Trial 514 pruned. \n",
      "[I 2025-08-16 16:40:42,672] Trial 515 pruned. \n",
      "[I 2025-08-16 16:40:43,098] Trial 516 pruned. \n",
      "[I 2025-08-16 16:40:43,391] Trial 517 pruned. \n",
      "[I 2025-08-16 16:40:43,674] Trial 518 pruned. \n",
      "[I 2025-08-16 16:40:44,050] Trial 519 pruned. \n",
      "[I 2025-08-16 16:40:44,331] Trial 520 pruned. \n",
      "[I 2025-08-16 16:40:44,896] Trial 521 pruned. \n",
      "[I 2025-08-16 16:40:45,190] Trial 522 pruned. \n",
      "[I 2025-08-16 16:40:45,523] Trial 523 pruned. \n",
      "[I 2025-08-16 16:40:45,810] Trial 524 pruned. \n",
      "[I 2025-08-16 16:40:46,094] Trial 525 pruned. \n",
      "[I 2025-08-16 16:40:46,386] Trial 526 pruned. \n",
      "[I 2025-08-16 16:40:46,663] Trial 527 pruned. \n",
      "[I 2025-08-16 16:40:46,948] Trial 528 pruned. \n",
      "[I 2025-08-16 16:40:47,283] Trial 529 pruned. \n",
      "[I 2025-08-16 16:40:47,571] Trial 530 pruned. \n",
      "[I 2025-08-16 16:40:47,846] Trial 531 pruned. \n",
      "[I 2025-08-16 16:40:48,227] Trial 532 pruned. \n",
      "[I 2025-08-16 16:40:48,508] Trial 533 pruned. \n",
      "[I 2025-08-16 16:40:48,799] Trial 534 pruned. \n",
      "[I 2025-08-16 16:40:49,869] Trial 535 pruned. \n",
      "[I 2025-08-16 16:40:50,964] Trial 536 pruned. \n",
      "[I 2025-08-16 16:40:51,341] Trial 537 pruned. \n",
      "[I 2025-08-16 16:40:51,615] Trial 538 pruned. \n",
      "[I 2025-08-16 16:40:51,904] Trial 539 pruned. \n",
      "[I 2025-08-16 16:40:52,203] Trial 540 pruned. \n",
      "[I 2025-08-16 16:40:52,585] Trial 541 pruned. \n",
      "[I 2025-08-16 16:40:53,672] Trial 542 pruned. \n",
      "[I 2025-08-16 16:40:54,329] Trial 543 pruned. \n",
      "[I 2025-08-16 16:40:54,846] Trial 544 pruned. \n",
      "[I 2025-08-16 16:40:55,371] Trial 545 pruned. \n",
      "[I 2025-08-16 16:40:55,683] Trial 546 pruned. \n",
      "[I 2025-08-16 16:40:55,983] Trial 547 pruned. \n",
      "[I 2025-08-16 16:40:56,268] Trial 548 pruned. \n",
      "[I 2025-08-16 16:40:56,658] Trial 549 pruned. \n",
      "[I 2025-08-16 16:40:57,087] Trial 550 pruned. \n",
      "[I 2025-08-16 16:40:57,377] Trial 551 pruned. \n",
      "[I 2025-08-16 16:40:57,659] Trial 552 pruned. \n",
      "[I 2025-08-16 16:40:57,951] Trial 553 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:41:13,830] Trial 554 finished with value: 0.0009441329731249853 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'lr': 0.0008407491464411416, 'dropout': 0.05002542476312887, 'weight_decay': 3.152870330604674e-05}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:41:14,164] Trial 555 pruned. \n",
      "[I 2025-08-16 16:41:14,770] Trial 556 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:41:30,240] Trial 557 finished with value: 0.0010752845418584697 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'num_decoder_layers': 2, 'lr': 0.0008436624656114319, 'dropout': 0.05317555979868476, 'weight_decay': 3.2564943466782633e-05}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:41:30,852] Trial 558 pruned. \n",
      "[I 2025-08-16 16:41:31,197] Trial 559 pruned. \n",
      "[I 2025-08-16 16:41:32,317] Trial 560 pruned. \n",
      "[I 2025-08-16 16:41:32,694] Trial 561 pruned. \n",
      "[I 2025-08-16 16:41:33,028] Trial 562 pruned. \n",
      "[I 2025-08-16 16:41:33,399] Trial 563 pruned. \n",
      "[I 2025-08-16 16:41:33,743] Trial 564 pruned. \n",
      "[I 2025-08-16 16:41:34,119] Trial 565 pruned. \n",
      "[I 2025-08-16 16:41:34,458] Trial 566 pruned. \n",
      "[I 2025-08-16 16:41:34,837] Trial 567 pruned. \n",
      "[I 2025-08-16 16:41:35,368] Trial 568 pruned. \n",
      "[I 2025-08-16 16:41:35,748] Trial 569 pruned. \n",
      "[I 2025-08-16 16:41:36,129] Trial 570 pruned. \n",
      "[I 2025-08-16 16:41:36,508] Trial 571 pruned. \n",
      "[I 2025-08-16 16:41:37,435] Trial 572 pruned. \n",
      "[I 2025-08-16 16:41:37,819] Trial 573 pruned. \n",
      "[I 2025-08-16 16:41:38,187] Trial 574 pruned. \n",
      "[I 2025-08-16 16:41:38,568] Trial 575 pruned. \n",
      "[I 2025-08-16 16:41:38,920] Trial 576 pruned. \n",
      "[I 2025-08-16 16:41:39,269] Trial 577 pruned. \n",
      "[I 2025-08-16 16:41:39,815] Trial 578 pruned. \n",
      "[I 2025-08-16 16:41:40,225] Trial 579 pruned. \n",
      "[I 2025-08-16 16:41:40,569] Trial 580 pruned. \n",
      "[I 2025-08-16 16:41:41,169] Trial 581 pruned. \n",
      "[I 2025-08-16 16:41:41,514] Trial 582 pruned. \n",
      "[I 2025-08-16 16:41:41,894] Trial 583 pruned. \n",
      "[I 2025-08-16 16:41:42,231] Trial 584 pruned. \n",
      "[I 2025-08-16 16:41:42,616] Trial 585 pruned. \n",
      "[I 2025-08-16 16:41:42,903] Trial 586 pruned. \n",
      "[I 2025-08-16 16:41:43,187] Trial 587 pruned. \n",
      "[I 2025-08-16 16:41:43,560] Trial 588 pruned. \n",
      "[I 2025-08-16 16:41:43,848] Trial 589 pruned. \n",
      "[I 2025-08-16 16:41:44,132] Trial 590 pruned. \n",
      "[I 2025-08-16 16:41:44,473] Trial 591 pruned. \n",
      "[I 2025-08-16 16:41:44,775] Trial 592 pruned. \n",
      "[I 2025-08-16 16:41:45,053] Trial 593 pruned. \n",
      "[I 2025-08-16 16:41:45,428] Trial 594 pruned. \n",
      "[I 2025-08-16 16:41:45,715] Trial 595 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:41:48,025] Trial 596 pruned. \n",
      "[I 2025-08-16 16:41:48,567] Trial 597 pruned. \n",
      "[I 2025-08-16 16:41:48,863] Trial 598 pruned. \n",
      "[I 2025-08-16 16:41:49,151] Trial 599 pruned. \n",
      "[I 2025-08-16 16:41:49,443] Trial 600 pruned. \n",
      "[I 2025-08-16 16:41:49,794] Trial 601 pruned. \n",
      "[I 2025-08-16 16:41:50,091] Trial 602 pruned. \n",
      "[I 2025-08-16 16:41:50,379] Trial 603 pruned. \n",
      "[I 2025-08-16 16:41:50,730] Trial 604 pruned. \n",
      "[I 2025-08-16 16:41:51,225] Trial 605 pruned. \n",
      "[I 2025-08-16 16:41:51,505] Trial 606 pruned. \n",
      "[I 2025-08-16 16:41:51,792] Trial 607 pruned. \n",
      "[I 2025-08-16 16:41:52,090] Trial 608 pruned. \n",
      "[I 2025-08-16 16:41:52,637] Trial 609 pruned. \n",
      "[I 2025-08-16 16:41:52,938] Trial 610 pruned. \n",
      "[I 2025-08-16 16:41:53,975] Trial 611 pruned. \n",
      "[I 2025-08-16 16:41:54,266] Trial 612 pruned. \n",
      "[I 2025-08-16 16:41:54,549] Trial 613 pruned. \n",
      "[I 2025-08-16 16:41:54,895] Trial 614 pruned. \n",
      "[I 2025-08-16 16:41:55,186] Trial 615 pruned. \n",
      "[I 2025-08-16 16:41:55,477] Trial 616 pruned. \n",
      "[I 2025-08-16 16:41:55,889] Trial 617 pruned. \n",
      "[I 2025-08-16 16:41:56,280] Trial 618 pruned. \n",
      "[I 2025-08-16 16:41:56,600] Trial 619 pruned. \n",
      "[I 2025-08-16 16:41:56,981] Trial 620 pruned. \n",
      "[I 2025-08-16 16:41:57,303] Trial 621 pruned. \n",
      "[I 2025-08-16 16:41:57,877] Trial 622 pruned. \n",
      "[I 2025-08-16 16:41:58,164] Trial 623 pruned. \n",
      "[I 2025-08-16 16:41:58,515] Trial 624 pruned. \n",
      "[I 2025-08-16 16:41:58,970] Trial 625 pruned. \n",
      "[I 2025-08-16 16:41:59,847] Trial 626 pruned. \n",
      "[I 2025-08-16 16:42:00,236] Trial 627 pruned. \n",
      "[I 2025-08-16 16:42:00,525] Trial 628 pruned. \n",
      "[I 2025-08-16 16:42:00,815] Trial 629 pruned. \n",
      "[I 2025-08-16 16:42:01,100] Trial 630 pruned. \n",
      "[I 2025-08-16 16:42:01,401] Trial 631 pruned. \n",
      "[I 2025-08-16 16:42:01,811] Trial 632 pruned. \n",
      "[I 2025-08-16 16:42:02,110] Trial 633 pruned. \n",
      "[I 2025-08-16 16:42:02,410] Trial 634 pruned. \n",
      "[I 2025-08-16 16:42:02,799] Trial 635 pruned. \n",
      "[I 2025-08-16 16:42:03,121] Trial 636 pruned. \n",
      "[I 2025-08-16 16:42:03,422] Trial 637 pruned. \n",
      "[I 2025-08-16 16:42:04,356] Trial 638 pruned. \n",
      "[I 2025-08-16 16:42:04,651] Trial 639 pruned. \n",
      "[I 2025-08-16 16:42:04,932] Trial 640 pruned. \n",
      "[I 2025-08-16 16:42:05,374] Trial 641 pruned. \n",
      "[I 2025-08-16 16:42:05,851] Trial 642 pruned. \n",
      "[I 2025-08-16 16:42:06,150] Trial 643 pruned. \n",
      "[I 2025-08-16 16:42:06,448] Trial 644 pruned. \n",
      "[I 2025-08-16 16:42:06,809] Trial 645 pruned. \n",
      "[I 2025-08-16 16:42:07,140] Trial 646 pruned. \n",
      "[I 2025-08-16 16:42:07,911] Trial 647 pruned. \n",
      "[I 2025-08-16 16:42:08,266] Trial 648 pruned. \n",
      "[I 2025-08-16 16:42:08,572] Trial 649 pruned. \n",
      "[I 2025-08-16 16:42:08,866] Trial 650 pruned. \n",
      "[I 2025-08-16 16:42:09,254] Trial 651 pruned. \n",
      "[I 2025-08-16 16:42:09,580] Trial 652 pruned. \n",
      "[I 2025-08-16 16:42:09,964] Trial 653 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:42:12,449] Trial 654 pruned. \n",
      "[I 2025-08-16 16:42:12,747] Trial 655 pruned. \n",
      "[I 2025-08-16 16:42:13,031] Trial 656 pruned. \n",
      "[I 2025-08-16 16:42:13,376] Trial 657 pruned. \n",
      "[I 2025-08-16 16:42:14,100] Trial 658 pruned. \n",
      "[I 2025-08-16 16:42:14,548] Trial 659 pruned. \n",
      "[I 2025-08-16 16:42:14,841] Trial 660 pruned. \n",
      "[I 2025-08-16 16:42:15,197] Trial 661 pruned. \n",
      "[I 2025-08-16 16:42:15,487] Trial 662 pruned. \n",
      "[I 2025-08-16 16:42:15,813] Trial 663 pruned. \n",
      "[I 2025-08-16 16:42:16,227] Trial 664 pruned. \n",
      "[I 2025-08-16 16:42:16,529] Trial 665 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:42:28,985] Trial 666 finished with value: 0.0009924703729612863 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'num_decoder_layers': 1, 'lr': 0.001863139741982903, 'dropout': 0.051746618397908166, 'weight_decay': 7.719454385040013e-06}. Best is trial 213 with value: 0.0004531779747648055.\n",
      "[I 2025-08-16 16:42:29,264] Trial 667 pruned. \n",
      "[I 2025-08-16 16:42:29,549] Trial 668 pruned. \n",
      "[I 2025-08-16 16:42:29,839] Trial 669 pruned. \n",
      "[I 2025-08-16 16:42:30,120] Trial 670 pruned. \n",
      "[I 2025-08-16 16:42:30,557] Trial 671 pruned. \n",
      "[I 2025-08-16 16:42:30,839] Trial 672 pruned. \n",
      "[I 2025-08-16 16:42:31,124] Trial 673 pruned. \n",
      "[I 2025-08-16 16:42:31,408] Trial 674 pruned. \n",
      "[I 2025-08-16 16:42:31,691] Trial 675 pruned. \n",
      "[I 2025-08-16 16:42:31,974] Trial 676 pruned. \n",
      "[I 2025-08-16 16:42:32,260] Trial 677 pruned. \n",
      "[I 2025-08-16 16:42:32,549] Trial 678 pruned. \n",
      "[I 2025-08-16 16:42:32,831] Trial 679 pruned. \n",
      "[I 2025-08-16 16:42:33,254] Trial 680 pruned. \n",
      "[I 2025-08-16 16:42:33,538] Trial 681 pruned. \n",
      "[I 2025-08-16 16:42:33,818] Trial 682 pruned. \n",
      "[I 2025-08-16 16:42:34,212] Trial 683 pruned. \n",
      "[I 2025-08-16 16:42:34,500] Trial 684 pruned. \n",
      "[I 2025-08-16 16:42:34,790] Trial 685 pruned. \n",
      "[I 2025-08-16 16:42:35,080] Trial 686 pruned. \n",
      "[I 2025-08-16 16:42:35,371] Trial 687 pruned. \n",
      "[I 2025-08-16 16:42:35,660] Trial 688 pruned. \n",
      "[I 2025-08-16 16:42:36,700] Trial 689 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:42:38,606] Trial 690 pruned. \n",
      "[I 2025-08-16 16:42:38,894] Trial 691 pruned. \n",
      "[I 2025-08-16 16:42:39,186] Trial 692 pruned. \n",
      "[I 2025-08-16 16:42:39,474] Trial 693 pruned. \n",
      "[I 2025-08-16 16:42:40,195] Trial 694 pruned. \n",
      "[I 2025-08-16 16:42:40,487] Trial 695 pruned. \n",
      "[I 2025-08-16 16:42:40,770] Trial 696 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:42:44,976] Trial 697 pruned. \n",
      "[I 2025-08-16 16:42:45,274] Trial 698 pruned. \n",
      "[I 2025-08-16 16:42:45,591] Trial 699 pruned. \n",
      "[I 2025-08-16 16:42:45,904] Trial 700 pruned. \n",
      "[I 2025-08-16 16:42:46,190] Trial 701 pruned. \n",
      "[I 2025-08-16 16:42:46,648] Trial 702 pruned. \n",
      "[I 2025-08-16 16:42:46,963] Trial 703 pruned. \n",
      "[I 2025-08-16 16:42:47,263] Trial 704 pruned. \n",
      "[I 2025-08-16 16:42:47,550] Trial 705 pruned. \n",
      "[I 2025-08-16 16:42:48,015] Trial 706 pruned. \n",
      "[I 2025-08-16 16:42:48,302] Trial 707 pruned. \n",
      "[I 2025-08-16 16:42:48,589] Trial 708 pruned. \n",
      "[I 2025-08-16 16:42:48,882] Trial 709 pruned. \n",
      "[I 2025-08-16 16:42:49,237] Trial 710 pruned. \n",
      "[I 2025-08-16 16:42:49,541] Trial 711 pruned. \n",
      "[I 2025-08-16 16:42:49,830] Trial 712 pruned. \n",
      "[I 2025-08-16 16:42:50,128] Trial 713 pruned. \n",
      "[I 2025-08-16 16:42:50,412] Trial 714 pruned. \n",
      "[I 2025-08-16 16:42:50,695] Trial 715 pruned. \n",
      "[I 2025-08-16 16:42:50,996] Trial 716 pruned. \n",
      "[I 2025-08-16 16:42:51,337] Trial 717 pruned. \n",
      "[I 2025-08-16 16:42:51,657] Trial 718 pruned. \n",
      "[I 2025-08-16 16:42:52,288] Trial 719 pruned. \n",
      "[I 2025-08-16 16:42:52,822] Trial 720 pruned. \n",
      "[I 2025-08-16 16:42:53,123] Trial 721 pruned. \n",
      "[I 2025-08-16 16:42:53,470] Trial 722 pruned. \n",
      "[I 2025-08-16 16:42:54,036] Trial 723 pruned. \n",
      "[I 2025-08-16 16:42:54,351] Trial 724 pruned. \n",
      "[I 2025-08-16 16:42:54,709] Trial 725 pruned. \n",
      "[I 2025-08-16 16:42:54,996] Trial 726 pruned. \n",
      "[I 2025-08-16 16:42:55,296] Trial 727 pruned. \n",
      "[I 2025-08-16 16:42:55,589] Trial 728 pruned. \n",
      "[I 2025-08-16 16:42:55,934] Trial 729 pruned. \n",
      "[I 2025-08-16 16:42:56,240] Trial 730 pruned. \n",
      "[I 2025-08-16 16:42:56,531] Trial 731 pruned. \n",
      "[I 2025-08-16 16:42:56,908] Trial 732 pruned. \n",
      "[I 2025-08-16 16:42:57,508] Trial 733 pruned. \n",
      "[I 2025-08-16 16:42:57,834] Trial 734 pruned. \n",
      "[I 2025-08-16 16:42:58,215] Trial 735 pruned. \n",
      "[I 2025-08-16 16:42:58,530] Trial 736 pruned. \n",
      "[I 2025-08-16 16:42:58,834] Trial 737 pruned. \n",
      "[I 2025-08-16 16:42:59,153] Trial 738 pruned. \n",
      "[I 2025-08-16 16:42:59,501] Trial 739 pruned. \n",
      "[I 2025-08-16 16:42:59,798] Trial 740 pruned. \n",
      "[I 2025-08-16 16:43:00,144] Trial 741 pruned. \n",
      "[I 2025-08-16 16:43:00,474] Trial 742 pruned. \n",
      "/home/tensor/PharmaControl/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:501: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-08-16 16:43:03,337] Trial 743 pruned. \n",
      "[I 2025-08-16 16:43:03,807] Trial 744 pruned. \n",
      "[I 2025-08-16 16:43:04,239] Trial 745 pruned. \n",
      "[I 2025-08-16 16:43:04,587] Trial 746 pruned. \n",
      "[I 2025-08-16 16:43:04,862] Trial 747 pruned. \n",
      "[I 2025-08-16 16:43:05,152] Trial 748 pruned. \n",
      "[I 2025-08-16 16:43:05,446] Trial 749 pruned. \n",
      "[I 2025-08-16 16:43:05,728] Trial 750 pruned. \n",
      "[I 2025-08-16 16:43:06,057] Trial 751 pruned. \n",
      "[I 2025-08-16 16:43:06,599] Trial 752 pruned. \n",
      "[I 2025-08-16 16:43:06,893] Trial 753 pruned. \n",
      "[I 2025-08-16 16:43:07,428] Trial 754 pruned. \n",
      "[I 2025-08-16 16:43:07,723] Trial 755 pruned. \n",
      "[I 2025-08-16 16:43:08,019] Trial 756 pruned. \n",
      "[I 2025-08-16 16:43:08,434] Trial 757 pruned. \n",
      "[I 2025-08-16 16:43:08,723] Trial 758 pruned. \n",
      "[I 2025-08-16 16:43:09,009] Trial 759 pruned. \n",
      "[I 2025-08-16 16:43:09,340] Trial 760 pruned. \n",
      "[I 2025-08-16 16:43:09,691] Trial 761 pruned. \n",
      "[I 2025-08-16 16:43:10,097] Trial 762 pruned. \n",
      "[I 2025-08-16 16:43:10,397] Trial 763 pruned. \n",
      "[I 2025-08-16 16:43:10,681] Trial 764 pruned. \n",
      "[I 2025-08-16 16:43:10,978] Trial 765 pruned. \n",
      "[I 2025-08-16 16:43:11,272] Trial 766 pruned. \n",
      "[I 2025-08-16 16:43:11,623] Trial 767 pruned. \n",
      "[I 2025-08-16 16:43:12,002] Trial 768 pruned. \n",
      "[I 2025-08-16 16:43:12,398] Trial 769 pruned. \n",
      "[I 2025-08-16 16:43:12,697] Trial 770 pruned. \n",
      "[I 2025-08-16 16:43:12,989] Trial 771 pruned. \n",
      "[I 2025-08-16 16:43:13,294] Trial 772 pruned. \n",
      "[I 2025-08-16 16:43:14,016] Trial 773 pruned. \n",
      "[I 2025-08-16 16:43:14,303] Trial 774 pruned. \n",
      "[I 2025-08-16 16:43:14,657] Trial 775 pruned. \n",
      "[I 2025-08-16 16:43:14,955] Trial 776 pruned. \n",
      "[I 2025-08-16 16:43:15,282] Trial 777 pruned. \n",
      "[I 2025-08-16 16:43:15,640] Trial 778 pruned. \n",
      "[I 2025-08-16 16:43:15,936] Trial 779 pruned. \n",
      "[I 2025-08-16 16:43:16,232] Trial 780 pruned. \n",
      "[I 2025-08-16 16:43:16,523] Trial 781 pruned. \n",
      "[I 2025-08-16 16:43:16,821] Trial 782 pruned. \n",
      "[I 2025-08-16 16:43:17,167] Trial 783 pruned. \n",
      "[I 2025-08-16 16:43:17,463] Trial 784 pruned. \n",
      "[I 2025-08-16 16:43:17,938] Trial 785 pruned. \n",
      "[I 2025-08-16 16:43:18,289] Trial 786 pruned. \n",
      "[I 2025-08-16 16:43:18,591] Trial 787 pruned. \n",
      "[I 2025-08-16 16:43:18,960] Trial 788 pruned. \n",
      "[I 2025-08-16 16:43:19,266] Trial 789 pruned. \n",
      "[I 2025-08-16 16:43:19,609] Trial 790 pruned. \n",
      "[I 2025-08-16 16:43:19,935] Trial 791 pruned. \n",
      "[I 2025-08-16 16:43:20,220] Trial 792 pruned. \n",
      "[I 2025-08-16 16:43:20,606] Trial 793 pruned. \n",
      "[I 2025-08-16 16:43:20,904] Trial 794 pruned. \n",
      "[I 2025-08-16 16:43:21,228] Trial 795 pruned. \n",
      "[I 2025-08-16 16:43:21,526] Trial 796 pruned. \n",
      "[I 2025-08-16 16:43:21,810] Trial 797 pruned. \n",
      "[I 2025-08-16 16:43:22,654] Trial 798 pruned. \n",
      "[I 2025-08-16 16:43:23,072] Trial 799 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Hyperparameter optimization completed!\n",
      "Best trial found:\n",
      "  Validation Loss: 0.000453\n",
      "  Parameters:\n",
      "    d_model: 128\n",
      "    nhead: 8\n",
      "    num_encoder_layers: 1\n",
      "    num_decoder_layers: 1\n",
      "    lr: 0.0009059439748286063\n",
      "    dropout: 0.050044940567043526\n",
      "    weight_decay: 1.6434038497210028e-06\n"
     ]
    }
   ],
   "source": [
    "# --- Run Configured Optuna Study ---\n",
    "print(f\"\\n🔍 Starting hyperparameter optimization...\")\n",
    "print(f\"  Trials: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(f\"  Epochs per trial: {OPTUNA_CONFIG['tuning_epochs']}\")\n",
    "print(f\"  Batch size: {OPTUNA_CONFIG['tuning_batch_size']}\")\n",
    "\n",
    "# Create study with pruning\n",
    "study = optuna.create_study(\n",
    "   # direction=OPTUNA_CONFIG['study_direction'],\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=OPTUNA_CONFIG['n_trials'])\n",
    "\n",
    "# Save study for later analysis\n",
    "joblib.dump(study, PATHS['optuna_study'])\n",
    "\n",
    "print(f\"\\n🎯 Hyperparameter optimization completed!\")\n",
    "print(f\"Best trial found:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Validation Loss: {best_trial.value:.6f}\")\n",
    "print(f\"  Parameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Store best hyperparameters for final training\n",
    "BEST_HPARAMS = best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training the Final Model\n",
    "\n",
    "Now that we have identified the optimal hyperparameters through Optuna, we'll train the final model using the complete training configuration with early stopping, learning rate scheduling, and comprehensive logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting final model training with best hyperparameters...\n",
      "  Epochs: 50\n",
      "  Batch size: 64\n",
      "  Early stopping patience: 8\n",
      "\n",
      "📊 Training progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 163/163 [00:02<00:00, 56.06it/s, loss=0.003055, avg_loss=0.033704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train: 0.033704 | Val: 0.001405 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.001405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 163/163 [00:03<00:00, 50.90it/s, loss=0.001684, avg_loss=0.002241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 | Train: 0.002241 | Val: 0.000797 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 163/163 [00:02<00:00, 54.71it/s, loss=0.001324, avg_loss=0.001507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 | Train: 0.001507 | Val: 0.000689 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 163/163 [00:02<00:00, 54.82it/s, loss=0.001027, avg_loss=0.001202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 | Train: 0.001202 | Val: 0.000535 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 163/163 [00:03<00:00, 53.19it/s, loss=0.000863, avg_loss=0.001011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 | Train: 0.001011 | Val: 0.001315 | LR: 9.06e-04\n",
      "    → No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 163/163 [00:03<00:00, 53.77it/s, loss=0.000867, avg_loss=0.000908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 | Train: 0.000908 | Val: 0.000713 | LR: 9.06e-04\n",
      "    → No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 163/163 [00:03<00:00, 54.18it/s, loss=0.000715, avg_loss=0.000845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 | Train: 0.000845 | Val: 0.000604 | LR: 9.06e-04\n",
      "    → No improvement for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 163/163 [00:02<00:00, 55.54it/s, loss=0.000618, avg_loss=0.000726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8 | Train: 0.000726 | Val: 0.000454 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 163/163 [00:02<00:00, 54.61it/s, loss=0.000641, avg_loss=0.000675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 | Train: 0.000675 | Val: 0.000438 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 163/163 [00:02<00:00, 55.98it/s, loss=0.000562, avg_loss=0.000653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 | Train: 0.000653 | Val: 0.000356 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 163/163 [00:02<00:00, 55.47it/s, loss=0.000537, avg_loss=0.000565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11 | Train: 0.000565 | Val: 0.000379 | LR: 9.06e-04\n",
      "    → No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 163/163 [00:02<00:00, 56.68it/s, loss=0.000498, avg_loss=0.000558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12 | Train: 0.000558 | Val: 0.000531 | LR: 9.06e-04\n",
      "    → No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 163/163 [00:02<00:00, 55.53it/s, loss=0.000511, avg_loss=0.000503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 | Train: 0.000503 | Val: 0.000376 | LR: 9.06e-04\n",
      "    → No improvement for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 163/163 [00:02<00:00, 58.32it/s, loss=0.000423, avg_loss=0.000486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14 | Train: 0.000486 | Val: 0.000428 | LR: 9.06e-04\n",
      "    → No improvement for 4 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 163/163 [00:02<00:00, 57.15it/s, loss=0.000406, avg_loss=0.000490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15 | Train: 0.000490 | Val: 0.000315 | LR: 9.06e-04\n",
      "    ✓ New best validation loss: 0.000315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 163/163 [00:02<00:00, 58.93it/s, loss=0.000444, avg_loss=0.000449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16 | Train: 0.000449 | Val: 0.000382 | LR: 9.06e-04\n",
      "    → No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 163/163 [00:03<00:00, 46.58it/s, loss=0.000539, avg_loss=0.000480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17 | Train: 0.000480 | Val: 0.000393 | LR: 9.06e-04\n",
      "    → No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 163/163 [00:02<00:00, 58.12it/s, loss=0.000408, avg_loss=0.000435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18 | Train: 0.000435 | Val: 0.000467 | LR: 9.06e-04\n",
      "    → No improvement for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 163/163 [00:03<00:00, 53.76it/s, loss=0.000408, avg_loss=0.000420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19 | Train: 0.000420 | Val: 0.000333 | LR: 9.06e-04\n",
      "    → No improvement for 4 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 163/163 [00:03<00:00, 50.26it/s, loss=0.000513, avg_loss=0.000422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20 | Train: 0.000422 | Val: 0.000475 | LR: 9.06e-04\n",
      "    → No improvement for 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 163/163 [00:02<00:00, 56.71it/s, loss=0.000388, avg_loss=0.000421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📉 Learning rate reduced: 9.06e-04 → 4.53e-04\n",
      "Epoch  21 | Train: 0.000421 | Val: 0.000609 | LR: 9.06e-04\n",
      "    → No improvement for 6 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 163/163 [00:03<00:00, 52.11it/s, loss=0.000370, avg_loss=0.000370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 | Train: 0.000370 | Val: 0.000360 | LR: 4.53e-04\n",
      "    → No improvement for 7 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 163/163 [00:02<00:00, 56.66it/s, loss=0.000394, avg_loss=0.000362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23 | Train: 0.000362 | Val: 0.000344 | LR: 4.53e-04\n",
      "    → No improvement for 8 epochs\n",
      "\n",
      "🛑 Early stopping triggered after 23 epochs (patience: 8)\n",
      "\n",
      "✅ Training completed successfully!\n",
      "  Best validation loss: 0.000315\n",
      "  Model saved to: ../data/best_predictor_model.pth\n",
      "  Training log saved to: ../data/training_log.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model Training with Configuration ---\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\n🚀 Starting final model training with best hyperparameters...\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['final_epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['final_batch_size']}\")\n",
    "print(f\"  Early stopping patience: {TRAINING_CONFIG['patience']}\")\n",
    "\n",
    "# Create datasets and loaders with final configuration\n",
    "train_dataset = GranulationDataset(df_train, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "val_dataset = GranulationDataset(df_val, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=TRAINING_CONFIG['final_batch_size'], \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=TRAINING_CONFIG['final_batch_size'], \n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Initialize final model with best hyperparameters\n",
    "final_model = GranulationPredictor(\n",
    "    cma_features=len(CMA_COLS),\n",
    "    cpp_features=len(CPP_COLS),\n",
    "    d_model=BEST_HPARAMS['d_model'], \n",
    "    nhead=BEST_HPARAMS['nhead'],\n",
    "    num_encoder_layers=BEST_HPARAMS['num_encoder_layers'],\n",
    "    num_decoder_layers=BEST_HPARAMS['num_decoder_layers'],\n",
    "    dropout=BEST_HPARAMS['dropout']\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer with configuration\n",
    "criterion = WeightedHorizonMSELoss(\n",
    "    horizon=HORIZON,\n",
    "    start_weight=LOSS_CONFIG['start_weight'],\n",
    "    end_weight=LOSS_CONFIG['end_weight']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(), \n",
    "    lr=BEST_HPARAMS['lr'],\n",
    "    weight_decay=BEST_HPARAMS.get('weight_decay', 0.0)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (fixed verbose parameter)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Training tracking\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_wts = copy.deepcopy(final_model.state_dict())\n",
    "training_history = []\n",
    "\n",
    "# Initialize training log\n",
    "log_fields = ['epoch', 'train_loss', 'val_loss', 'lr', 'timestamp']\n",
    "with open(PATHS['training_log'], 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(log_fields)\n",
    "\n",
    "print(f\"\\n📊 Training progress:\")\n",
    "\n",
    "for epoch in range(TRAINING_CONFIG['final_epochs']):\n",
    "    # Training phase\n",
    "    final_model.train()\n",
    "    train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TRAINING_CONFIG['final_epochs']}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = final_model(past_cmas, past_cpps, future_cpps)\n",
    "        loss = criterion(prediction, future_cmas_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), TRAINING_CONFIG['gradient_clip_value'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.6f}\",\n",
    "            'avg_loss': f\"{train_loss/num_batches:.6f}\"\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / num_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    final_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n",
    "            prediction = final_model(past_cmas, past_cpps, future_cpps)\n",
    "            val_loss += criterion(prediction, future_cmas_target).item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    old_lr = current_lr\n",
    "    scheduler.step(avg_val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Manual verbose output for learning rate changes\n",
    "    if new_lr != old_lr:\n",
    "        print(f\"\\n📉 Learning rate reduced: {old_lr:.2e} → {new_lr:.2e}\")\n",
    "    \n",
    "    # Log training metrics\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'lr': current_lr,\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "    \n",
    "    # Save to CSV log\n",
    "    with open(PATHS['training_log'], 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, current_lr, timestamp])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d} | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping logic with configurable parameters\n",
    "    if avg_val_loss < best_val_loss - TRAINING_CONFIG['min_delta']:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_wts = copy.deepcopy(final_model.state_dict())\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"    ✓ New best validation loss: {best_val_loss:.6f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"    → No improvement for {epochs_no_improve} epochs\")\n",
    "    \n",
    "    if epochs_no_improve >= TRAINING_CONFIG['patience']:\n",
    "        print(f\"\\n🛑 Early stopping triggered after {epoch+1} epochs (patience: {TRAINING_CONFIG['patience']})\")\n",
    "        break\n",
    "\n",
    "# Load best model weights and save\n",
    "final_model.load_state_dict(best_model_wts)\n",
    "torch.save({\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'hyperparameters': BEST_HPARAMS,\n",
    "    'config': {\n",
    "        'CMA_COLS': CMA_COLS,\n",
    "        'CPP_COLS': CPP_COLS,\n",
    "        'LOOKBACK': LOOKBACK,\n",
    "        'HORIZON': HORIZON\n",
    "    },\n",
    "    'training_history': training_history,\n",
    "    'best_val_loss': best_val_loss\n",
    "}, PATHS['model_save'])\n",
    "\n",
    "print(f\"\\n✅ Training completed successfully!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"  Model saved to: {PATHS['model_save']}\")\n",
    "print(f\"  Training log saved to: {PATHS['training_log']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Validation and Baseline Comparison\n",
    "\n",
    "With the trained model, we'll perform comprehensive evaluation including multiple metrics, visualization of predictions, and error analysis to validate the model's performance for MPC deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# --- Enhanced Model Evaluation with Multiple Metrics ---\nprint(\"🧪 Evaluating final model on test set...\")\n\n# Create test dataset and loader\ntest_dataset = GranulationDataset(df_test, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=VALIDATION_CONFIG['eval_batch_size'], \n    shuffle=False,\n    num_workers=2\n)\n\ndef calculate_metrics(predictions, targets, feature_names):\n    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n    metrics = {}\n    \n    for i, feature in enumerate(feature_names):\n        pred_feature = predictions[:, :, i].flatten()\n        target_feature = targets[:, :, i].flatten()\n        \n        # Mean Absolute Error\n        mae = np.mean(np.abs(pred_feature - target_feature))\n        \n        # Root Mean Square Error\n        rmse = np.sqrt(np.mean((pred_feature - target_feature)**2))\n        \n        # Mean Absolute Percentage Error\n        mape = np.mean(np.abs((target_feature - pred_feature) / (target_feature + 1e-8))) * 100\n        \n        # R-squared\n        ss_res = np.sum((target_feature - pred_feature)**2)\n        ss_tot = np.sum((target_feature - np.mean(target_feature))**2)\n        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n        \n        # Directional accuracy for first step (h=1) across all samples\n        # Compare if prediction and target move in same direction from historical value\n        # Extract first step predictions and targets for all samples\n        first_step_pred = predictions[:, 0, i]  # Shape: (n_samples,)\n        first_step_target = targets[:, 0, i]    # Shape: (n_samples,)\n        \n        # For directional accuracy, we need a reference point (last historical value)\n        # Since we don't have historical data in this context, we'll calculate\n        # step-to-step directional accuracy within the horizon for each sample\n        sample_directional_accuracies = []\n        for sample_idx in range(predictions.shape[0]):\n            sample_pred = predictions[sample_idx, :, i]\n            sample_target = targets[sample_idx, :, i]\n            \n            if len(sample_pred) > 1:\n                pred_diff = np.diff(sample_pred)\n                target_diff = np.diff(sample_target)\n                sample_acc = np.mean(np.sign(pred_diff) == np.sign(target_diff)) * 100\n                sample_directional_accuracies.append(sample_acc)\n        \n        directional_accuracy = np.mean(sample_directional_accuracies) if sample_directional_accuracies else 0.0\n        \n        metrics[feature] = {\n            'MAE': mae,\n            'RMSE': rmse, \n            'MAPE': mape,\n            'R2': r2,\n            'Directional_Accuracy': directional_accuracy\n        }\n    \n    return metrics\n\n# --- Full Test Set Evaluation ---\nfinal_model.eval()\nall_predictions_scaled = []\nall_targets_scaled = []\n\nprint(\"🔄 Running inference on test set...\")\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_loader):\n        past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n        \n        # Get predictions (scaled)\n        predictions = final_model(past_cmas, past_cpps, future_cpps)\n        \n        all_predictions_scaled.append(predictions.cpu().numpy())\n        all_targets_scaled.append(future_cmas_target.cpu().numpy())\n        \n        if batch_idx % 10 == 0:\n            print(f\"  Processed {batch_idx+1}/{len(test_loader)} batches\")\n\n# Concatenate all predictions and targets\nall_predictions_scaled = np.concatenate(all_predictions_scaled, axis=0)\nall_targets_scaled = np.concatenate(all_targets_scaled, axis=0)\n\nprint(f\"✓ Processed {all_predictions_scaled.shape[0]} test samples\")\n\n# --- Convert to Original Scale ---\nprint(\"🔄 Converting predictions back to original scale...\")\nall_predictions_unscaled = np.zeros_like(all_predictions_scaled)\nall_targets_unscaled = np.zeros_like(all_targets_scaled)\n\nfor i, col in enumerate(CMA_COLS):\n    # Reshape for scaler (expects 2D)\n    pred_reshaped = all_predictions_scaled[:, :, i].reshape(-1, 1)\n    target_reshaped = all_targets_scaled[:, :, i].reshape(-1, 1)\n    \n    # Inverse transform\n    pred_orig = scalers[col].inverse_transform(pred_reshaped)\n    target_orig = scalers[col].inverse_transform(target_reshaped)\n    \n    # Reshape back to original shape\n    all_predictions_unscaled[:, :, i] = pred_orig.reshape(all_predictions_scaled.shape[0], -1)\n    all_targets_unscaled[:, :, i] = target_orig.reshape(all_targets_scaled.shape[0], -1)\n\n# --- Calculate Comprehensive Metrics ---\nprint(\"📊 Calculating performance metrics...\")\nmetrics_scaled = calculate_metrics(all_predictions_scaled, all_targets_scaled, CMA_COLS)\nmetrics_unscaled = calculate_metrics(all_predictions_unscaled, all_targets_unscaled, CMA_COLS)\n\n# --- Display Results ---\nprint(f\"\\n🎯 COMPREHENSIVE TEST SET RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Test Samples: {all_predictions_unscaled.shape[0]:,}\")\nprint(f\"Prediction Horizon: {HORIZON} steps\")\nprint(f\"Model Architecture: {BEST_HPARAMS['d_model']}-dim transformer\")\nprint(f\"Best Validation Loss: {best_val_loss:.6f}\")\n\nprint(f\"\\n📈 Performance Metrics (Original Scale):\")\nprint(\"-\" * 40)\nfor feature in CMA_COLS:\n    m = metrics_unscaled[feature]\n    unit = \"μm\" if feature == \"d50\" else \"%\"\n    print(f\"\\n{feature.upper()} ({unit}):\")\n    print(f\"  MAE:  {m['MAE']:.3f} {unit}\")\n    print(f\"  RMSE: {m['RMSE']:.3f} {unit}\")\n    print(f\"  MAPE: {m['MAPE']:.1f}%\")\n    print(f\"  R²:   {m['R2']:.3f}\")\n    print(f\"  Dir.Acc: {m['Directional_Accuracy']:.1f}% (within-horizon)\")\n\n# --- Visualization ---\nprint(f\"\\n📊 Creating visualizations...\")\n\n# Plot multiple sample predictions\nnum_samples = min(VALIDATION_CONFIG['plot_samples'], all_predictions_unscaled.shape[0])\nsample_indices = np.random.choice(all_predictions_unscaled.shape[0], num_samples, replace=False)\n\nfig, axes = plt.subplots(len(CMA_COLS), num_samples, figsize=(5*num_samples, 4*len(CMA_COLS)))\nif len(CMA_COLS) == 1:\n    axes = axes.reshape(1, -1)\nif num_samples == 1:\n    axes = axes.reshape(-1, 1)\n\nfig.suptitle('Transformer Model: Test Set Predictions vs Ground Truth', fontsize=16)\n\nfor col_idx, col in enumerate(CMA_COLS):\n    for sample_idx in range(num_samples):\n        ax = axes[col_idx, sample_idx]\n        \n        sample_id = sample_indices[sample_idx]\n        time_steps = range(HORIZON)\n        \n        target = all_targets_unscaled[sample_id, :, col_idx]\n        prediction = all_predictions_unscaled[sample_id, :, col_idx]\n        \n        ax.plot(time_steps, target, 'b-', linewidth=2, marker='o', markersize=3, \n                label='Ground Truth', alpha=0.8)\n        ax.plot(time_steps, prediction, 'r--', linewidth=2, marker='s', markersize=3,\n                label='Prediction', alpha=0.8)\n        \n        # Calculate sample-specific metrics\n        sample_mae = np.mean(np.abs(prediction - target))\n        \n        if col == 'd50':\n            ax.set_ylabel('d50 (μm)', fontsize=10)\n            ax.set_title(f'Sample {sample_id+1} - Particle Size\\n(MAE: {sample_mae:.1f} μm)', fontsize=9)\n        elif col == 'lod':\n            ax.set_ylabel('LOD (%)', fontsize=10)\n            ax.set_title(f'Sample {sample_id+1} - Moisture Content\\n(MAE: {sample_mae:.3f} %)', fontsize=9)\n        \n        ax.legend(fontsize=8)\n        ax.grid(True, alpha=0.3)\n        ax.set_xlabel('Time Steps', fontsize=10)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n# --- Error Distribution Analysis ---\nfig, axes = plt.subplots(1, len(CMA_COLS), figsize=(6*len(CMA_COLS), 4))\nif len(CMA_COLS) == 1:\n    axes = [axes]\n\nfig.suptitle('Prediction Error Distribution Analysis', fontsize=14)\n\nfor i, col in enumerate(CMA_COLS):\n    errors = (all_predictions_unscaled[:, :, i] - all_targets_unscaled[:, :, i]).flatten()\n    \n    axes[i].hist(errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n    axes[i].axvline(np.mean(errors), color='red', linestyle='--', \n                   label=f'Mean: {np.mean(errors):.3f}')\n    axes[i].axvline(0, color='black', linestyle='-', alpha=0.5)\n    \n    unit = \"μm\" if col == \"d50\" else \"%\"\n    axes[i].set_xlabel(f'Prediction Error ({unit})', fontsize=10)\n    axes[i].set_ylabel('Frequency', fontsize=10)\n    axes[i].set_title(f'{col.upper()} Error Distribution', fontsize=12)\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n# --- Save Results ---\nif VALIDATION_CONFIG['save_predictions']:\n    results_file = os.path.join(DATA_DIR, 'test_predictions.npz')\n    np.savez(results_file,\n             predictions=all_predictions_unscaled,\n             targets=all_targets_unscaled,\n             metrics=metrics_unscaled,\n             config=BEST_HPARAMS)\n    print(f\"💾 Test predictions saved to: {results_file}\")\n\nprint(f\"\\n✅ Model evaluation completed successfully!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharmacontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}