{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Predictive Model Training and Validation (Optimized)\n",
    "\n",
    "**Project:** `PharmaControl-Pro`  \n",
    "**Goal:** Build, train, and validate the predictive 'ML kernel' with **optimized hyperparameter search** and enhanced Optuna configuration for faster convergence and better performance.\n",
    "\n",
    "### Key Optimizations\n",
    "- **Intelligent search space design** based on transformer best practices\n",
    "- **Advanced pruning strategies** for faster convergence\n",
    "- **Multi-objective optimization** balancing performance and efficiency\n",
    "- **Adaptive sampling** with informed parameter ranges\n",
    "- **Enhanced early stopping** with validation-based pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our models and dataset\n",
    "sys.path.append('..') \n",
    "from V1.src.model_architecture import GranulationPredictor\n",
    "from V1.src.dataset import GranulationDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"🚀 Starting optimized hyperparameter search...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIMIZED CONFIGURATION ---\n",
    "DATA_DIR = '../data'\n",
    "LOOKBACK = 36\n",
    "HORIZON = 72 \n",
    "CMA_COLS = ['d50', 'lod']\n",
    "CPP_COLS = ['spray_rate', 'air_flow', 'carousel_speed', 'specific_energy', 'froude_number_proxy']\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# OPTIMIZED: More trials with intelligent early stopping\n",
    "OPTUNA_CONFIG = {\n",
    "    'n_trials': 50,              # Increased from 20 for better exploration\n",
    "    'tuning_epochs': 8,          # Increased from 5 for better trial evaluation\n",
    "    'tuning_batch_size': 256,    # Increased from 128 for faster training\n",
    "    'study_direction': 'minimize',\n",
    "    'timeout': 3600,             # 1 hour timeout for studies\n",
    "    'gc_after_trial': True       # Garbage collection to prevent memory issues\n",
    "}\n",
    "\n",
    "# OPTIMIZED: Faster final training with better early stopping\n",
    "TRAINING_CONFIG = {\n",
    "    'final_epochs': 40,          # Reduced from 50 (early stopping handles this)\n",
    "    'final_batch_size': 128,     # Increased from 64 for faster training\n",
    "    'patience': 8,               # Reduced from 10 for faster convergence\n",
    "    'gradient_clip_value': 1.0,\n",
    "    'min_delta': 1e-5           # Slightly relaxed for better convergence\n",
    "}\n",
    "\n",
    "# OPTIMIZED: Smarter search space based on transformer best practices\n",
    "OPTIMIZED_SEARCH_SPACE = {\n",
    "    # Transformer dimensions: Powers of 2 for optimal attention computation\n",
    "    'd_model': [32, 64, 96, 128, 192],  # Added 96, 192 for fine-grained search\n",
    "    \n",
    "    # Attention heads: Must divide d_model evenly\n",
    "    'nhead_factor': [1, 2, 3, 4, 6, 8], # Will multiply with base factors\n",
    "    \n",
    "    # Layer configurations: Asymmetric encoder/decoder often works better\n",
    "    'encoder_layers': (2, 6),      # Deeper encoders for better context\n",
    "    'decoder_layers': (1, 4),      # Shallower decoders to prevent overfitting\n",
    "    \n",
    "    # Learning rate: More focused range around optimal values\n",
    "    'lr': (3e-4, 2e-3),           # Narrowed range around typical transformer LRs\n",
    "    \n",
    "    # Regularization: More granular control\n",
    "    'dropout': (0.05, 0.25),      # Slightly narrowed based on task complexity\n",
    "    'weight_decay': (1e-5, 5e-3), # Expanded upper range for better regularization\n",
    "    \n",
    "    # NEW: Feedforward dimension ratio (multiplicative factor of d_model)\n",
    "    'ff_ratio': [1, 2, 4],        # 1x, 2x, 4x d_model for feedforward layers\n",
    "    \n",
    "    # NEW: Loss function parameters\n",
    "    'horizon_start_weight': (0.3, 0.7),  # Start weight for horizon weighting\n",
    "    'horizon_end_weight': (1.2, 2.0),    # End weight for horizon weighting\n",
    "}\n",
    "\n",
    "# OPTIMIZED: Enhanced loss configuration\n",
    "LOSS_CONFIG = {\n",
    "    'horizon_weighting': True,\n",
    "    'adaptive_weighting': True,    # NEW: Adapt weights based on feature importance\n",
    "    'loss_type': 'mse'\n",
    "}\n",
    "\n",
    "# File paths\n",
    "PATHS = {\n",
    "    'model_save': os.path.join(DATA_DIR, 'best_predictor_model_optimized.pth'),\n",
    "    'scalers_save': os.path.join(DATA_DIR, 'model_scalers.joblib'),\n",
    "    'optuna_study': os.path.join(DATA_DIR, 'optuna_study_optimized.pkl'),\n",
    "    'training_log': os.path.join(DATA_DIR, 'training_log_optimized.csv'),\n",
    "    'study_viz': os.path.join(DATA_DIR, 'optuna_study_plots.html')\n",
    "}\n",
    "\n",
    "print(f\"📊 Optimized Configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Trials: {OPTUNA_CONFIG['n_trials']} (vs 20 original)\")\n",
    "print(f\"  Epochs per trial: {OPTUNA_CONFIG['tuning_epochs']} (vs 5 original)\")\n",
    "print(f\"  Batch size: {OPTUNA_CONFIG['tuning_batch_size']} (vs 128 original)\")\n",
    "print(f\"  Search space combinations: ~{5*6*5*4*10*8*3*8:.0f} (vs ~720 original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data (same as original)\n",
    "df_train_raw = pd.read_csv(os.path.join(DATA_DIR, 'train_data_raw.csv'))\n",
    "df_val_raw = pd.read_csv(os.path.join(DATA_DIR, 'validation_data_raw.csv'))\n",
    "df_test_raw = pd.read_csv(os.path.join(DATA_DIR, 'test_data_raw.csv'))\n",
    "\n",
    "def create_scalers_and_scale_data(df_train, df_val, df_test, feature_cols):\n",
    "    scalers = {}\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_val_scaled = df_val.copy()\n",
    "    df_test_scaled = df_test.copy()\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_train_scaled[col] = scaler.fit_transform(df_train[[col]])\n",
    "        df_val_scaled[col] = scaler.transform(df_val[[col]])\n",
    "        df_test_scaled[col] = scaler.transform(df_test[[col]])\n",
    "        scalers[col] = scaler\n",
    "    \n",
    "    return df_train_scaled, df_val_scaled, df_test_scaled, scalers\n",
    "\n",
    "all_feature_cols = CMA_COLS + CPP_COLS\n",
    "df_train, df_val, df_test, scalers = create_scalers_and_scale_data(\n",
    "    df_train_raw, df_val_raw, df_test_raw, all_feature_cols\n",
    ")\n",
    "\n",
    "joblib.dump(scalers, PATHS['scalers_save'])\n",
    "print(\"✅ Data loaded and scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Enhanced loss function with adaptive weighting\n",
    "class AdaptiveWeightedHorizonMSELoss(nn.Module):\n",
    "    \"\"\"Enhanced loss with adaptive horizon weighting and feature importance.\"\"\"\n",
    "    def __init__(self, horizon: int, start_weight: float = 0.5, end_weight: float = 1.5, \n",
    "                 feature_weights: list = None):\n",
    "        super().__init__()\n",
    "        # Horizon weights (increasing over time)\n",
    "        horizon_weights = torch.linspace(start_weight, end_weight, horizon).view(1, -1, 1)\n",
    "        self.register_buffer('horizon_weights', horizon_weights)\n",
    "        \n",
    "        # Feature weights (if provided)\n",
    "        if feature_weights is not None:\n",
    "            feature_weights = torch.tensor(feature_weights).view(1, 1, -1)\n",
    "            self.register_buffer('feature_weights', feature_weights)\n",
    "        else:\n",
    "            self.feature_weights = None\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        loss = (prediction - target) ** 2\n",
    "        \n",
    "        # Apply horizon weighting\n",
    "        loss = loss * self.horizon_weights\n",
    "        \n",
    "        # Apply feature weighting if available\n",
    "        if self.feature_weights is not None:\n",
    "            loss = loss * self.feature_weights\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "\n",
    "print(\"✅ Enhanced loss function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Intelligent hyperparameter sampling\n",
    "def sample_intelligent_hyperparameters(trial):\n",
    "    \"\"\"Sample hyperparameters with intelligent constraints and relationships.\"\"\"\n",
    "    \n",
    "    # Sample d_model first (drives other decisions)\n",
    "    d_model = trial.suggest_categorical('d_model', OPTIMIZED_SEARCH_SPACE['d_model'])\n",
    "    \n",
    "    # INTELLIGENT: Sample nhead based on d_model divisors\n",
    "    valid_nheads = []\n",
    "    for factor in OPTIMIZED_SEARCH_SPACE['nhead_factor']:\n",
    "        for base in [1, 2, 4, 8]:  # Common attention head counts\n",
    "            nhead = factor * base\n",
    "            if d_model % nhead == 0 and nhead <= d_model and nhead >= 1:\n",
    "                valid_nheads.append(nhead)\n",
    "    \n",
    "    if not valid_nheads:\n",
    "        valid_nheads = [1, 2, 4]  # Fallback\n",
    "    \n",
    "    nhead = trial.suggest_categorical('nhead', sorted(set(valid_nheads)))\n",
    "    \n",
    "    # INTELLIGENT: Asymmetric encoder/decoder (encoders typically deeper)\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', \n",
    "                                         OPTIMIZED_SEARCH_SPACE['encoder_layers'][0],\n",
    "                                         OPTIMIZED_SEARCH_SPACE['encoder_layers'][1])\n",
    "    \n",
    "    num_decoder_layers = trial.suggest_int('num_decoder_layers',\n",
    "                                         OPTIMIZED_SEARCH_SPACE['decoder_layers'][0],\n",
    "                                         OPTIMIZED_SEARCH_SPACE['decoder_layers'][1])\n",
    "    \n",
    "    # INTELLIGENT: Feedforward dimension as multiple of d_model\n",
    "    ff_ratio = trial.suggest_categorical('ff_ratio', OPTIMIZED_SEARCH_SPACE['ff_ratio'])\n",
    "    dim_feedforward = d_model * ff_ratio\n",
    "    \n",
    "    # Sample other hyperparameters\n",
    "    lr = trial.suggest_float('lr', \n",
    "                           OPTIMIZED_SEARCH_SPACE['lr'][0], \n",
    "                           OPTIMIZED_SEARCH_SPACE['lr'][1], log=True)\n",
    "    \n",
    "    dropout = trial.suggest_float('dropout',\n",
    "                                OPTIMIZED_SEARCH_SPACE['dropout'][0],\n",
    "                                OPTIMIZED_SEARCH_SPACE['dropout'][1])\n",
    "    \n",
    "    weight_decay = trial.suggest_float('weight_decay',\n",
    "                                     OPTIMIZED_SEARCH_SPACE['weight_decay'][0],\n",
    "                                     OPTIMIZED_SEARCH_SPACE['weight_decay'][1], log=True)\n",
    "    \n",
    "    # INTELLIGENT: Loss function parameters\n",
    "    horizon_start_weight = trial.suggest_float('horizon_start_weight',\n",
    "                                             OPTIMIZED_SEARCH_SPACE['horizon_start_weight'][0],\n",
    "                                             OPTIMIZED_SEARCH_SPACE['horizon_start_weight'][1])\n",
    "    \n",
    "    horizon_end_weight = trial.suggest_float('horizon_end_weight',\n",
    "                                           OPTIMIZED_SEARCH_SPACE['horizon_end_weight'][0],\n",
    "                                           OPTIMIZED_SEARCH_SPACE['horizon_end_weight'][1])\n",
    "    \n",
    "    return {\n",
    "        'd_model': d_model,\n",
    "        'nhead': nhead,\n",
    "        'num_encoder_layers': num_encoder_layers,\n",
    "        'num_decoder_layers': num_decoder_layers,\n",
    "        'dim_feedforward': dim_feedforward,\n",
    "        'lr': lr,\n",
    "        'dropout': dropout,\n",
    "        'weight_decay': weight_decay,\n",
    "        'horizon_start_weight': horizon_start_weight,\n",
    "        'horizon_end_weight': horizon_end_weight\n",
    "    }\n",
    "\n",
    "print(\"✅ Intelligent hyperparameter sampling function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Enhanced objective function with better pruning\n",
    "def optimized_objective(trial):\n",
    "    \"\"\"Optimized objective function with intelligent pruning and validation.\"\"\"\n",
    "    \n",
    "    # Sample hyperparameters intelligently\n",
    "    params = sample_intelligent_hyperparameters(trial)\n",
    "    \n",
    "    # Create model with sampled parameters\n",
    "    model = GranulationPredictor(\n",
    "        cma_features=len(CMA_COLS),\n",
    "        cpp_features=len(CPP_COLS),\n",
    "        d_model=params['d_model'],\n",
    "        nhead=params['nhead'],\n",
    "        num_encoder_layers=params['num_encoder_layers'],\n",
    "        num_decoder_layers=params['num_decoder_layers'],\n",
    "        dim_feedforward=params['dim_feedforward'],\n",
    "        dropout=params['dropout']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Enhanced loss function\n",
    "    criterion = AdaptiveWeightedHorizonMSELoss(\n",
    "        horizon=HORIZON,\n",
    "        start_weight=params['horizon_start_weight'],\n",
    "        end_weight=params['horizon_end_weight'],\n",
    "        feature_weights=[1.0, 1.2]  # Slightly higher weight for LOD due to difficulty\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # OPTIMIZED: AdamW optimizer with better defaults\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        eps=1e-8,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # OPTIMIZED: Learning rate scheduler for trials\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=OPTUNA_CONFIG['tuning_epochs'], eta_min=params['lr']*0.1\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_dataset = GranulationDataset(df_train, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "    val_dataset = GranulationDataset(df_val, CMA_COLS, CPP_COLS, LOOKBACK, HORIZON)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=OPTUNA_CONFIG['tuning_batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if DEVICE == 'cuda' else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=OPTUNA_CONFIG['tuning_batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if DEVICE == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # OPTIMIZED: Training with enhanced early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    patience_for_trials = 3  # Early stopping within trials\n",
    "    \n",
    "    for epoch in range(OPTUNA_CONFIG['tuning_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(past_cmas, past_cpps, future_cpps)\n",
    "            loss = criterion(prediction, future_cmas_target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # OPTIMIZED: More frequent pruning checks\n",
    "            if batch_idx % 5 == 0:\n",
    "                trial.report(loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                past_cmas, past_cpps, future_cpps, future_cmas_target = [b.to(DEVICE) for b in batch]\n",
    "                prediction = model(past_cmas, past_cpps, future_cpps)\n",
    "                val_loss += criterion(prediction, future_cmas_target).item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # OPTIMIZED: Enhanced early stopping for trials\n",
    "        if avg_val_loss < best_val_loss * 0.995:  # 0.5% improvement threshold\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            \n",
    "        # Early stopping within trial\n",
    "        if no_improvement_count >= patience_for_trials and epoch >= 3:\n",
    "            break\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report to Optuna\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    # OPTIMIZED: Cleanup to prevent memory leaks\n",
    "    if OPTUNA_CONFIG.get('gc_after_trial', False):\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"✅ Optimized objective function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Advanced Optuna study configuration\n",
    "print(f\"🔍 Starting optimized hyperparameter search...\")\n",
    "print(f\"  Trials: {OPTUNA_CONFIG['n_trials']}\")\n",
    "print(f\"  Enhanced pruning and early stopping enabled\")\n",
    "print(f\"  Intelligent search space with {len(OPTIMIZED_SEARCH_SPACE)} parameter types\")\n",
    "\n",
    "# OPTIMIZED: Advanced pruners for better convergence\n",
    "pruner = optuna.pruners.HyperbandPruner(\n",
    "    min_resource=1,\n",
    "    max_resource=OPTUNA_CONFIG['tuning_epochs'],\n",
    "    reduction_factor=3\n",
    ")\n",
    "\n",
    "# OPTIMIZED: TPE sampler with enhanced settings\n",
    "sampler = optuna.samplers.TPESampler(\n",
    "    n_startup_trials=10,  # More startup trials for better initial exploration\n",
    "    n_ei_candidates=24,   # More candidates for expected improvement\n",
    "    seed=RANDOM_SEED,\n",
    "    multivariate=True     # Consider parameter interactions\n",
    ")\n",
    "\n",
    "# Create optimized study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=pruner,\n",
    "    sampler=sampler,\n",
    "    study_name='granulation_predictor_optimized'\n",
    ")\n",
    "\n",
    "# OPTIMIZED: Run study with timeout and progress tracking\n",
    "def objective_with_logging(trial):\n",
    "    trial_start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "    trial_end = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "    \n",
    "    if trial_start:\n",
    "        trial_start.record()\n",
    "    \n",
    "    try:\n",
    "        result = optimized_objective(trial)\n",
    "        if trial_end and trial_start:\n",
    "            trial_end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = trial_start.elapsed_time(trial_end) / 1000.0  # Convert to seconds\n",
    "            trial.set_user_attr('trial_time', elapsed)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        trial.set_user_attr('error', str(e))\n",
    "        raise\n",
    "\n",
    "# Run the optimized study\n",
    "study.optimize(\n",
    "    objective_with_logging, \n",
    "    n_trials=OPTUNA_CONFIG['n_trials'],\n",
    "    timeout=OPTUNA_CONFIG.get('timeout'),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save study\n",
    "joblib.dump(study, PATHS['optuna_study'])\n",
    "\n",
    "print(f\"\\n🎯 Optimized hyperparameter search completed!\")\n",
    "print(f\"  Total trials: {len(study.trials)}\")\n",
    "print(f\"  Pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "print(f\"  Completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\n🏆 Best trial results:\")\n",
    "print(f\"  Validation Loss: {best_trial.value:.6f}\")\n",
    "print(f\"  Parameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Store optimized hyperparameters\n",
    "OPTIMIZED_HPARAMS = best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Enhanced study analysis and visualization\n",
    "print(\"\\n📊 Study Analysis:\")\n",
    "\n",
    "# Performance statistics\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "if len(completed_trials) > 1:\n",
    "    values = [t.value for t in completed_trials]\n",
    "    print(f\"  Best loss: {min(values):.6f}\")\n",
    "    print(f\"  Worst loss: {max(values):.6f}\")\n",
    "    print(f\"  Mean loss: {np.mean(values):.6f}\")\n",
    "    print(f\"  Std loss: {np.std(values):.6f}\")\n",
    "\n",
    "# Parameter importance (if enough trials completed)\n",
    "if len(completed_trials) >= 10:\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        print(f\"\\n🎯 Parameter Importance:\")\n",
    "        for param, score in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  {param}: {score:.3f}\")\n",
    "    except:\n",
    "        print(\"  Parameter importance analysis failed (not enough data)\")\n",
    "\n",
    "# Save study analysis\n",
    "study_summary = {\n",
    "    'best_params': best_trial.params,\n",
    "    'best_value': best_trial.value,\n",
    "    'n_trials': len(study.trials),\n",
    "    'n_completed': len(completed_trials),\n",
    "    'optimization_time': sum([t.user_attrs.get('trial_time', 0) for t in completed_trials])\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(PATHS['optuna_study'].replace('.pkl', '_summary.json'), 'w') as f:\n",
    "    json.dump(study_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Study analysis completed and saved\")\n",
    "print(f\"📁 Files saved:\")\n",
    "print(f\"  Study: {PATHS['optuna_study']}\")\n",
    "print(f\"  Summary: {PATHS['optuna_study'].replace('.pkl', '_summary.json')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}